<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Project 4 — Neural Fields and NeRF (CS180/280A)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <!-- Simple styling (self-contained) -->
  <style>
    :root {
      --fg: #1b1f23;
      --muted: #586069;
      --bg: #ffffff;
      --accent: #0366d6;
      --border: #e1e4e8;
      --code-bg: #f6f8fa;
    }
    html, body {
      margin: 0; padding: 0; background: var(--bg); color: var(--fg);
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;
      line-height: 1.55;
    }
    .container { max-width: 980px; margin: 0 auto; padding: 32px 16px 80px; }
    header { margin-bottom: 16px; }
    h1 { font-size: 32px; margin: 0 0 8px; }
    .sub { color: var(--muted); margin: 0 0 24px; }
    h2 { margin-top: 48px; border-bottom: 1px solid var(--border); padding-bottom: 6px; }
    h3 { margin-top: 28px; }
    h4 { margin-top: 20px; }
    p { margin: 12px 0; }
    ul { margin: 8px 0 16px 20px; }
    li { margin: 6px 0; }
    a { color: var(--accent); text-decoration: none; }
    a:hover { text-decoration: underline; }
    code, pre { background: var(--code-bg); border: 1px solid var(--border); }
    code { padding: 0 4px; border-radius: 3px; }
    pre { padding: 12px; border-radius: 6px; overflow-x: auto; }
    .toc {
      border: 1px solid var(--border); background: #fafbfc; border-radius: 6px;
      padding: 12px 14px; margin: 18px 0 26px;
    }
    .grid {
      display: grid; grid-template-columns: repeat(auto-fill, minmax(240px, 1fr));
      gap: 12px; margin: 12px 0;
    }
    figure { margin: 12px 0; }
    figcaption { font-size: 13px; color: var(--muted); margin-top: 4px; }
    img, video { width: 100%; height: auto; border: 1px solid var(--border); border-radius: 6px; background: #fff; }
    .callout {
      border-left: 4px solid var(--accent); padding: 10px 14px; background: #f1f8ff; margin: 16px 0; border-radius: 4px;
    }
    details { border: 1px solid var(--border); border-radius: 6px; padding: 10px 12px; margin: 12px 0; background: #fff; }
    details > summary { cursor: pointer; font-weight: 600; }
    .pill {
      display: inline-block; font-size: 12px; padding: 2px 8px; border: 1px solid var(--border);
      border-radius: 999px; background: #f6f8fa; color: #586069; margin-right: 6px;
    }
    .two-col {
      display: grid; grid-template-columns: 1fr 1fr; gap: 12px;
    }
    @media (max-width: 820px) { .two-col { grid-template-columns: 1fr; } }
    .kbd { border: 1px solid var(--border); border-bottom-width: 3px; border-radius: 4px; padding: 0 6px; background: #f6f8fa; font-weight: 600; }
    .req { font-weight: 600; color: #22863a; }
    .xref { color: #6f42c1; }
  </style>
  <!-- MathJax for equations -->
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <div class="container">
    <header>
      <h1>Project 4 — Neural Fields and NeRF</h1>
      <p class="sub">
        Author: <strong>Yuchen Zhang</strong> · Course: CS180/280A · Fall 2025
      </p>
      <div class="toc">
        <strong>Contents</strong>
        <ul>
          <li><a href="#intro">1. Introduction</a></li>
          <li><a href="#part0">2. Part 0 — Data Acquisition &amp; Dataset</a>
            <ul>
              <li><a href="#part01">0.1 Camera Calibration</a></li>
              <li><a href="#part02">0.2 Single-Tag Camera Poses</a></li>
              <li><a href="#part03">0.3 Pose Visualization</a></li>
              <li><a href="#part04">0.4 Create NeRF Dataset (npz)</a></li>
            </ul>
          </li>
          <li><a href="#part1">3. Part 1 — 2D Neural Field</a></li>
          <li><a href="#part2">4. Part 2 — NeRF</a>
            <ul>
              <li><a href="#p21">2.1 Rays from Cameras</a></li>
              <li><a href="#p22">2.2 Sampling Along Rays</a></li>
              <li><a href="#p23">2.3 Multi-view Rays Dataloader</a></li>
              <li><a href="#p24">2.4 Training Skeleton</a></li>
              <li><a href="#p25">2.5 Full NeRF (coarse)</a></li>
              <li><a href="#p26">2.6 Custom Dataset + Extras</a></li>
            </ul>
          </li>
        </ul>
      </div>
    </header>

    <section id="intro">
      <h2>1. Introduction</h2>
      <p>
        This project builds a complete pipeline from data capture to neural rendering. In Part 0, I calibrate
        a camera using ArUco markers, estimate camera poses from a single square tag, visualize the rig, and
        assemble an undistorted NeRF dataset (npz) with intrinsics and camera-to-world poses. Part 1 fits a 2D
        neural field to an image using sinusoidal positional encoding, logging PSNR and visualizing improvement.
        Part 2 implements a NeRF: creating rays from cameras, sampling points along rays, composing with volume
        rendering, training on a multi-view dataset, and rendering novel views and depth/disparity videos.
      </p>
      <p class="callout">
        Reference pages: my Project 3 page
        <a href="https://yuchenzhang789.github.io/proj_3/index.html" target="_blank" rel="noopener">link</a>
        and a helpful reference from a friend
        <a href="https://zhangzhendan-berkeley.github.io/zhangzhendan-Berkeley/4/index.html" target="_blank" rel="noopener">link</a>.
        The implementation follows the <a href="https://cal-cs180.github.io/fa25/hw/proj4/index.html" target="_blank" rel="noopener">Project 4 spec</a>.
      </p>
    </section>

    <section id="part0">
      <h2>2. Part 0 — Data Acquisition &amp; Dataset</h2>

      <section id="part01">
        <h3>0.1 Camera Calibration</h3>
        <p>
          I detect ArUco tags in a small board layout and calibrate with multiple images. For each detected tag,
          I push its four 2D corners and corresponding 3D square corners into OpenCV’s <code>calibrateCamera</code>.
          I fix <code>K3</code> and zero the tangential distortion to produce a stable intrinsics estimate for NeRF.
        </p>
        <div class="two-col">
          <figure>
            <img src="assets/part0/calib_detections.jpg" alt="Detected ArUco tags overlay" />
            <figcaption>Detected ArUco corners used as calibration samples.</figcaption>
          </figure>
          <figure>
            <img src="assets/part0/reproj_hist.png" alt="Per-sample RMS reprojection histogram" />
            <figcaption>Per-sample RMS reprojection error (lower is better).</figcaption>
          </figure>
        </div>

        <details>
          <summary>Implementation details (part01_calibrate_camera.py)</summary>
          <ul>
            <li>ArUco detection: <code>cv2.aruco.ArucoDetector</code> with a 2×3 grid of IDs {0..5}.</li>
            <li>Object points: <code>objp_for_tag(id, TAG_SIZE_M, H_SPACING_M, V_SPACING_M)</code> on the z=0 plane.</li>
            <li>Calibration: <code>cv2.calibrateCamera</code> with <code>CALIB_FIX_K3 | CALIB_ZERO_TANGENT_DIST</code>.</li>
            <li>Diagnostics: save <code>K, dist</code>, per-sample RMS, and reprojection error into <code>camera_calibration.npz</code>.</li>
          </ul>
          <p><span class="pill">Code</span> <a href="./part01_calibrate_camera.py">part01_calibrate_camera.py</a></p>
        </details>

        <h4>Results</h4>
        <ul>
          <li class="req">Intrinsic matrix K estimated (saved to <code>camera_calibration.npz</code>).</li>
          <li class="req">Distortion parameters recorded.</li>
          <li class="req">Mean reprojection error and per-sample RMS visualized.</li>
        </ul>
      </section>

      <section id="part02">
        <h3>0.2 Single-Tag Camera Poses</h3>
        <p>
          For each image, I detect the specified tag ID and solve PnP with the square-specific IPPE method to obtain
          two candidate solutions. I disambiguate by choosing the solution whose camera z is above the plane (z &gt; 0).
          Then I convert OpenCV’s w2c to c2w for NeRF.
        </p>
        <div class="two-col">
          <figure>
            <img src="assets/part0/tag_pnp.png" alt="Pose solvePnP overview" />
            <figcaption>IPPE solvePnPGeneric on the square tag; pick the physically consistent solution.</figcaption>
          </figure>
          <figure>
            <img src="assets/part0/poses_quiver.png" alt="Camera positions/quiver plot" />
            <figcaption>Camera centers and viewing directions from recovered c2w transforms.</figcaption>
          </figure>
        </div>

        <details>
          <summary>Implementation details (part03_find_camera_pose.py)</summary>
          <ul>
            <li>Detection: <code>cv2.aruco.ArucoDetector</code> → <code>corners, ids</code>.</li>
            <li>Pose: <code>cv2.solvePnPGeneric(..., SOLVEPNP_IPPE_SQUARE)</code> → choose solution by comparing
                <code>cam = -R^T t</code> z-components.</li>
            <li>c2w: set <code>R_c2w = R^T</code>, <code>t_c2w = -R^T t</code>; save each pose under <code>poses/pose_###.npz</code>.</li>
          </ul>
          <p><span class="pill">Code</span> <a href="./part03_find_camera_pose.py">part03_find_camera_pose.py</a></p>
        </details>
      </section>

      <section id="part03">
        <h3>0.3 Pose Visualization</h3>
        <p>
          I visualize the recovered rig with <code>viser</code>, adding a frustum for each camera with the undistorted image.
          This helps verify consistent orientation and overall path coverage.
        </p>
        <figure>
          <img src="assets/part0/viser_frustums.png" alt="Viser frustums visualization" />
          <figcaption>Viser-based 3D visualization of camera frustums in world coordinates.</figcaption>
        </figure>

        <details>
          <summary>Implementation notes</summary>
          <ul>
            <li>Field-of-view computed from fy: <code>fov_y = 2 atan(H/2 / fy)</code>.</li>
            <li>Frustum geometry uses c2w rotation and translation directly.</li>
          </ul>
          <p><span class="pill">Code</span> <a href="./part23_visualize_2.py">part23_visualize_2.py</a></p>
        </details>
      </section>

      <section id="part04">
        <h3>0.4 Create NeRF Dataset (npz)</h3>
        <p>
          Using the calibration and poses, I undistort all images with a single optimal <code>newK</code> computed from the
          first image, optionally crop by ROI, convert to RGB, and split into train/val/test. I store
          <code>images_train</code>, <code>c2ws_train</code>, <code>images_val</code>, <code>c2ws_val</code>,
          <code>c2ws_test</code>, and average <code>focal</code> in <code>dataset/my_data.npz</code>.
        </p>

        <div class="two-col">
          <figure>
            <img src="assets/part0/undist_before.jpg" alt="Before undistort" />
            <figcaption>Original frame (BGR).</figcaption>
          </figure>
          <figure>
            <img src="assets/part0/undist_after.jpg" alt="After undistort" />
            <figcaption>Undistorted frame (RGB) using single newK.</figcaption>
          </figure>
        </div>

        <figure>
          <img src="assets/part0/split_grid.png" alt="Train/Val/Test sample grid" />
          <figcaption>Train/Val/Test sampling preview from my dataset.</figcaption>
        </figure>

        <details>
          <summary>Implementation details (part04_create_dataset.py)</summary>
          <ul>
            <li>Compute single <code>newK</code> and ROI from first image via <code>cv2.getOptimalNewCameraMatrix</code>.</li>
            <li>Undistort every image with <code>(K, dist) → newK</code>. Optionally crop to ROI. Convert BGR→RGB. Keep uint8.</li>
            <li>Split 70/15/15 with fixed seed; save arrays and <code>focal = (fx + fy)/2</code>.</li>
          </ul>
          <p>
            <span class="pill">Code</span> <a href="./part04_create_dataset.py">part04_create_dataset.py</a><br />
            <span class="pill">Resizer</span> <a href="./resize_dataset.py">resize_dataset.py</a>
          </p>
        </details>

        <h4>Deliverables checklist</h4>
        <ul>
          <li class="req">Show tag detections and calibration diagnostics.</li>
          <li class="req">Show pose recovery and a 3D visualization.</li>
          <li class="req">Show undistortion before/after and dataset split preview.</li>
          <li class="req">Provide dataset stats: H×W, focal, counts per split.</li>
        </ul>
      </section>
    </section>

    <section id="part1">
      <h2>3. Part 1 — 2D Neural Field</h2>
      <p>
        I fit a sinusoidally encoded MLP to a single RGB image. Inputs are normalized pixel coordinates
        with positional encoding up to frequency L. The network predicts RGB in [0,1]. I report PSNR over
        iterations, save reconstructions at milestones, and include a small sweep over width and L.
      </p>

      <div class="grid">
        <figure>
          <img src="part1_outputs/base_W256_L10/recon_it0001.png" alt="Iter 1" />
          <figcaption>Iter 1</figcaption>
        </figure>
        <figure>
          <img src="part1_outputs/base_W256_L10/recon_it0050.png" alt="Iter 50" />
          <figcaption>Iter 50</figcaption>
        </figure>
        <figure>
          <img src="part1_outputs/base_W256_L10/recon_it0200.png" alt="Iter 200" />
          <figcaption>Iter 200</figcaption>
        </figure>
        <figure>
          <img src="part1_outputs/base_W256_L10/recon_it2000.png" alt="Iter 2000" />
          <figcaption>Iter 2000 (final baseline)</figcaption>
        </figure>
      </div>

      <div class="two-col">
        <figure>
          <img src="part1_outputs/base_W256_L10/psnr_curve.png" alt="PSNR curve" />
          <figcaption>PSNR over iterations (baseline).</figcaption>
        </figure>
        <figure>
          <img src="part1_outputs/sweep_grid.png" alt="2x2 sweep grid" />
          <figcaption>2×2 sweep over width ∈ {64, 256} and L ∈ {4, 10}.</figcaption>
        </figure>
      </div>

      <details>
        <summary>Implementation details (part1_neural_field_2d.py)</summary>
        <ul>
          <li>Positional Encoding (2D): PE(x) = [x, sin(2^k π x), cos(2^k π x)] for k=0..L-1.</li>
          <li>MLP: width=256, depth=3, Sigmoid output; MSE loss with Adam; chunked full-image render.</li>
          <li>Device selection prefers MPS, then CUDA, then CPU; optional AMP on CUDA.</li>
        </ul>
        <p><span class="pill">Code</span> <a class="xref" href="./part1_neural_field_2d.py">part1_neural_field_2d.py</a></p>
      </details>

      <h4>Observations</h4>
      <ul>
        <li>Higher L (more frequencies) captures fine detail faster but risks ringing if width is small.</li>
        <li>Width 256 significantly improves convergence vs width 64 at similar L.</li>
      </ul>
    </section>

    <section id="part2">
      <h2>4. Part 2 — NeRF</h2>

      <section id="p21">
        <h3>2.1 Rays from Cameras</h3>
        <p>
          I implement SE(3) transforms, camera intrinsics inversion, and pixel-to-ray conversion. Sanity tests
          ensure invertibility and the center ray direction for a unit pinhole camera.
        </p>
        <div class="two-col">
          <figure>
            <img src="assets/part2/invert_pose.png" alt="Invert pose test" />
            <figcaption>Transform ∘ inverse consistency test.</figcaption>
          </figure>
          <figure>
            <img src="assets/part2/center_ray.png" alt="Center ray test" />
            <figcaption>Center pixel ray points along +Z for identity pose/intrinsics.</figcaption>
          </figure>
        </div>
        <details>
          <summary>Implementation details (part21_create_rays.py)</summary>
          <ul>
            <li><code>invert_pose</code>, <code>transform</code>, <code>pixel_to_camera</code>, <code>pixel_to_ray</code>, <code>get_rays_from_image</code>.</li>
            <li>Pixel centers use +0.5 offset; directions normalized for stability.</li>
          </ul>
          <p><span class="pill">Code</span> <a href="./part21_create_rays.py">part21_create_rays.py</a></p>
        </details>
      </section>

      <section id="p22">
        <h3>2.2 Sampling Along Rays</h3>
        <p>
          I provide uniform and interval-based stratified sampling of <code>t</code> values in [near, far].
          Given ray origin <code>o</code> and direction <code>d</code>, points are <code>o + t d</code>.
        </p>
        <figure>
          <img src="assets/part2/sampling_lines.png" alt="Sampling along rays" />
          <figcaption>Stratified interval sampling along rays (near→far).</figcaption>
        </figure>
        <details>
          <summary>Implementation details (part22_sampling.py)</summary>
          <ul>
            <li>Two APIs: <code>sample_points_along_rays</code> (uniform) and <code>sample_points_intervals</code> (interval jitter).</li>
            <li>Both return <code>(t_vals, pts)</code> and support MPS/CUDA/CPU.</li>
          </ul>
          <p><span class="pill">Code</span> <a href="./part22_sampling.py">part22_sampling.py</a></p>
        </details>
      </section>

      <section id="p23">
        <h3>2.3 Multi-view Rays Dataloader</h3>
        <p>
          I flatten multi-view images into global buffers of UVs, image IDs, pixel colors, and rays.
          Global and per-image sampling modes are supported, with optional full precompute for speed.
        </p>
        <div class="two-col">
          <figure>
            <img src="assets/part2/uv_check.png" alt="UV indexing check" />
            <figcaption>UV indexing sanity: dataset pixels == image[uv[:,1], uv[:,0]].</figcaption>
          </figure>
          <figure>
            <img src="assets/part2/rays_pts_vis.png" alt="Rays and sampled points in 3D" />
            <figcaption>Viser visualization: cameras, example rays, and sampled points.</figcaption>
          </figure>
        </div>
        <details>
          <summary>Implementation details (part23_rays_dataloader.py)</summary>
          <ul>
            <li><code>RaysData</code> builds rays via <code>_rays_for_image</code>; <code>sample_rays</code> supports "global" and "per_image".</li>
            <li>Device-first selection prefers MPS, then CUDA, then CPU.</li>
          </ul>
          <p><span class="pill">Code</span> <a href="./part23_rays_dataloader.py">part23_rays_dataloader.py</a> · <a href="./part23_visualize_2.py">part23_visualize_2.py</a></p>
        </details>
      </section>

      <section id="p24">
        <h3>2.4 Training Skeleton</h3>
        <p>
          I train a NeRF-like MLP on Lego using stratified sampling, volume rendering, and MSE loss,
          periodically rendering a validation view and logging PSNR.
        </p>
        <div class="two-col">
          <figure>
            <img src="part24_outputs/val_render_010000.npy.png" alt="Validation preview" />
            <figcaption>Validation render preview (downscaled for speed).</figcaption>
          </figure>
          <figure>
            <img src="part24_outputs/psnr_curve.png" alt="PSNR log" />
            <figcaption>PSNR vs iteration.</figcaption>
          </figure>
        </div>
        <details>
          <summary>Implementation details (part24_train_nerf.py)</summary>
          <ul>
            <li>Front-to-back compositing with white background:
              <div style="margin-top:6px">
                <code>alpha_i = 1 - exp(-sigma_i * delta_i), T_i = Π_{j<i}(1 - alpha_j), w_i = T_i alpha_i,</code><br/>
                <code>C = Σ_i w_i c_i + (1 - Σ_i w_i)</code>
              </div>
            </li>
            <li>Validation builds a reduced grid and renders without jitter.</li>
          </ul>
          <p><span class="pill">Code</span> <a href="./part24_train_nerf.py">part24_train_nerf.py</a></p>
        </details>
      </section>

      <section id="p25">
        <h3>2.5 Full NeRF (coarse)</h3>
        <p>
          The full pipeline trains a coarse NeRF on Lego, saves checkpoints, visualizes sampled rays/points,
          logs validation PSNR, and renders a novel-view video along <code>c2ws_test</code>. Volume rendering is
          factored in a reusable module.
        </p>

        <div class="two-col">
          <figure>
            <img src="part25_outputs/rays_samples_step00300.png" alt="Rays and samples visualization" />
            <figcaption>One-time 3D snapshot of cameras, random rays, and jittered samples.</figcaption>
          </figure>
          <figure>
            <img src="part25_outputs/psnr_curve.png" alt="PSNR curve" />
            <figcaption>Validation PSNR over checkpoints.</figcaption>
          </figure>
        </div>

        <div class="grid">
          <figure>
            <img src="part25_outputs/val_pred_step03000.png" alt="Validation prediction at step 3000" />
            <figcaption>Validation prediction (coarse) at final checkpoint.</figcaption>
          </figure>
          <figure>
            <video controls muted src="part25_outputs/novel_view_step03000.mp4"></video>
            <figcaption>Novel-view video along c2ws_test (MP4).</figcaption>
          </figure>
        </div>

        <details>
          <summary>Implementation notes and files</summary>
          <ul>
            <li><span class="pill">Training</span> <a href="./part25_train_nerf.py">part25_train_nerf.py</a></li>
            <li><span class="pill">Renderer</span> <a href="./part25_render_video.py">part25_render_video.py</a></li>
            <li><span class="pill">Vol. Rendering</span> <a href="./volume_rendering.py">volume_rendering.py</a> (exports <code>volrend</code> and <code>volrend_deltas</code>).</li>
            <li><span class="pill">Model</span> <a href="./nerf_model.py">nerf_model.py</a> (NeRF with PE on xyz and dir, skip at layer 4).</li>
          </ul>
        </details>
      </section>

      <section id="p26">
        <h3>2.6 Custom Dataset + Bells &amp; Whistles</h3>
        <p>
          I train on my custom dataset with focal-only intrinsics. For novel views, I render along <code>c2ws_test</code>
          or synthesize a spherical orbit. I also render grayscale depth maps and disparity visualizations, and produce
          long videos with camera-path interpolation.
        </p>

        <div class="grid">
          <figure>
            <video controls muted src="custom_outputs/custom_orbit.gif"></video>
            <figcaption>Custom dataset GIF (spherical orbit).</figcaption>
          </figure>
          <figure>
            <video controls muted src="custom_outputs/circle_long.mp4"></video>
            <figcaption>Long orbit video via path upsampling + loops.</figcaption>
          </figure>
          <figure>
            <video controls muted src="depth_video.mp4"></video>
            <figcaption>Depth video (grayscale, closer=brighter).</figcaption>
          </figure>
          <figure>
            <video controls muted src="depth_disparity.mp4"></video>
            <figcaption>Disparity visualization (1/depth) for better contrast.</figcaption>
          </figure>
          <figure>
            <video controls muted src="depth_comparison.mp4"></video>
            <figcaption>Side-by-side RGB (left) and disparity (right).</figcaption>
          </figure>
        </div>

        <details>
          <summary>Implementation details</summary>
          <ul>
            <li><span class="pill">Training (custom)</span> <a href="./part26_train_custom_nerf.py">part26_train_custom_nerf.py</a>
              builds K from focal, includes safe downsampling on MPS, and optional GIF export.</li>
            <li><span class="pill">Simple video</span> <a href="./part26_render_video_custom.py">part26_render_video_custom.py</a>
              uses <code>c2ws_test</code> if present or builds K from <code>focal</code>.</li>
            <li><span class="pill">Long video</span> <a href="./part26_render_video_custom_long.py">part26_render_video_custom_long.py</a>
              supports quaternion SLERP pose upsampling and spherical path synthesis (x/y/z axis, z-up or y-up).</li>
            <li><span class="pill">Depth renderer</span> <a href="./part_bw_render_depth.py">part_bw_render_depth.py</a>:
              <ul>
                <li>Renders RGB and depth with identical pipeline; depth = E[t] with compositing weights.</li>
                <li>Grayscale mapping with percentile-based normalization and gamma contrast; background masked by accumulated alpha.</li>
                <li>Disparity mode uses inverse depth for better contrast; optional side-by-side RGB+depth.</li>
              </ul>
            </li>
          </ul>
        </details>

        <h4>Key formulas</h4>
        <p>
          Depth composition with weights w<sub>i</sub>:
          <br/>
          <code>depth = Σ_i w_i t_i, where w_i = T_i α_i, α_i = 1 - exp(-σ_i Δ_i), T_i = Π_{j<i} (1 - α_j)</code>
        </p>
      </section>
    </section>

    <section id="appendix">
      <h2>Appendix — How to reproduce</h2>
      <ol>
        <li>Calibrate camera and estimate poses:
          <pre><code>python part01_calibrate_camera.py
python part03_find_camera_pose.py</code></pre>
        </li>
        <li>Create dataset (npz) and (optionally) resize:
          <pre><code>python part04_create_dataset.py
python resize_dataset.py</code></pre>
        </li>
        <li>Part 1 (2D field):
          <pre><code>python part1_neural_field_2d.py</code></pre>
        </li>
        <li>Part 2.5 (Lego) train + render:
          <pre><code>python part25_train_nerf.py
python part25_render_video.py --checkpoint part25_outputs/nerf_coarse_step03000.pt \
  --dataset data/lego_200x200.npz --out part25_outputs/novel_view_step03000.mp4</code></pre>
        </li>
        <li>Part 2.6 (custom) train + videos + depth:
          <pre><code># Train
python part26_train_custom_nerf.py --dataset_path dataset/my_data_resized.npz --out_dir custom_outputs --gif_after_train

# Simple path
python part26_render_video_custom.py --dataset dataset/my_data_resized.npz \
  --checkpoint custom_outputs/custom_nerf_step014000.pt --out custom_outputs/circle.mp4

# Long path (upsampled/loops)
python part26_render_video_custom_long.py --dataset dataset/my_data_resized.npz \
  --checkpoint custom_outputs/custom_nerf_step014000.pt --out custom_outputs/circle_long.mp4 \
  --use_test_path --upsample 9 --loops 3 --close_loop

# Depth videos (grayscale/disparity/side-by-side)
python part_bw_render_depth.py --checkpoint part25_outputs/nerf_coarse_step03000.pt \
  --dataset data/lego_200x200.npz --out depth_video.mp4
python part_bw_render_depth.py --checkpoint part25_outputs/nerf_coarse_step03000.pt \
  --dataset data/lego_200x200.npz --out depth_disparity.mp4 --use_disparity
python part_bw_render_depth.py --checkpoint part25_outputs/nerf_coarse_step03000.pt \
  --dataset data/lego_200x200.npz --out depth_comparison.mp4 --side_by_side --use_disparity</code></pre>
        </li>
      </ol>
      <p class="callout">
        Note: Replace image/video paths above with your actual outputs. This page references common default paths
        produced by the scripts. To satisfy the display requirements, ensure each subsection has the required
        figures (detections, undistort before/after, pose/rig visualization, PSNR curves, validation images,
        and novel-view videos).
      </p>
    </section>

    <section id="links">
      <h2>Links to source files</h2>
      <ul>
        <li><a href="./part01_calibrate_camera.py">part01_calibrate_camera.py</a></li>
        <li><a href="./part03_find_camera_pose.py">part03_find_camera_pose.py</a></li>
        <li><a href="./part04_create_dataset.py">part04_create_dataset.py</a> · <a href="./resize_dataset.py">resize_dataset.py</a></li>
        <li><a href="./part1_neural_field_2d.py">part1_neural_field_2d.py</a></li>
        <li><a href="./part21_create_rays.py">part21_create_rays.py</a></li>
        <li><a href="./part22_sampling.py">part22_sampling.py</a></li>
        <li><a href="./part23_rays_dataloader.py">part23_rays_dataloader.py</a> · <a href="./part23_visualize_2.py">part23_visualize_2.py</a></li>
        <li><a href="./volume_rendering.py">volume_rendering.py</a></li>
        <li><a href="./nerf_model.py">nerf_model.py</a></li>
        <li><a href="./part24_train_nerf.py">part24_train_nerf.py</a></li>
        <li><a href="./part25_train_nerf.py">part25_train_nerf.py</a> · <a href="./part25_render_video.py">part25_render_video.py</a></li>
        <li><a href="./part26_train_custom_nerf.py">part26_train_custom_nerf.py</a></li>
        <li><a href="./part26_render_video_custom.py">part26_render_video_custom.py</a> · <a href="./part26_render_video_custom_long.py">part26_render_video_custom_long.py</a></li>
        <li><a href="./part_bw_render_depth.py">part_bw_render_depth.py</a></li>
      </ul>
    </section>

    <footer style="margin-top:40px; color:#6a737d; font-size: 13px;">
      © 2025 Yuchen Zhang. Built for CS180/280A Project 4. Replace placeholders under assets/ with your generated figures.
    </footer>
  </div>
</body>
</html>

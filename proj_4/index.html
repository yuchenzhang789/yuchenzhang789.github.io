<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Project 4 — Neural Fields and NeRF (CS180/280A)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <!-- Simple styling (self-contained) -->
  <style>
    :root {
      --fg: #1b1f23;
      --muted: #586069;
      --bg: #ffffff;
      --accent: #0366d6;
      --border: #e1e4e8;
      --code-bg: #f6f8fa;
    }
    html, body {
      margin: 0; padding: 0; background: var(--bg); color: var(--fg);
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;
      line-height: 1.55;
    }
    .container { max-width: 980px; margin: 0 auto; padding: 32px 16px 80px; }
    header { margin-bottom: 16px; }
    h1 { font-size: 32px; margin: 0 0 8px; }
    .sub { color: var(--muted); margin: 0 0 24px; }
    h2 { margin-top: 48px; border-bottom: 1px solid var(--border); padding-bottom: 6px; }
    h3 { margin-top: 28px; }
    h4 { margin-top: 20px; }
    p { margin: 12px 0; }
    ul { margin: 8px 0 16px 20px; }
    li { margin: 6px 0; }
    a { color: var(--accent); text-decoration: none; }
    a:hover { text-decoration: underline; }
    code, pre { background: var(--code-bg); border: 1px solid var(--border); }
    code { padding: 0 4px; border-radius: 3px; }
    pre { padding: 12px; border-radius: 6px; overflow-x: auto; }
    .toc {
      border: 1px solid var(--border); background: #fafbfc; border-radius: 6px;
      padding: 12px 14px; margin: 18px 0 26px;
    }
    .grid {
      display: grid; grid-template-columns: repeat(auto-fill, minmax(240px, 1fr));
      gap: 12px; margin: 12px 0;
    }
    figure { margin: 12px 0; }
    figcaption { font-size: 13px; color: var(--muted); margin-top: 4px; }
    img, video { width: 100%; height: auto; border: 1px solid var(--border); border-radius: 6px; background: #fff; }
    .callout {
      border-left: 4px solid var(--accent); padding: 10px 14px; background: #f1f8ff; margin: 16px 0; border-radius: 4px;
    }
    details { border: 1px solid var(--border); border-radius: 6px; padding: 10px 12px; margin: 12px 0; background: #fff; }
    details > summary { cursor: pointer; font-weight: 600; }
    .pill {
      display: inline-block; font-size: 12px; padding: 2px 8px; border: 1px solid var(--border);
      border-radius: 999px; background: #f6f8fa; color: #586069; margin-right: 6px;
    }
    .two-col {
      display: grid; grid-template-columns: 1fr 1fr; gap: 12px;
    }
    @media (max-width: 820px) { .two-col { grid-template-columns: 1fr; } }
    .kbd { border: 1px solid var(--border); border-bottom-width: 3px; border-radius: 4px; padding: 0 6px; background: #f6f8fa; font-weight: 600; }
    .req { font-weight: 600; color: #22863a; }
    .xref { color: #6f42c1; }
  </style>
  <!-- MathJax for equations -->
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <div class="container">
    <header>
      <h1>Project 4 — Neural Fields and NeRF</h1>
      <p class="sub">
        Author: <strong>Yuchen Zhang</strong> · Course: CS180/280A · Fall 2025
      </p>
      <div class="toc">
        <strong>Contents</strong>
        <ul>
          <li><a href="#intro">1. Introduction</a></li>
          <li><a href="#part0">2. Part 0 — Data Acquisition &amp; Dataset</a>
            <ul>
              <li><a href="#part01">0.1 Camera Calibration</a></li>
              <li><a href="#part02">0.2 Single-Tag Camera Poses</a></li>
              <li><a href="#part03">0.3 Pose Visualization</a></li>
              <li><a href="#part04">0.4 Create NeRF Dataset (npz)</a></li>
            </ul>
          </li>
          <li><a href="#part1">3. Part 1 — 2D Neural Field</a>
            <ul>
              <li><a href="#part1-setup">1.0 Architecture &amp; Positional Encoding</a></li>
              <li><a href="#part1-provided">1.1 Provided Image</a></li>
              <li><a href="#part1-mine">1.2 My Own Image</a></li>
            </ul>
          </li>
          <li><a href="#part2">4. Part 2 — NeRF</a>
            <ul>
              <li><a href="#p21">2.1 Rays from Cameras</a></li>
              <li><a href="#p22">2.2 Sampling Along Rays</a></li>
              <li><a href="#p23">2.3 Multi-view Rays Dataloader</a></li>
              <li><a href="#p24">2.4 Training Skeleton</a></li>
              <li><a href="#p25">2.5 Full NeRF (coarse)</a></li>
              <li><a href="#p26">2.6 Custom Dataset + Extras</a></li>
            </ul>
          </li>
        </ul>
      </div>
    </header>

    <section id="intro">
      <h2>Introduction</h2>
      <p>
        This project builds a complete pipeline from data capture to neural rendering. In Part 0, I calibrate
        a camera using ArUco markers, estimate camera poses from a single square tag, visualize the rig, and
        assemble an undistorted NeRF dataset (npz) with intrinsics and camera-to-world poses. Part 1 fits a 2D
        neural field to an image using sinusoidal positional encoding, logging PSNR and visualizing improvement.
        Part 2 implements a NeRF: creating rays from cameras, sampling points along rays, composing with volume
        rendering, training on a multi-view dataset, and rendering novel views and depth/disparity videos.
      </p>
      <p class="callout">
        The implementation follows the <a href="https://cal-cs180.github.io/fa25/hw/proj4/index.html" target="_blank" rel="noopener">Project 4 spec</a>.
      </p>
    </section>

    <section id="part0">
      <h2>Part 0 — Data Acquisition &amp; Dataset</h2>

      <section id="part01">
        <h3>0.1 Camera Calibration</h3>
        <p>
          I detect ArUco tags in a 2*3 layout and calibrate with 30+ images. For each detected tag,
          I push its four 2D corners and corresponding 3D square corners into OpenCV’s <code>calibrateCamera</code>.
          I fix <code>K3</code> and zero the tangential distortion to produce a stable intrinsics estimate for NeRF.
        </p>
        <div class="two-col">
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part01_1.jpeg" alt="Sample calibration image 1" />
            <figcaption>Sample calibration image 1</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part01_2.png" alt="Sample calibration image 2" />
            <figcaption>Sample calibration image 2</figcaption>
          </figure>
        </div>

        <details>
          <summary>Implementation details (part01_calibrate_camera.py)</summary>
          <ul>
            <li>ArUco detection: <code>cv2.aruco.ArucoDetector</code> with a 2×3 grid of IDs {0..5}.</li>
            <li>Object points: <code>objp_for_tag(id, TAG_SIZE_M, H_SPACING_M, V_SPACING_M)</code> on the z=0 plane.</li>
            <li>Calibration: <code>cv2.calibrateCamera</code> with <code>CALIB_FIX_K3 | CALIB_ZERO_TANGENT_DIST</code>.</li>
            <li>Diagnostics: save <code>K, dist</code>, per-sample RMS, and reprojection error into <code>camera_calibration.npz</code>.</li>
          </ul>
          <p><span class="pill">Code</span> <a href="./part01_calibrate_camera.py">part01_calibrate_camera.py</a></p>
        </details>

        <h4>Results</h4>
        <ul>
          <li class="req">Intrinsic matrix K estimated (saved to <code>camera_calibration.npz</code>).</li>
          <li class="req">Distortion parameters recorded.</li>
          <li class="req">Mean reprojection error and per-sample RMS visualized.</li>
        </ul>
      </section>

      <section id="part02">
        <h3>0.2 Single-Tag Camera Poses</h3>
        <p>
          Here are pictures of my own labubu to be used as datasets for later parts.
        </p>
        <div class="two-col">
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part02_1.jpeg" alt="Pose solvePnP overview" />
            <figcaption>Sample image for my own labubu</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part02_2.jpeg" alt="Camera positions/quiver plot" />
            <figcaption>Sample image for my own labubu</figcaption>
          </figure>
        </div>
      </section>

      <section id="part03">
        <h3>0.3 Pose Visualization</h3>
        <p>
          For each image, I detect the specified tag ID (here I used ID 5) and solve PnP with the square-specific IPPE method to obtain
          two candidate solutions. I disambiguate by choosing the solution whose camera z is above the plane (z &gt; 0).
          Then I convert OpenCV’s w2c to c2w for NeRF.
          
          I visualize the recovered rig with <code>viser</code>, adding a frustum for each camera with the undistorted image.
          This helps verify consistent orientation and overall path coverage.
        </p>
        <figure>
          <img src="https://yuchenzhang789.github.io/proj_4/outputs/part03_1.png" alt="Viser frustums visualization" />
          <figcaption>Viser-based 3D visualization of camera frustums in world coordinates.</figcaption>
        </figure>
        <figure>
          <img src="https://yuchenzhang789.github.io/proj_4/outputs/part03_2.png" alt="Viser frustums visualization" />
          <figcaption>Viser-based 3D visualization of camera frustums in world coordinates.</figcaption>
        </figure>

        <details>
          <summary>Implementation notes</summary>
          <ul>
            <li>Detection: <code>cv2.aruco.ArucoDetector</code> → <code>corners, ids</code>.</li>
            <li>Pose: <code>cv2.solvePnPGeneric(..., SOLVEPNP_IPPE_SQUARE)</code> → choose solution by comparing
                <code>cam = -R^T t</code> z-components.</li>
            <li>c2w: set <code>R_c2w = R^T</code>, <code>t_c2w = -R^T t</code>; save each pose under <code>poses/pose_###.npz</code>.</li>
            <li>Field-of-view computed from fy: <code>fov_y = 2 atan(H/2 / fy)</code>.</li>
            <li>Frustum geometry uses c2w rotation and translation directly.</li>
          </ul>
          <p><span class="pill">Code</span> <a href="./part23_visualize_2.py">part23_visualize_2.py</a></p>
        </details>
      </section>

      <section id="part04">
        <h3>0.4 Create NeRF Dataset (npz)</h3>
        <p>
          Using the calibration and poses, I undistort all images with a single optimal <code>newK</code> computed from the
          first image, optionally crop by ROI, convert to RGB, and split into train/val/test. I store
          <code>images_train</code>, <code>c2ws_train</code>, <code>images_val</code>, <code>c2ws_val</code>,
          <code>c2ws_test</code>, and average <code>focal</code> in <code>dataset/my_data.npz</code>.
        </p>

        <details>
          <summary>Implementation details (part04_create_dataset.py)</summary>
          <ul>
            <li>Compute single <code>newK</code> and ROI from first image via <code>cv2.getOptimalNewCameraMatrix</code>.</li>
            <li>Undistort every image with <code>(K, dist) → newK</code>. Optionally crop to ROI. Convert BGR→RGB. Keep uint8.</li>
            <li>Split 70/15/15 with fixed seed; save arrays and <code>focal = (fx + fy)/2</code>.</li>
          </ul>
          <p>
            <span class="pill">Code</span> <a href="./part04_create_dataset.py">part04_create_dataset.py</a><br />
            <span class="pill">Resizer</span> <a href="./resize_dataset.py">resize_dataset.py</a>
          </p>
        </details>
      </section>
    </section>

    <section id="part1">
      <h2>Part 1 — 2D Neural Field</h2>
      <p>
        I fit a sinusoidally encoded MLP to a single RGB image. Inputs are normalized pixel coordinates
        with positional encoding up to frequency L. The network predicts RGB in [0,1]. I report PSNR over
        iterations, save reconstructions at milestones, and include a small sweep over width and L.
      </p>

      <!-- 1.0 Model & PE (NEW) -->
      <section id="part1-setup">
        <h3>1.0 Architecture &amp; Positional Encoding</h3>
        <div class="two-col">
          <figure>
            <!-- Image 1 -->
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part1_model_arch.png" alt="2D Neural Field MLP architecture" />
            <figcaption>Model architecture (MLP with PE input).</figcaption>
          </figure>
          <figure>
            <!-- Image 2 -->
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part1_pe_equation.png" alt="Positional Encoding (PE) equation" />
            <figcaption>Sinusoidal positional encoding (2D) used for coordinates.</figcaption>
          </figure>
        </div>
        <ul>
          <li><strong>Layers (depth):</strong> 3 hidden layers (total depth = 3 hidden + output).</li>
          <li><strong>Width:</strong> 256 hidden units per layer.</li>
          <li><strong>Learning rate:</strong> 1e-2 (Adam).</li>
          <li><strong>PE frequencies:</strong> L = 10 → input dim = 2 + 4L = 42.</li>
          <li><strong>Activation:</strong> ReLU in hidden layers, Sigmoid at the output.</li>
          <li><strong>Batch size:</strong> 10,000 random pixels/iteration; <strong>Iterations:</strong> 2,000 (baseline).</li>
          <li><strong>Device:</strong> Apple Silicon (MPS) with chunked full-image render (chunk = 131,072).</li>
          <li><strong>Loss:</strong> MSE, metric: PSNR (10 log10(1/MSE)).</li>
        </ul>
      </section>

      <!-- 1.1 Provided Image -->
      <section id="part1-provided">
        <h3>1.1 Provided Fox Image</h3>
        <div class="grid">
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part1_fox_1.png" alt="Iter 1 (provided)" />
            <figcaption>Iter 1</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part1_fox_2.png" alt="Iter 50 (provided)" />
            <figcaption>Iter 50</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part1_fox_3.png" alt="Iter 200 (provided)" />
            <figcaption>Iter 100</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part1_fox_4.png" alt="Iter 2000 (provided)" />
            <figcaption>Iter 200</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part1_fox_5.png" alt="Iter 2000 (provided)" />
            <figcaption>Iter 2000</figcaption>
          </figure>
        </div>

        <div class="two-col">
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part1_fox_psnr.png" alt="PSNR curve (provided)" />
            <figcaption>PSNR over iterations</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part1_fox_grid.png" alt="2x2 sweep grid (provided)" />
            <figcaption>2×2 sweep over width ∈ {64, 256} and L ∈ {4, 10}.</figcaption>
          </figure>
        </div>
      </section>

      <!-- 1.2 My Own Image -->
      <section id="part1-mine">
        <h3>1.2 My Own Image</h3>
        <div class="grid">
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part1_labubu_1.jpeg" alt="Iter 1 (mine)" />
            <figcaption>Iter 1</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part1_labubu_2.jpeg" alt="Iter 50 (mine)" />
            <figcaption>Iter 50</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part1_labubu_3.jpeg" alt="Iter 100 (mine)" />
            <figcaption>Iter 100</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part1_labubu_4.jpeg" alt="Iter 200 (mine)" />
            <figcaption>Iter 200</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part1_labubu_5.jpeg" alt="Iter 2000 (mine)" />
            <figcaption>Iter 2000</figcaption>
          </figure>
        </div>

        <div class="two-col">
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part1_labubu_psnr.png" alt="PSNR curve (mine)" />
            <figcaption>PSNR over iterations</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part1_labubu_grid.jpeg" alt="2x2 sweep grid (mine)" />
            <figcaption>2×2 sweep over width ∈ {64, 256} and L ∈ {4, 10}.</figcaption>
          </figure>
        </div>
      </section>

      <details>
        <summary>Implementation details (part1_neural_field_2d.py)</summary>
        <ul>
          <li>Positional Encoding (2D): PE(x) = [x, sin(2^k π x), cos(2^k π x)] for k=0..L-1.</li>
          <li>MLP: width=256, depth=3, Sigmoid output; MSE loss with Adam; chunked full-image render.</li>
          <li>Device selection prefers MPS, then CUDA, then CPU; optional AMP on CUDA.</li>
        </ul>
        <p><span class="pill">Code</span> <a class="xref" href="./part1_neural_field_2d.py">part1_neural_field_2d.py</a></p>
      </details>

      <h4>Observations</h4>
      <ul>
        <li>Higher L (more frequencies) captures fine detail faster but risks ringing if width is small.</li>
        <li>Width 256 significantly improves convergence vs width 64 at similar L.</li>
      </ul>
    </section>

    <section id="part2">
      <h2>4. Part 2 — NeRF</h2>

      <section id="p21">
        <h3>2.1 Rays from Cameras</h3>
        <p>
          I implement SE(3) transforms, camera intrinsics inversion, and pixel-to-ray conversion. Sanity tests
          ensure invertibility and the center ray direction for a unit pinhole camera.
        </p>
        
        <details>
          <summary>Implementation details (part21_create_rays.py)</summary>
          <ul>
            <li><code>invert_pose</code>, <code>transform</code>, <code>pixel_to_camera</code>, <code>pixel_to_ray</code>, <code>get_rays_from_image</code>.</li>
            <li>Pixel centers use +0.5 offset; directions normalized for stability.</li>
          </ul>
          <p><span class="pill">Code</span> <a href="./part21_create_rays.py">part21_create_rays.py</a></p>
        </details>
      </section>

      <section id="p22">
        <h3>2.2 Sampling Along Rays</h3>
        <p>
          I provide uniform and interval-based stratified sampling of <code>t</code> values in [near, far].
          Given ray origin <code>o</code> and direction <code>d</code>, points are <code>o + t d</code>.
        </p>
        
        <details>
          <summary>Implementation details (part22_sampling.py)</summary>
          <ul>
            <li>Two APIs: <code>sample_points_along_rays</code> (uniform) and <code>sample_points_intervals</code> (interval jitter).</li>
            <li>Both return <code>(t_vals, pts)</code> and support MPS/CUDA/CPU.</li>
          </ul>
          <p><span class="pill">Code</span> <a href="./part22_sampling.py">part22_sampling.py</a></p>
        </details>
      </section>

      <section id="p23">
        <h3>2.3 Multi-view Rays Dataloader</h3>
        <p>
          I flatten multi-view images into global buffers of UVs, image IDs, pixel colors, and rays.
          Global and per-image sampling modes are supported, with optional full precompute for speed.
        </p>
        <div class="two-col">
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_3_1.png" alt="UV indexing check" />
            <figcaption>Visualization of camera frustums, sampled rays, and 3D sample points..</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_3_2.png" alt="UV indexing check" />
            <figcaption>Visualization of camera frustums, sampled rays, and 3D sample points..</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_3_3.png" alt="UV indexing check" />
            <figcaption>Visualization of camera frustums, sampled rays, and 3D sample points..</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_3_4.png" alt="UV indexing check" />
            <figcaption>Visualization of camera frustums, sampled rays, and 3D sample points..</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_3_5.png" alt="UV indexing check" />
            <figcaption>Visualization of camera frustums, sampled rays, and 3D sample points..</figcaption>
          </figure>
        </div>
        <details>
          <summary>Implementation details (part23_rays_dataloader.py)</summary>
          <ul>
            <li><code>RaysData</code> builds rays via <code>_rays_for_image</code>; <code>sample_rays</code> supports "global" and "per_image".</li>
            
          </ul>
          <p><span class="pill">Code</span> <a href="./part23_rays_dataloader.py">part23_rays_dataloader.py</a> · <a href="./part23_visualize_2.py">part23_visualize_2.py</a></p>
        </details>
      </section>

      <section id="p24">
        <h3>2.4 Training Skeleton</h3>
        <p>
          I train a NeRF-like MLP on Lego using stratified sampling, volume rendering, and MSE loss,
          periodically rendering a validation view and logging PSNR. The model architecture is shown here, it follows the suggested architecture given by the course staffs.
        </p>
        <div class="two-col">
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_4_1.png" alt="Validation preview" />
            <figcaption>Model Architecture</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_4_2.png" alt="PSNR log" />
            <figcaption>Equation for Volume Rendering</figcaption>
          </figure>
        </div>
        <details>
          <summary>Implementation details (part24_train_nerf.py)</summary>
          <ul>
            <li>Front-to-back compositing with white background:
              <div style="margin-top:6px">
                <code>alpha_i = 1 - exp(-sigma_i * delta_i), T_i = Π_{j<i}(1 - alpha_j), w_i = T_i alpha_i, </code><br/>
                <code>C = Σ_i w_i c_i + (1 - Σ_i w_i)</code>
              </div>
            </li>
            <li>Validation builds a reduced grid and renders without jitter.</li>
            <li>Device-first selection prefers MPS for my M2 chip.</li>
          </ul>
          <p><span class="pill">Code</span> <a href="./part24_train_nerf.py">part24_train_nerf.py</a></p>
        </details>
      </section>

      <!-- UPDATED SECTION 2.5 -->
      <section id="p25">
        <h3>2.5 Full NeRF (coarse)</h3>
        <p>
          This section presents the NeRF training progression on the Lego dataset. I show:
        </p>
        <ul>
          <li>Rays & sampled points visualization (single snapshot).</li>
          <li>PSNR curve over validation checkpoints.</li>
          <li>Five rendered validation snapshots at different training iterations.</li>
          <li>Two GIFs comparing novel view quality at 1000 vs 3000 iterations.</li>
        </ul>

        <!-- Rays projection + PSNR -->
        <div class="two-col">
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_5_rays.png" alt="Rays & samples 3D projection" />
            <figcaption>Rays & sampled points visualization (camera frustums + jittered samples).</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_5_psnr.png" alt="PSNR validation curve" />
            <figcaption>Validation PSNR vs checkpoint index (coarse network).</figcaption>
          </figure>
        </div>

        <h4>Validation Snapshots During Training</h4>
        <p>
          These five images track qualitative improvement: early underfitting → sharper geometry and color
          after extended optimization.
        </p>
        <div class="snapshots-grid">
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_5_1.png" alt="Validation render at 100 iterations" />
            <figcaption>Iter 100</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_5_2.png" alt="Validation render at 200 iterations" />
            <figcaption>Iter 200</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_5_3.png" alt="Validation render at 400 iterations" />
            <figcaption>Iter 400</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_5_4.png" alt="Validation render at 800 iterations" />
            <figcaption>Iter 800</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_5_5.png" alt="Validation render at 3000 iterations" />
            <figcaption>Iter 3000</figcaption>
          </figure>
        </div>

        <h4>Novel View Comparison (GIFs)</h4>
        <p>
          Comparing the novel view path rendered at an early checkpoint (1000 iters) versus the final target
          (3000 iters). Improvements appear in sharper edges, reduced noise in low-density regions, and better
          color consistency on occluded parts.
        </p>
        <div class="two-col">
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_5_short.gif" alt="Novel view orbit at 1000 iterations" />
            <figcaption>Orbit GIF @ 1000 iterations.</figcaption>
          </figure>
          <figure>
            <img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_5_long.gif" alt="Novel view orbit at 3000 iterations" />
            <figcaption>Orbit GIF @ 3000 iterations.</figcaption>
          </figure>
        </div>

        <details>
          <summary>Implementation notes & files</summary>
          <ul>
            <li><span class="pill">Training</span> <a href="./part25_train_nerf.py">part25_train_nerf.py</a></li>
            <li><span class="pill">Renderer</span> <a href="./part25_render_video.py">part25_render_video.py</a></li>
            <li><span class="pill">Volume Rendering</span> <a href="./volume_rendering.py">volume_rendering.py</a> — uses <code>volrend_deltas</code>.</li>
            <li><span class="pill">Model</span> <a href="./nerf_model.py">nerf_model.py</a> (PE on xyz & view dir, skip layer 4).</li>
          </ul>
          <p>
            Snapshot cadence chosen to highlight early underfitting (100–800) vs later quality (3000).
            GIFs produced by rendering <code>c2ws_test</code> path with <code>downscale=2</code> for speed.
          </p>
        </details>
      </section>

      <!-- 2.6 Custom datasets -->
      <section id="p26">
        <h3>2.6 NeRF on Lafufu & Custom Datasets</h3>
        <p>
          I trained the NeRF with my own captured custom dataset using the previous model architecture. This took more iterations to reach a preferable loss compared to the Lego scene.
        </p>

        <!-- Engineering & Hyperparameter Choices (ADDED) -->
        <h4>2.6.1 — Engineering &amp; Hyperparameter Choices</h4>
        <ul>
          <li><strong>Samples per ray:</strong> <code>n_samples = 64</code> (↑ from 32 on Lego) to improve quality on my real data.</li>
          <li><strong>Model capacity:</strong> hidden width <code>W = 256</code>, depth <code>D = 8</code>, skip at layer <code>4</code>.</li>
          <li><strong>Training schedule:</strong> <code>iters = 15000</code> (custom dataset).</li>
          <li><strong>Rays per iteration:</strong> <code>rays_per_batch = 8192</code> (to keep VRAM manageable).</li>
          <li><strong>Near/Far:</strong> <code>near = 0.02</code>, <code>far = 0.5</code> (tabletop scale; reduces “air” artifacts).</li>
          <li><strong>Rendering chunk:</strong> <code>render_chunk = 8192</code> for validation/video (fits on MPS).</li>
          <li><strong>Optimizer:</strong> Adam, <strong>LR</strong> <code>5e-4</code>; cosine LR decay optional.</li>
          <li><strong>Device &amp; precision:</strong> MPS (Apple Silicon), float32; PNG frames always saved for stability.</li>
        </ul>
        <p><em>Why these choices?</em> A slightly larger sampling budget and the same-width MLP stabilize fine details on my real scene while keeping memory in check. Tighter near/far avoids translucent fog and reduces background bias. Lower render chunk avoids OOM during validation/video.</p>

        <!-- 2.6.2 Custom -->
        <section id="p26-custom">
          <h4>2.6.2 Custom Dataset</h4>
          <div class="two-col">
            <figure><img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_6_loss.png" alt="Loss for custom image" /><figcaption>Loss for custom image</figcaption></figure>
            <figure><img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_6_psnr.png" alt="PSNR for custom image" /><figcaption>PSNR for custom image</figcaption></figure>
          </div>
          <h5>Training Snapshots</h5>
          <div class="grid">
            <figure><img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_6_1.png" alt="Custom 500" /><figcaption>Iter 500</figcaption></figure>
            <figure><img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_6_2.png" alt="Custom 1000" /><figcaption>Iter 1000</figcaption></figure>
            <figure><img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_6_3.png" alt="Custom 2000" /><figcaption>Iter 2000</figcaption></figure>
            <figure><img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_6_4.png" alt="Custom 4000" /><figcaption>Iter 4000</figcaption></figure>
            <figure><img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_6_5.png" alt="Custom 15000" /><figcaption>Iter 15000</figcaption></figure>
          </div>
          <h5>Novel View Orbit GIF</h5>
          <div class="two-col">
            <figure><img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_6_gif.gif" alt="Custom orbit 15000" /><figcaption>Custom @ 15000 iters</figcaption></figure>
          </div>
          <details>
            <summary>Files (Custom)</summary>
            <ul>
              <li><span class="pill">Train</span> <a href="./part26_train_custom_nerf.py">part26_train_custom_nerf.py</a></li>
              <li><span class="pill">Render</span> <a href="./part26_render_video_custom.py">part26_render_video_custom.py</a></li>
              <li><span class="pill">Long Path</span> <a href="./part26_render_video_custom_long.py">part26_render_video_custom_long.py</a></li>
            </ul>
          </details>
        </section>
      </section>

      <!-- 2.7 Bells & Whistles depth/disparity (now GIFs) -->
      <section id="p_bw">
        <h3>2.7 Bells & Whistles — Depth / Disparity</h3>
        <p>
          Depth visualizations rendered using identical ray marching pipeline as RGB (expected depth
          = Σ w<sub>i</sub> t<sub>i</sub>). I also used different contrast to improve the video quality. Side-by-side comparisons
          highlight geometric consistency of the learned field. <strong>White</strong> indicates closer distance to the camera in the depth video.
        </p>
        <div class="two-col">
          <figure><img src="https://yuchenzhang789.github.io/proj_4/outputs/part2_5_long.gif" alt="Original Lego GIF" /><figcaption>Original Lego GIF.</figcaption></figure>
          <figure><img src="https://yuchenzhang789.github.io/proj_4/outputs/part_bw.gif" alt="Depth GIF" /><figcaption>Depth Lego GIF.</figcaption></figure>
        </div>
        <details>
          <summary>Depth Rendering Implementation</summary>
          <ul>
            <li><span class="pill">Script</span> <a href="./part_bw_render_depth.py">part_bw_render_depth.py</a></li>
            <li>Background masked using accumulated alpha threshold (≥ 0.05).</li>
            <li>Using color contrast to improve quality</li>
          </ul>
        </details>
      </section>
    </section>

    <section id="appendix">
      <h2>Appendix — How to reproduce</h2>
      <ol>
        <li>Calibrate camera and estimate poses:
          <pre><code>python part01_calibrate_camera.py
python part03_find_camera_pose.py</code></pre>
        </li>
        <li>Create dataset (npz) and (optionally) resize:
          <pre><code>python part04_create_dataset.py
python resize_dataset.py</code></pre>
        </li>
        <li>Part 1 (2D field):
          <pre><code>python part1_neural_field_2d.py</code></pre>
        </li>
        <li>Part 2.5 (Lego) train + render:
          <pre><code>python part25_train_nerf.py
python part25_render_video.py --checkpoint part25_outputs/nerf_coarse_step03000.pt \
  --dataset data/lego_200x200.npz --out part25_outputs/novel_view_step03000.mp4</code></pre>
        </li>
        <li>Part 2.6 (custom) train + videos + depth:
          <pre><code># Train
python part26_train_custom_nerf.py --dataset_path dataset/my_data_resized.npz --out_dir custom_outputs --gif_after_train

# Simple path
python part26_render_video_custom.py --dataset dataset/my_data_resized.npz \
  --checkpoint custom_outputs/custom_nerf_step014000.pt --out custom_outputs/circle.mp4

# Long path (upsampled/loops)
python part26_render_video_custom_long.py --dataset dataset/my_data_resized.npz \
  --checkpoint custom_outputs/custom_nerf_step014000.pt --out custom_outputs/circle_long.mp4 \
  --use_test_path --upsample 9 --loops 3 --close_loop

# Depth videos (grayscale/disparity/side-by-side)
python part_bw_render_depth.py --checkpoint part25_outputs/nerf_coarse_step03000.pt \
  --dataset data/lego_200x200.npz --out depth_video.mp4
python part_bw_render_depth.py --checkpoint part25_outputs/nerf_coarse_step03000.pt \
  --dataset data/lego_200x200.npz --out depth_disparity.mp4 --use_disparity
python part_bw_render_depth.py --checkpoint part25_outputs/nerf_coarse_step03000.pt \
  --dataset data/lego_200x200.npz --out depth_comparison.mp4 --side_by_side --use_disparity</code></pre>
        </li>
      </ol>
    </section>

    <section id="links">
      <h2>Links to source files</h2>
      <ul>
        <li><a href="./part01_calibrate_camera.py">part01_calibrate_camera.py</a></li>
        <li><a href="./part03_find_camera_pose.py">part03_find_camera_pose.py</a></li>
        <li><a href="./part04_create_dataset.py">part04_create_dataset.py</a> · <a href="./resize_dataset.py">resize_dataset.py</a></li>
        <li><a href="./part1_neural_field_2d.py">part1_neural_field_2d.py</a></li>
        <li><a href="./part21_create_rays.py">part21_create_rays.py</a></li>
        <li><a href="./part22_sampling.py">part22_sampling.py</a></li>
        <li><a href="./part23_rays_dataloader.py">part23_rays_dataloader.py</a> · <a href="./part23_visualize_2.py">part23_visualize_2.py</a></li>
        <li><a href="./volume_rendering.py">volume_rendering.py</a></li>
        <li><a href="./nerf_model.py">nerf_model.py</a></li>
        <li><a href="./part24_train_nerf.py">part24_train_nerf.py</a></li>
        <li><a href="./part25_train_nerf.py">part25_train_nerf.py</a> · <a href="./part25_render_video.py">part25_render_video.py</a></li>
        <li><a href="./part26_train_custom_nerf.py">part26_train_custom_nerf.py</a></li>
        <li><a href="./part26_render_video_custom.py">part26_render_video_custom.py</a> · <a href="./part26_render_video_custom_long.py">part26_render_video_custom_long.py</a></li>
        <li><a href="./part_bw_render_depth.py">part_bw_render_depth.py</a></li>
      </ul>
    </section>

    <footer style="margin-top:40px; color:#6a737d; font-size: 13px;">
      © 2025 Yuchen Zhang. Built for CS180/280A Project 4. Replace placeholders under outputs/ with your generated figures.
    </footer>
  </div>
</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Image Processing Project Deliverables</title>
    <!-- Load Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@100..900&display=swap" rel="stylesheet">
    <style>
        /* Custom font import and application */
        body {
            font-family: 'Inter', sans-serif;
        }
        /* Custom scrollbar for aesthetics */
        ::-webkit-scrollbar {
            width: 8px;
        }
        ::-webkit-scrollbar-thumb {
            background: #475569; /* Slate 600 */
            border-radius: 4px;
        }
        ::-webkit-scrollbar-thumb:hover {
            background: #64748b; /* Slate 500 */
        }
        
        /* === Custom styles for image zoom on hover === */
        .image-zoom-container {
            /* Ensures the element can be lifted above others and enables smooth transition */
            position: relative; 
            cursor: zoom-in;
            transition: transform 0.4s ease-in-out, box-shadow 0.4s ease-in-out;
            z-index: 10; /* Base Z-index */
        }

        .image-zoom-container:hover {
            /* Zoom in 2x and add a pronounced shadow */
            transform: scale(2.0); 
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.7);
            z-index: 50; /* Bring to front aggressively */
        }
        /* ============================================== */
    </style>
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    colors: {
                        'background': '#0f172a', // Slate 900
                        'primary': '#334155',    // Slate 700
                        'accent': '#3b82f6',     // Blue 500
                    }
                }
            }
        }
    </script>
</head>
<body class="bg-background text-slate-300 min-h-screen">

    <header class="bg-primary shadow-lg p-6 sticky top-0 z-10">
        <div class="max-w-7xl mx-auto flex justify-between items-center">
            <!-- START: Button and Title Group -->
            <div class="flex items-center space-x-4">
                <a href="https://yuchenzhang789.github.io/index.html" class="px-3 py-1 bg-accent hover:bg-blue-600 text-white text-sm font-semibold rounded-lg transition duration-150 shadow-md">
                    ← Back to Main Webpage
                </a>
                <h1 class="text-3xl font-extrabold text-white">Image Analysis Project</h1>
            </div>
            <!-- END: Button and Title Group -->
            <p class="text-sm text-slate-400">Computer Vision Deliverables Showcase</p>
        </div>
    </header>

    <main class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-12 space-y-16">

        <!-- ==================================================================== -->
        <!-- SECTION 1: Part 1: Filters and Edges -->
        <!-- ==================================================================== -->
        <section class="bg-primary/50 p-8 rounded-xl shadow-2xl border-t-4 border-accent" id="part1">
            <h2 class="text-4xl font-bold mb-8 text-accent">Part 1: Filters and Edges</h2>

            <!-- 1.1 Convolution Implementation (RESTRUCTURED) -->
            <div class="mb-12 border-b border-slate-700 pb-8" id="part1-1">
                <h3 class="text-3xl font-bold mb-6 text-white border-b border-slate-600 pb-2">1.1. Convolution Implementations & Comparison</h3>
                <p class="mb-8">This section compares three convolution methods—a slow **4-loop** approach, a faster **2-loop** (optimized NumPy) method, and the highly optimized **Scipy built-in** function—across different types of kernels. The corresponding code implementations and analysis are provided in a separate subsection below.</p>

                <!-- 1.1.1 9x9 Box Filter -->
                <div class="mb-10 p-6 bg-slate-700/30 rounded-lg shadow-inner" id="filter-box">
                    <h4 class="text-2xl font-semibold mb-6 text-accent">1.1.1. 9x9 Box Filter (Smoothing) Results</h4>
                    
                    <div class="grid md:grid-cols-2 lg:grid-cols-3 gap-4">
                        <figure class="rounded-lg shadow-xl bg-primary image-zoom-container">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/1_1_portrait2box9forloop_4.jpg" alt="Convolution result using 4 nested for loops for the box filter" class="w-full h-auto">
                            <figcaption class="p-3 text-center text-sm">Result: 4-Loop Implementation (11.457 seconds)</figcaption>
                        </figure>
                        <figure class="rounded-lg shadow-xl bg-primary image-zoom-container">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/1_1_portrait2box9forloop_2.jpg" alt="Convolution result using 2 for loops and numpy slicing for the box filter" class="w-full h-auto">
                            <figcaption class="p-3 text-center text-sm">Result: 2-Loop Implementation (1.774 seconds)</figcaption>
                        </figure>
                        <figure class="rounded-lg shadow-xl bg-primary image-zoom-container">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/1_1_portrait2box9scipy.jpg" alt="Convolution result using scipy.signal.convolve2d for the box filter" class="w-full h-auto">
                            <figcaption class="p-3 text-center text-sm">Result: Scipy Built-in (0.458 seconds)</figcaption>
                        </figure>
                    </div>

                    <!-- CONSOLIDATED SUMMARY BLOCK REMOVED FROM HERE -->
                </div>

                <!-- 1.1.2 Dx Filter -->
                <div class="mb-10 p-6 bg-slate-700/30 rounded-lg shadow-inner" id="filter-dx">
                    <h4 class="text-2xl font-semibold mb-6 text-accent">1.1.2. Dx Filter (Partial Derivative in X) Results</h4>
                    
                    <div class="grid md:grid-cols-2 lg:grid-cols-3 gap-4">
                        <figure class="rounded-lg shadow-xl bg-primary image-zoom-container">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/1_1_portrait2dx9forloop_4.jpg" alt="Convolution result using 4 nested for loops for the Dx filter" class="w-full h-auto">
                            <figcaption class="p-3 text-center text-sm">Result: 4-Loop Implementation (0.688 seconds)</figcaption>
                        </figure>
                        <figure class="rounded-lg shadow-xl bg-primary image-zoom-container">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/1_1_portrait2dx9forloop_2.jpg" alt="Convolution result using 2 for loops and numpy slicing for the Dx filter" class="w-full h-auto">
                            <figcaption class="p-3 text-center text-sm">Result: 2-Loop Implementation (0.905 seconds)</figcaption>
                        </figure>
                        <figure class="rounded-lg shadow-xl bg-primary image-zoom-container">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/1_1_portrait2dx9scipy.jpg" alt="Convolution result using scipy.signal.convolve2d for the Dx filter" class="w-full h-auto">
                            <figcaption class="p-3 text-center text-sm">Result: Scipy Built-in (0.390 seconds)</figcaption>
                        </figure>
                    </div>
                </div>

                <!-- 1.1.3 Dy Filter -->
                <div class="mb-10 p-6 bg-slate-700/30 rounded-lg shadow-inner" id="filter-dy">
                    <h4 class="text-2xl font-semibold mb-6 text-accent">1.1.3. Dy Filter (Partial Derivative in Y) Results</h4>
                    
                    <div class="grid md:grid-cols-2 lg:grid-cols-3 gap-4">
                        <figure class="rounded-lg shadow-xl bg-primary image-zoom-container">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/1_1_portrait2dy9forloop_4.jpg" alt="Convolution result using 4 nested for loops for the Dy filter" class="w-full h-auto">
                            <figcaption class="p-3 text-center text-sm">Result: 4-Loop Implementation (0.302 seconds)</figcaption>
                        </figure>
                        <figure class="rounded-lg shadow-xl bg-primary image-zoom-container">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/1_1_portrait2dy9forloop_2.jpg" alt="Convolution result using 2 for loops and numpy slicing for the Dy filter" class="w-full h-auto">
                            <figcaption class="p-3 text-center text-sm">Result: 2-Loop Implementation (0.975 seconds)</figcaption>
                        </figure>
                        <figure class="rounded-lg shadow-xl bg-primary image-zoom-container">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/1_1_portrait2dy9scipy.jpg" alt="Convolution result using scipy.signal.convolve2d for the Dy filter" class="w-full h-auto">
                            <figcaption class="p-3 text-center text-sm">Result: Scipy Built-in (0.423 seconds)</figcaption>
                        </figure>
                    </div>
                </div>
                
                <!-- 1.1.4 Code Snippets and Analysis (UPDATED SECTION) -->
                <div class="p-6 bg-slate-700/30 rounded-lg shadow-inner border-t border-slate-600" id="code-snippets">
                    <h4 class="text-2xl font-semibold mb-6 text-accent">1.1.4. Code Implementations, Runtime, and Boundary Analysis</h4>
                    
                    <!-- CONSOLIDATED SUMMARY BLOCK (MOVED HERE) -->
                    <div class="mt-2 p-4 bg-slate-800 rounded-lg">
                        <h5 class="text-xl font-medium text-slate-200 mb-2">Runtime and Boundary Analysis</h5>
                        <p class="text-sm">The runtime analysis across all kernel sizes (9x9 Box, Dx/Dy) established a clear performance hierarchy: the average runtime showed that the **4-loop method** was the **slowest** (e.g., $11.457s$ for 9x9), the **2-loop method** (using optimized NumPy slicing/strides) was significantly **faster** (e.g., $1.774s$ for 9x9), and the **Scipy Built-in** function was consistently the **fastest** (e.g., $0.458s$ for 9x9). We also observed that a **larger filter size**, such as the **9x9 box filter**, necessitates a much **longer runtime** compared to the smaller derivative filters. Functionally, all three methods produced identical results, confirming that the boundary conditions were robustly handled in all cases using **zero-padding** on the edge pixels.</p>
                    </div>
                    <!-- END CONSOLIDATED SUMMARY BLOCK -->

                    <h5 class="text-xl font-medium text-slate-200 mt-6 mb-4">Code Snippets Visualization</h5>
                    <p class="mb-4 text-sm text-slate-400">Screenshots of the Python code demonstrating the convolution function implementation using 4-loop, 2-loop, and Scipy.</p>
                    
                    <div class="grid md:grid-cols-3 gap-4">
                        <figure class="rounded-lg shadow-xl bg-primary border-4 border-slate-500 image-zoom-container">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/1_1_code_forloop4.jpg" alt="Screenshot of the code snippet for the box filter implementation" class="w-full h-auto">
                            <figcaption class="p-3 text-center text-sm font-semibold text-sky-300">4 for loop Implementation</figcaption>
                        </figure>
                        <figure class="rounded-lg shadow-xl bg-primary border-4 border-slate-500 image-zoom-container">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/1_1_code_forloop_2.jpg" alt="Screenshot of the code snippet for the Dx filter implementation" class="w-full h-auto">
                            <figcaption class="p-3 text-center text-sm font-semibold text-sky-300">2 for loop Implementation</figcaption>
                        </figure>
                        <figure class="rounded-lg shadow-xl bg-primary border-4 border-slate-500 image-zoom-container">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/1_1_code_scipy.jpg" alt="Screenshot of the code snippet for the Dy filter implementation" class="w-full h-auto">
                            <figcaption class="p-3 text-center text-sm font-semibold text-sky-300">Built-in Scipy Implementation</figcaption>
                        </figure>
                    </div>
                </div>

            </div>

            <!-- 1.2 Partial Derivatives and Edges -->
            <div class="mb-12 border-b border-slate-700 pb-8" id="part1-2">
                <h3 class="text-2xl font-semibold mb-4 text-white">1.2. Partial Derivatives, Gradient, and Edges</h3>
                <p class="mb-6">Using finite difference filters (e.g., [-1, 0, 1]), we computed the image gradient, magnitude, and binarized edges.</p>
                
                <div class="grid md:grid-cols-2 lg:grid-cols-4 gap-4">
                    <figure class="rounded-lg shadow-xl bg-primary image-zoom-container">
                        <img src="https://yuchenzhang789.github.io/proj_2/output/1_2_cameraman_dx.jpg" alt="Partial derivative in X direction" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">Partial Derivative in x</figcaption>
                    </figure>
                    <figure class="rounded-lg shadow-xl bg-primary image-zoom-container">
                        <img src="https://yuchenzhang789.github.io/proj_2/output/1_2_cameraman_dy.jpg" alt="Partial derivative in Y direction" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">Partial Derivative in y</figcaption>
                    </figure>
                    <figure class="rounded-lg shadow-xl bg-primary image-zoom-container">
                        <img src="https://yuchenzhang789.github.io/proj_2/output/1_2_cameraman_gradient_magnitude.jpg" alt="Gradient Magnitude Image" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">Gradient Magnitude Image</figcaption>
                    </figure>
                    <figure class="rounded-lg shadow-xl bg-primary image-zoom-container">
                        <img src="https://yuchenzhang789.github.io/proj_2/output/1_2_cameraman_edge_map.jpg" alt="Binarized Edge Image" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">Binarized Edge Image (Thresholded = 0.31)</figcaption>
                    </figure>
                </div>

                <div class="mt-6 p-4 bg-slate-800 rounded-lg">
                    <h4 class="text-xl font-medium text-slate-200 mb-2">Edge Threshold Justification</h4>
                    <p class="text-sm">I chose a threshold of **0.31** for binarization. This value represents a minor tradeoff: it effectively removes background noise while still retaining important structural details in the cameraman's image.</p>
                </div>
            </div>

            <!-- 1.3 Gaussian and DoG Filters -->
            <div class="mb-12 border-b border-slate-700 pb-8" id="part1-3">
                <h3 class="text-2xl font-semibold mb-4 text-white">1.3. Gaussian and Difference of Gaussian (DoG) Filters</h3>
                <p class="mb-6">I constructed Gaussian filters using cv2.getGaussianKernel and built DoG filters by taking the deriavitives of the Gaussian filter. I set the sigma to be 1 and edge threshold as 0.28 to preserve details while removing noises.</p>

                <h4 class="text-xl font-medium text-slate-200 mb-4">Filter Visualizations and Applications ($\sigma=3$ for Gaussian, $\sigma_1=1, \sigma_2=5$ for DoG)</h4>
                <div class="grid md:grid-cols-3 gap-4">
                    <figure class="rounded-lg shadow-xl bg-primary image-zoom-container">
                        <!-- UPDATED URL for Gaussian Filter Visualization -->
                        <img src="https://yuchenzhang789.github.io/proj_2/output/1_3_gaussian_filter.jpg" alt="Visualization of the Gaussian Filter" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">Gaussian Filter Visualization</figcaption>
                    </figure>
                    <figure class="rounded-lg shadow-xl bg-primary image-zoom-container">
                        <!-- UPDATED URL for DoG Filter Visualization (using Dx component) -->
                        <img src="https://yuchenzhang789.github.io/proj_2/output/1_3_dog_x.jpg" alt="Visualization of the DoG Filter" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">DoG Filter Visualization (X Component)</figcaption>
                    </figure>
                    <figure class="rounded-lg shadow-xl bg-primary image-zoom-container">
                        <!-- UPDATED URL for DoG Filter Visualization (using Dx component) -->
                        <img src="https://yuchenzhang789.github.io/proj_2/output/1_3_dog_y.jpg" alt="Visualization of the DoG Filter" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">DoG Filter Visualization (Y Component)</figcaption>
                    </figure>
                    <figure class="rounded-lg shadow-xl bg-primary image-zoom-container">
                        <!-- UPDATED URL for Cameraman Smoothed (Gaussian) -->
                        <img src="https://yuchenzhang789.github.io/proj_2/output/1_3_cameraman_gaussian.jpg" alt="Cameraman smoothed with Gaussian" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">Cameraman Smoothed with Gaussian Filter</figcaption>
                    </figure>
                    <figure class="rounded-lg shadow-xl bg-primary image-zoom-container">
                        <!-- UPDATED URL for Cameraman Edges (DoG result) -->
                        <img src="https://yuchenzhang789.github.io/proj_2/output/1_3_cameraman_dog.jpg" alt="Cameraman edges detected with DoG" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">Cameraman Edges (One step approach)</figcaption>
                    </figure>
                    <figure class="rounded-lg shadow-xl bg-primary image-zoom-container">
                        <!-- UPDATED URL for Cameraman Edges (Finite Difference Comparison) -->
                        <img src="https://yuchenzhang789.github.io/proj_2/output/1_3_cameraman_edge_map.jpg" alt="Cameraman edges detected with Finite Difference" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">Cameraman Edges (Two step approach)</figcaption>
                    </figure>
                </div>
                <div class="mt-6 p-4 bg-slate-800 rounded-lg">
                    <h4 class="text-xl font-medium text-slate-200 mb-2">Comparison of Edge Detection</h4>
                    <p class="text-sm">The Finite Difference method is highly sensitive to noise, producing thick, noisy edges. The **DoG filter**, due to its band-pass nature, inherently incorporates smoothing, resulting in thinner, cleaner, and better localized edges.</p>
                    <p class="text-sm">In addition, I have shown the edge map created in two approaches: 1. One step approach: the image is convolved directly with DoG. 2. Two steps approach: the image in first convolved with Gaussian filter the computed the gradients to find the edges. The two resulted images are identical, proving the mathematical equvalence of two approaches.</p>
                </div>
            </div>

            <!-- 1.4 Bells and Whistles: Gradient Orientations -->
            <div id="part1-bw">
                <h3 class="text-2xl font-semibold mb-4 text-white border-b border-accent pb-2">Bells and Whistles: Gradient Orientations</h3>
                <p class="mb-6">I computed the gradient orientation for every pixel using the arctangent function without relying on built-in angle functions.</p>
                
                <div class="grid md:grid-cols-2 gap-4">
                    <figure class="rounded-lg shadow-xl bg-primary image-zoom-container">
                        <!-- UPDATED URL for Gradient Orientation Visualization -->
                        <img src="https://yuchenzhang789.github.io/proj_2/output/1_bw_cameraman_color_gradient.jpg" alt="Visualization of Gradient Orientations" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">Cameraman Gradient Orientation Map</figcaption>
                    </figure>
                    <div class="bg-slate-800 p-4 rounded-lg">
                        <h4 class="text-xl font-medium text-slate-200 mb-2">Process Explanation</h4>
                        <ul class="list-disc list-inside space-y-2 text-sm">
                            <li>Compute partial derivatives (dx, dy) in 2d space and use np.arctan2 to find the vector direction.</li>
                            <li>Implemente a hue by mapping the ratio of partial derivatives into an angle in the range [0, 2\pi)</li>
                            <li>Stack the hue with gradient magnitude to convert to rgb using skcolor.hsv2rgb to show the image in rgb channels.</li>
                        </ul>
                    </div>
                </div>
            </div>

        </section>

        <!-- ==================================================================== -->
        <!-- SECTION 2: Part 2: Applications -->
        <!-- ==================================================================== -->
        <section class="bg-primary/50 p-8 rounded-xl shadow-2xl border-t-4 border-accent" id="part2">
            <h2 class="text-4xl font-bold mb-8 text-accent">Part 2: Applications</h2>

            <!-- 2.1 Image Sharpening -->
            <div class="mb-12 border-b border-slate-700 pb-8" id="part2-1">
                <h3 class="text-2xl font-semibold mb-4 text-white">2.1. Image Sharpening</h3>
                <p class="mb-4">This works by generating a high-frequency detail layer (the 'mask') and adding it back to the original image. The process is:</p>
                <div class="bg-slate-800 p-4 rounded-lg mb-6">
                    <code class="block text-accent font-mono text-sm">Sharpened = Original + alpha * (Original - Blurred)</code>
                </div>
                <p class="mb-6">The blurred version is obtained via applying a Gaussian filter, which removes high frequencies. Subtracting the blurred image from the original isolates the high-frequency components. Multiplying this by the amplification factor alpha to control the sharpening intensity.</p>

                <h4 class="text-xl font-medium text-slate-200 mb-4">Taj Mahal Example (alpha=1.0)</h4>
                <div class="grid md:grid-cols-4 gap-4">
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://yuchenzhang789.github.io/proj_2/data/taj.jpg" alt="Original Taj Mahal" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">Original Image</figcaption>
                    </figure>
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://yuchenzhang789.github.io/proj_2/output/2_1_taj_blurred.jpg" alt="Blurred Taj Mahal" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">Blurred</figcaption>
                    </figure>
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://yuchenzhang789.github.io/proj_2/output/2_1_taj_high_freq.jpg" alt="High frequency mask of Taj Mahal" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">High-Frequency</figcaption>
                    </figure>
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://yuchenzhang789.github.io/proj_2/output/2_1_taj_sharped.jpg" alt="Sharpened Taj Mahal" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">Sharpened</figcaption>
                    </figure>
                </div>
                
                <h4 class="text-xl font-medium text-slate-200 mt-8 mb-4">Berkeley Example (alpha=1.0)</h4>
                <div class="grid md:grid-cols-4 gap-4">
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://yuchenzhang789.github.io/proj_2/data/berkeley.jpg" alt="Original Taj Mahal" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">Original Image</figcaption>
                    </figure>
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://yuchenzhang789.github.io/proj_2/output/2_1_berkeley_blurred.jpg" alt="Blurred Taj Mahal" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">Blurred</figcaption>
                    </figure>
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://yuchenzhang789.github.io/proj_2/output/2_1_berkeley_high_freq.jpg" alt="High frequency mask of Taj Mahal" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">High-Frequency</figcaption>
                    </figure>
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://yuchenzhang789.github.io/proj_2/output/2_1_berkeley_sharped.jpg" alt="Sharpened Taj Mahal" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">Sharpened</figcaption>
                    </figure>
                </div>

                <!-- START: New Custom Image Set 2 -->
                <h4 class="text-xl font-medium text-slate-200 mt-8 mb-4">Varying Sharpening Amount (alpha)</h4>
                <div class="grid md:grid-cols-4 gap-4">
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://yuchenzhang789.github.io/proj_2/output/2_1_taj_sharped_0.5.jpg" alt="Original Taj Mahal" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">Alpha = 0.5</figcaption>
                    </figure>
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://yuchenzhang789.github.io/proj_2/output/2_1_taj_sharped.jpg" alt="Blurred Taj Mahal" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">Alpha = 1.0</figcaption>
                    </figure>
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://yuchenzhang789.github.io/proj_2/output/2_1_taj_sharped_3.jpg" alt="High frequency mask of Taj Mahal" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">Alpha = 3.0</figcaption>
                    </figure>
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://yuchenzhang789.github.io/proj_2/output/2_1_taj_sharped_6.jpg" alt="Sharpened Taj Mahal" class="w-full h-auto">
                        <figcaption class="p-3 text-center text-sm">Alpha = 6.0</figcaption>
                    </figure>
                </div>
                <!-- END: New Custom Image Set 2 -->
            </div>

            <!-- 2.2 Hybrid Images -->
            <div class="mb-12 border-b border-slate-700 pb-8" id="part2-2">
                <h3 class="text-2xl font-semibold mb-4 text-white">2.2. Hybrid Images (Blending High and Low Frequencies)</h3>
                <p class="mb-6">Hybrid images are created by blending the high spatial frequencies of one image with the low spatial frequencies of another. This allows the perception to shift based on viewing distance.</p>

                <h4 class="text-xl font-medium text-slate-200 mb-4">Hybrid 1: DerekPicture (High-Pass) + Nutmeg (Low-Pass) Process</h4>
                
                <!-- START UPDATED FULL PROCESS VISUALIZATION -->
                <div class="bg-slate-800 p-4 rounded-lg mb-6">
                    <h5 class="text-lg font-medium text-slate-200 mb-4 border-b border-slate-600 pb-2">Full Process Visualization (sigma_1=9, sigma_2=15)</h5>
                    <div class="grid grid-cols-2 sm:grid-cols-4 gap-4">
                        <!-- Row 1: Inputs and Initial Processing -->
                        <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                            <img src="https://yuchenzhang789.github.io/proj_2/data/nutmeg.jpg" alt="Original Derek Picture" class="w-full h-auto">
                            <figcaption class="p-2 text-center text-xs">Original 1 (High-Pass)</figcaption>
                        </figure>
                        <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                            <img src="https://yuchenzhang789.github.io/proj_2/data/DerekPicture.jpg" alt="Original Nutmeg Picture" class="w-full h-auto">
                            <figcaption class="p-2 text-center text-xs">Original 2 (Low-Pass)</figcaption>
                        </figure>
                        <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/2_2_fourier_img1_fft.jpg" alt="Fourier Transform of Original 1" class="w-full h-auto">
                            <figcaption class="p-2 text-center text-xs">FT: Image 1</figcaption>
                        </figure>
                        <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/2_2_fourier_img2_fft.jpg" alt="Fourier Transform of Original 2" class="w-full h-auto">
                            <figcaption class="p-2 text-center text-xs">FT: Image 2</figcaption>
                        </figure>

                        <!-- Row 2: Filtered Results and Final Output -->
                        <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/2_2_fourier_im1_high_fft.jpg" alt="High-Pass filtered Derek" class="w-full h-auto">
                            <figcaption class="p-2 text-center text-xs">Filtered 1 (High-Pass)</figcaption>
                        </figure>
                        <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/2_2_fourier_im2_low_fft.jpg" alt="Low-Pass filtered Nutmeg" class="w-full h-auto">
                            <figcaption class="p-2 text-center text-xs">Filtered 2 (Low-Pass)</figcaption>
                        </figure>
                        <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/2_2_fourier_hybrid_fft.jpg" alt="Fourier Transform of the Hybrid Image" class="w-full h-auto">
                            <figcaption class="p-2 text-center text-xs">FT: Final Hybrid</figcaption>
                        </figure>
                        <figure class="rounded-lg overflow-hidden shadow-xl bg-primary border-2 border-accent">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/2_2_nutmeg_DerekPicture_hybrid.jpg" alt="Final Hybrid Image" class="w-full h-auto">
                            <figcaption class="p-2 text-center text-xs font-bold text-accent">FINAL HYBRID</figcaption>
                        </figure>
                    </div>

                    <!-- Cutoff Choice Justification (NEW) -->
                    <div class="mt-4 p-3 bg-slate-700/50 rounded-lg">
                        <h6 class="text-sm font-semibold text-slate-300 mb-1">Cutoff Frequency Justification</h6>
                        <p class="text-xs text-slate-400">The cutoff frequencies were set to sigma_1 = 9 (for high-pass Nutmeg) and sigma_2 = 15 (for low-pass Derek). These values were chosen empirically. sigma_1 needs to be large enough to ensure the cat's low-frequency components are removed so the image is nearly invisible from afar. sigma_2 must be small enough to retain sufficient low-frequency detail of Derek for it to be visible from a distance, while its high frequencies are fully removed to avoid blurring interference when viewed up close.</p>
                    </div>
                </div>
                <!-- END UPDATED FULL PROCESS VISUALIZATION -->

                <h4 class="text-xl font-medium text-slate-200 mb-4">Custom Hybrid Images</h4>
                <div class="grid md:grid-cols-2 gap-8">
                    <!-- Hybrid 2 -->
                    <div class="space-y-3 bg-slate-800 p-4 rounded-lg">
                        <h5 class="text-lg font-medium text-slate-200">Hybrid 2: Cat + Selfie (sigma_1=9, sigma_2=15)</h5>
                        <div class="grid grid-cols-3 gap-2">
                            <figure class="overflow-hidden rounded-md">
                                <img src="https://yuchenzhang789.github.io/proj_2/data/toro.jpg" alt="Original Cat" class="w-full h-auto">
                            </figure>
                            <figure class="overflow-hidden rounded-md">
                                <img src="https://yuchenzhang789.github.io/proj_2/data/selfie.jpg" alt="Original Dog" class="w-full h-auto">
                            </figure>
                            <figure class="overflow-hidden rounded-md border-2 border-accent">
                                <img src="https://yuchenzhang789.github.io/proj_2/output/2_2_toro_selfie_hybrid_color_color.jpg" alt="Hybrid CatDog Image" class="w-full h-auto">
                            </figure>
                        </div>
                    </div>
                    <!-- Hybrid 3 -->
                    <div class="space-y-3 bg-slate-800 p-4 rounded-lg">
                        <h5 class="text-lg font-medium text-slate-200">Hybrid 3: Labubu + Actor (sigma_1=7, sigma_2=12)</h5>
                        <div class="grid grid-cols-3 gap-2">
                            <figure class="overflow-hidden rounded-md">
                                <img src="https://yuchenzhang789.github.io/proj_2/data/labubu.jpg" alt="Original Bird" class="w-full h-auto">
                            </figure>
                            <figure class="overflow-hidden rounded-md">
                                <img src="https://yuchenzhang789.github.io/proj_2/data/portrait2.jpg" alt="Original Plane" class="w-full h-auto">
                            </figure>
                            <figure class="overflow-hidden rounded-md border-2 border-accent">
                                <img src="https://yuchenzhang789.github.io/proj_2/output/2_2_labubu_portrait2_hybrid.jpg" alt="Hybrid PlaneBird Image" class="w-full h-auto">
                            </figure>
                        </div>
                    </div>
                </div>
                
                <!-- Bells and Whistles: Color Exploration (UPDATED) -->
                <div class="mt-6 p-4 bg-slate-900 rounded-lg border border-slate-700">
                    <h4 class="text-xl font-medium text-accent mb-4">Bells and Whistles: Color Exploration in Hybrid Images</h4>

                    <h5 class="text-lg font-medium text-slate-200 mb-3">Color Combinations</h5>
                    <div class="grid grid-cols-2 lg:grid-cols-4 gap-4">
                        <!-- Gray-Gray -->
                        <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/2_2_toro_selfie_hybrid_gray_gray.jpg" alt="Hybrid image combining two grayscale images" class="w-full h-auto">
                            <figcaption class="p-2 text-center text-xs">Gray (High) + Gray (Low)</figcaption>
                        </figure>
                        <!-- Gray-Color -->
                        <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/2_2_toro_selfie_hybrid_gray_color.jpg" alt="Hybrid image combining grayscale high-pass with color low-pass" class="w-full h-auto">
                            <figcaption class="p-2 text-center text-xs">Gray (High) + Color (Low)</figcaption>
                        </figure>
                        <!-- Color-Gray -->
                        <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/2_2_toro_selfie_hybrid_color_gray.jpg" alt="Hybrid image combining color high-pass with grayscale low-pass" class="w-full h-auto">
                            <figcaption class="p-2 text-center text-xs">Color (High) + Gray (Low)</figcaption>
                        </figure>
                        <!-- Color-Color -->
                        <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                            <img src="https://yuchenzhang789.github.io/proj_2/output/2_2_toro_selfie_hybrid_color_color.jpg" alt="Hybrid image combining two color images" class="w-full h-auto">
                            <figcaption class="p-2 text-center text-xs">Color (High) + Color (Low)</figcaption>
                        </figure>
                    </div>

                    <div class="mt-6 p-4 bg-slate-800 rounded-lg">
                        <h5 class="text-lg font-medium text-slate-200 mb-2">Analysis of Color Blending</h5>
                        <ul class="list-disc list-inside space-y-2 text-sm text-slate-400">
                            <li><strong>Gray + Gray:</strong> This yields a purely luminance-based hybrid. The perceptual shift is solely dependent on spatial frequency and viewing distance, offering a clean, traditional result.</li>
                            <li><strong>Gray + Color (Low-Pass):</strong> This provides the **best perceptual result**. The color of the low-frequency image (visible from afar) dominates the distant view, while the high-frequency image (seen up close) is perceived clearly in black and white, avoiding confusing chromatic fringing.</li>
                            <li><strong>Color (High-Pass) + Gray:</strong> Up close, the high-frequency image is seen in color, but from a distance, the gray low-frequency image loses its visual impact, making the distant perception less engaging.</li>
                            <li><strong>Color + Color:</strong> This often results in a poor hybrid due to **chromatic aberration** and **color leakage**. When viewed from afar, the high-frequency image's color channels interfere with the low-frequency image, leading to messy, indistinct colors or blurry edges when the eye tries to fuse the two.</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- 2.3 + 2.4 Multi-resolution Blending -->
            <div id="part2-3-4">
                <h3 class="text-2xl font-semibold mb-4 text-white">2.3 + 2.4. Multi-resolution Blending</h3>
                <p class="mb-6">Multi-resolution blending uses Laplacian stacks to decompose two input images, $I_A$ and $I_B$, and a Gaussian stack of a mask $M$ to blend the images at each frequency level, resulting in a seamless composite image.</p>
                <code class="block text-accent font-mono text-sm bg-slate-800 p-3 rounded-lg mb-6">Blended Stack Level $l = L_{A,l} \cdot G_{M,l} + L_{B,l} \cdot (1 - G_{M,l})$</code>

                <h4 class="text-xl font-medium text-slate-200 mb-4">Orange + Apple: Stacks Visualization ($N=5$ levels)</h4>
                <div class="grid grid-cols-3 gap-4 mb-8">
                    <!-- Stack Headers -->
                    <div class="text-center font-bold text-slate-400">Orange Laplacian Stack</div>
                    <div class="text-center font-bold text-slate-400">Gaussian Mask Stack</div>
                    <div class="text-center font-bold text-slate-400">Apple Laplacian Stack</div>

                    <!-- Stack Content (Mockup for 3 levels) -->
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://placehold.co/250x250/1e293b/cbd5e1?text=Orange+Laplacian+L0" alt="Orange Laplacian Stack Level 0" class="w-full h-auto">
                        <figcaption class="p-1 text-center text-xs">L0</figcaption>
                    </figure>
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://placehold.co/250x250/1e293b/cbd5e1?text=Mask+Gaussian+G0" alt="Mask Gaussian Stack Level 0" class="w-full h-auto">
                        <figcaption class="p-1 text-center text-xs">G0</figcaption>
                    </figure>
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://placehold.co/250x250/1e293b/cbd5e1?text=Apple+Laplacian+L0" alt="Apple Laplacian Stack Level 0" class="w-full h-auto">
                        <figcaption class="p-1 text-center text-xs">L0</figcaption>
                    </figure>
                    
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://placehold.co/250x250/1e293b/cbd5e1?text=Orange+Laplacian+L2" alt="Orange Laplacian Stack Level 2" class="w-full h-auto">
                        <figcaption class="p-1 text-center text-xs">L2</figcaption>
                    </figure>
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://placehold.co/250x250/1e293b/cbd5e1?text=Mask+Gaussian+G2" alt="Mask Gaussian Stack Level 2" class="w-full h-auto">
                        <figcaption class="p-1 text-center text-xs">G2</figcaption>
                    </figure>
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://placehold.co/250x250/1e293b/cbd5e1?text=Apple+Laplacian+L2" alt="Apple Laplacian Stack Level 2" class="w-full h-auto">
                        <figcaption class="p-1 text-center text-xs">L2</figcaption>
                    </figure>
                </div>

                <h4 class="text-xl font-medium text-slate-200 mb-4">Final Blended Results</h4>
                <div class="grid md:grid-cols-2 lg:grid-cols-4 gap-4">
                    <!-- Figure 3.42 Recreation (Oraple) -->
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://placehold.co/250x250/1e293b/cbd5e1?text=Oraple+Straight+Mask" alt="Oraple blend with straight line mask" class="w-full h-auto">
                        <figcaption class="p-2 text-center text-sm font-bold text-accent">Figure 3.42 (a-l) Recreation</figcaption>
                    </figure>
                    
                    <!-- Custom Blend 1 (Straight Line Mask) -->
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://placehold.co/250x250/1e293b/cbd5e1?text=Custom+Blend+Straight+Mask" alt="Custom blend with straight line mask" class="w-full h-auto">
                        <figcaption class="p-2 text-center text-sm">Custom Blend 1 (Straight Mask)</figcaption>
                    </figure>

                    <!-- Custom Blend 2 (Irregular Mask) -->
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://placehold.co/250x250/1e293b/cbd5e1?text=Custom+Blend+Irregular+Mask" alt="Custom blend with irregular mask" class="w-full h-auto">
                        <figcaption class="p-2 text-center text-sm">Custom Blend 2 (Irregular Mask)</figcaption>
                    </figure>
                    
                    <!-- Bells and Whistles - Exploration -->
                    <figure class="rounded-lg overflow-hidden shadow-xl bg-primary">
                        <img src="https://placehold.co/250x250/1e293b/cbd5e1?text=Exploration+Blend+Justification" alt="Custom blend demonstrating exploration" class="w-full h-auto">
                        <figcaption class="p-2 text-center text-sm">Exploration Blend (e.g., color/gray mix)</figcaption>
                    </figure>
                </div>

                <!-- Bells and Whistles -->
                <div class="mt-6 p-4 bg-slate-900 rounded-lg border border-slate-700">
                    <h4 class="text-xl font-medium text-accent mb-2">Bells and Whistles: Blending Exploration & Justification</h4>
                    <p class="text-sm">For the irregular mask blend (e.g., a hand-drawn or generated contour), the boundary definition was critical. We found that the blending result is significantly improved when the mask contour is soft and aligned with the high-contrast edges of the images being blended, as the Gaussian stack does the heavy lifting of feathering the transition. We experimented with different $\sigma$ values for the mask's Gaussian stack, finding a large $\sigma$ ($>10$) provides the best, most imperceptible blend across the transition zone.</p>
                </div>
            </div>

        </section>
        
    </main>

    <footer class="bg-primary p-4 text-center text-xs text-slate-500">
        &copy; 2024 Image Processing Project | Built with HTML & Tailwind CSS
    </footer>

</body>
</html>

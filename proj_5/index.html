<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>CS280A/CS180 Project 5 — Fun With Diffusion Models (Part A + Part B)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root{
      --bg:#111; --fg:#eee; --muted:#b9b9b9; --card:#171717; --border:#2a2a2a;
      --accent:#7bdcff; --accent2:#b0ffb7;
    }
    *{box-sizing:border-box}
    body{
      font-family: system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",sans-serif;
      max-width: 980px; margin: 0 auto; padding: 32px 16px 80px;
      line-height: 1.65; background: var(--bg); color: var(--fg);
    }
    a{ color: var(--accent); text-decoration: none; }
    a:hover{ text-decoration: underline; }
    h1{ font-size: 2.0rem; margin: 0 0 8px; }
    h2{ font-size: 1.45rem; margin-top: 40px; }
    h3{ font-size: 1.15rem; margin-top: 26px; }
    p, li{ color: var(--fg); }
    .muted{ color: var(--muted); }
    .pill{
      display:inline-block; padding:4px 10px; border:1px solid var(--border);
      border-radius: 999px; font-size: 0.9rem; color: var(--muted);
      margin-right: 8px; margin-top: 8px;
    }
    .card{
      background: var(--card); border: 1px solid var(--border);
      border-radius: 16px; padding: 16px 16px; margin: 14px 0;
    }
    .toc a{ display:block; padding: 4px 0; color: var(--accent); }
    .grid{
      /* display:grid; gap: 12px;
      grid-template-columns: repeat(2, minmax(0, 1fr)); */
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(90px, 1fr));
      gap: 12px;
    }
    @media (max-width: 820px){ .grid{ grid-template-columns: 1fr; } }
    figure{ margin: 0; }
    img{
      width: 100%; height: auto; display:block;
      border-radius: 14px; border: 1px solid var(--border);
      background: #0c0c0c;
    }
    figcaption{
      font-size: 0.92rem; color: var(--muted);
      margin-top: 6px;
    }
    .three{ grid-template-columns: repeat(3, minmax(0, 1fr)); }
    @media (max-width: 980px){ .three{ grid-template-columns: 1fr; } }
    .twoWide{ grid-template-columns: 1fr 1fr; }
    .kpi{
      display:flex; flex-wrap:wrap; gap:10px; margin-top: 10px;
    }
    code.inline{
      background:#0e0e0e; border:1px solid var(--border);
      padding: 2px 6px; border-radius: 8px; color: #d7d7d7;
    }
    hr{ border: none; border-top: 1px solid var(--border); margin: 28px 0; }
    .note{ border-left: 3px solid var(--accent2); padding-left: 12px; color: var(--muted); }
  </style>
</head>

<body>
  <header class="card">
    <h1>Project 5 — Fun With Diffusion Models</h1>
    <div class="muted">
      <span class="pill">Author: Yuchen Zhang</span>
      <span class="pill">CS280A Fall 2025</span>
      <span class="pill">Topics: Diffusion + Flow Matching</span>
    </div>
    <p class="muted" style="margin-top:10px;">
      This project contains two parts:
      <b>Part A</b> explores a pretrained diffusion model, DeepFloyd IF, and experimented with sampling loops, inpainting, and optical illusions.
      <b>Part B</b> trains a generative model from scratch on the MNIST dataset using UNets and flow matching, also played with time/class conditioning to improve sampling quality.
    </p>
  </header>

  <section class="card toc">
    <h2 style="margin-top:0;">Table of Contents</h2>
    <a href="#partA">Part A — The Power of Diffusion Models</a>
    <a href="#A0">A0. Setup + Prompt Embeddings</a>
    <a href="#A1">A1. Sampling Loops (Forward, Denoising, CFG, i2i, Inpainting)</a>
    <a href="#A2">A2. Optical Illusions (Visual Anagrams, Hybrid Images)</a>
    <a href="#A3">A3. Bells & Whistles (Part A)</a>
    <a href="#partB">Part B — Flow Matching from Scratch</a>
    <a href="#B1">B1. Train a Single-Step Denoising UNet</a>
    <a href="#B2">B2. Train a Flow Matching Model (Time Conditioning)</a>
    <a href="#B3">B3. Add Class Conditioning + CFG + Simplify Training</a>
    <a href="#B4">B4. Bells & Whistles (Part B)</a>
    <a href="#learned">Takeaways</a>
  </section>

  <hr />

  <!-- ========================= PART A ========================= -->
  <section id="partA">
    <h2>Part A — The Power of Diffusion Models</h2>

    <p>
        In Part A, I explore the fundamental mechanics of diffusion models by manually implementing the sampling loops
    rather than relying on a "black box" pipeline. I start by visualizing the <b>forward process</b>, taking a clean
    test image (the Campanile) and gradually corrupting it with Gaussian noise. Then, I implement the reverse process,
    demonstrating how a model can recover the image through <b>iterative denoising</b>. Finally, I apply these
    concepts to creative tasks, including <b>inpainting</b> (restoring missing parts of an image),
    <b>Visual Anagrams</b> (optical illusions), and <b>Hybrid Images</b>.
      </p>

    <!-- A0 -->

<section id="A0" class="card">
  <h3>A0. Setup & Text Embeddings</h3>
  <p>
    For this project, I utilized the <b>DeepFloyd IF</b> diffusion pipeline, a pixel-based diffusion model. 
    DeepFloyd operates in a cascaded fashion: <b>Stage I</b> generates a base <b>64×64</b> image, which can then be passed to 
    <b>Stage II</b> for super-resolution upscaling to <b>256×256</b>.
  </p>
  
  <p>
    The model is conditioned on text, but it does not process raw strings directly. Instead, text prompts must be encoded into 
    high-dimensional vectors (embeddings) using a T5 encoder. Since this encoding process is computationally heavy, 
    I pre-computed the embeddings for a set of specific prompts. This allowed me to iterate quickly on the diffusion 
    mechanics (like noise levels and guidance scales) without re-running the text encoder every time.
  </p>
  
  <p>
    To ensure scientific reproducibility, I fixed the random seed to <b>404</b> for all experiments. This guarantees that 
    any visual differences observed in later sections are due to algorithmic changes (e.g., comparing random sampling vs. CFG) 
    rather than the inherent randomness of the noise generation.
  </p>

  <ul>
    <li>
      <b>Selected Prompts:</b> I chose three distinct prompts to test the model's ability to handle different subjects and styles:
      <ol>
        <li><i>"a UC Berkeley oski"</i> (Artistic/Landscape)</li>
        <li><i>"a man fighting with a tree"</i> (Portrait/Realistic)</li>
        <li><i>"a frozen university campus"</i> (Lighting/Texture)</li>
      </ol>
    </li>
    <li>
      <b>Inference Parameters:</b> I primarily utilized <code class="inline">num_inference_steps = 20</code> for sampling. 
      This provides a strong balance between generation speed and visual fidelity. Then I also experimented with num_inference_steps = 5, 10, 50 to see the effect of inference steps.
    </li>
  </ul>

  <p class="muted">
    <b>Observation:</b> Even at the coarse Stage I resolution (64x64), the model successfully captures the semantic core 
    of the prompts. The "a man fighting with a tree" and "a frozen university campus" both show good visual of the scene, capturing most of the semantic elements. For example, a university campus and snow in the later prompt. However, the "a UC Berkeley oski" is the least precise visual, none of them correctly displayed the true oski figure. This indicates the model cannot create concepts it never seen before.
    Also the number of inference steps play a vital role. The low steps often produced course and unprecise images, whereas the photos become more precise as the steps increase. For example, in the UC Berkeley oski case, the low steps had completely lost, whereas the high steps showed a university building implying that it realized the comcept of UC Berkeley.
  </p>

  <div class="grid">
        <!-- Prompt 1 -->
        <figure>
          <img src="media/partA/0_0/a_UC_Berkeley_oski_20_64.png" alt="Stage I owl (20 steps)">
          <figcaption>a UC Berkeley oski — Stage I (64×64), 20 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_UC_Berkeley_oski_20_256.png" alt="Stage II owl (20 steps)">
          <figcaption>a UC Berkeley oski — Stage II (256×256), 20 steps</figcaption>
        </figure>
      
        <!-- Prompt 2 -->
        <figure>
          <img src="media/partA/0_0/a_man_fighting_with_a_tree_20_64.png" alt="Stage I village (20 steps)">
          <figcaption>a man fighting with a tree — Stage I (64×64), 20 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_man_fighting_with_a_tree_20_256.png" alt="Stage II village (20 steps)">
          <figcaption>a man fighting with a tree — Stage II (256×256), 20 steps</figcaption>
        </figure>
      
        <!-- Prompt 3 -->
        <figure>
          <img src="media/partA/0_0/a_frozen_university_campus_20_64.png" alt="Stage I islands (20 steps)">
          <figcaption>a frozen university campus — Stage I (64×64), 20 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_frozen_university_campus_20_256.png" alt="Stage II islands (20 steps)">
          <figcaption>a frozen university campus — Stage II (256×256), 20 steps</figcaption>
        </figure>
      </div>
      
      <h3 style="margin-top:18px;">5 inference steps</h3>
      <div class="grid">
        <!-- Prompt 1 -->
        <figure>
          <img src="media/partA/0_0/a_UC_Berkeley_oski_5_64.png" alt="Stage I owl (20 steps)">
          <figcaption>a UC Berkeley oski — Stage I (64×64), 5 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_UC_Berkeley_oski_5_256.png" alt="Stage II owl (20 steps)">
          <figcaption>a UC Berkeley oski — Stage II (256×256), 5 steps</figcaption>
        </figure>
      
        <!-- Prompt 2 -->
        <figure>
          <img src="media/partA/0_0/a_man_fighting_with_a_tree_5_64.png" alt="Stage I village (20 steps)">
          <figcaption>a man fighting with a tree — Stage I (64×64), 5 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_man_fighting_with_a_tree_5_256.png" alt="Stage II village (20 steps)">
          <figcaption>a man fighting with a tree — Stage II (256×256), 5 steps</figcaption>
        </figure>
      
        <!-- Prompt 3 -->
        <figure>
          <img src="media/partA/0_0/a_frozen_university_campus_5_64.png" alt="Stage I islands (20 steps)">
          <figcaption>a frozen university campus — Stage I (64×64), 5 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_frozen_university_campus_5_256.png" alt="Stage II islands (20 steps)">
          <figcaption>a frozen university campus — Stage II (256×256), 5 steps</figcaption>
        </figure>
      </div>

      <h3 style="margin-top:18px;">10 inference steps</h3>
      <div class="grid">
        <!-- Prompt 1 -->
        <figure>
          <img src="media/partA/0_0/a_UC_Berkeley_oski_10_64.png" alt="Stage I owl (20 steps)">
          <figcaption>a UC Berkeley oski — Stage I (64×64), 10 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_UC_Berkeley_oski_10_256.png" alt="Stage II owl (20 steps)">
          <figcaption>a UC Berkeley oski — Stage II (256×256), 10 steps</figcaption>
        </figure>
      
        <!-- Prompt 2 -->
        <figure>
          <img src="media/partA/0_0/a_man_fighting_with_a_tree_10_64.png" alt="Stage I village (20 steps)">
          <figcaption>a man fighting with a tree — Stage I (64×64), 10 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_man_fighting_with_a_tree_10_256.png" alt="Stage II village (20 steps)">
          <figcaption>a man fighting with a tree — Stage II (256×256), 10 steps</figcaption>
        </figure>
      
        <!-- Prompt 3 -->
        <figure>
          <img src="media/partA/0_0/a_frozen_university_campus_10_64.png" alt="Stage I islands (20 steps)">
          <figcaption>a frozen university campus — Stage I (64×64), 10 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_frozen_university_campus_10_256.png" alt="Stage II islands (20 steps)">
          <figcaption>a frozen university campus — Stage II (256×256), 10 steps</figcaption>
        </figure>
      </div>

      <h3 style="margin-top:18px;">50 inference steps</h3>
      <div class="grid">
        <!-- Prompt 1 -->
        <figure>
          <img src="media/partA/0_0/a_UC_Berkeley_oski_50_64.png" alt="Stage I owl (20 steps)">
          <figcaption>a UC Berkeley oski — Stage I (64×64), 50 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_UC_Berkeley_oski_50_256.png" alt="Stage II owl (20 steps)">
          <figcaption>a UC Berkeley oski — Stage II (256×256), 50 steps</figcaption>
        </figure>
      
        <!-- Prompt 2 -->
        <figure>
          <img src="media/partA/0_0/a_man_fighting_with_a_tree_50_64.png" alt="Stage I village (20 steps)">
          <figcaption>a man fighting with a tree — Stage I (64×64), 50 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_man_fighting_with_a_tree_50_256.png" alt="Stage II village (20 steps)">
          <figcaption>a man fighting with a tree — Stage II (256×256), 50 steps</figcaption>
        </figure>
      
        <!-- Prompt 3 -->
        <figure>
          <img src="media/partA/0_0/a_frozen_university_campus_50_64.png" alt="Stage I islands (20 steps)">
          <figcaption>a frozen university campus — Stage I (64×64), 50 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_frozen_university_campus_50_256.png" alt="Stage II islands (20 steps)">
          <figcaption>a frozen university campus — Stage II (256×256), 50 steps</figcaption>
        </figure>
      </div>
      
  </section>
  
</section>


    <!-- A1 -->
    <section id="A1" class="card">
      <h3>A1. Sampling Loops</h3>

      <h3 style="margin-top:10px;">A1.1 Implementing the Forward Process</h3>
    <p>
      The forward process produces a noisy image <b>x<sub>t</sub></b> from a clean image
      <b>x<sub>0</sub></b> by adding a Gaussian noise according to the noise schedule.
      
      Concretely, the equation at timestep <b>t</b>:
      <span class="muted">
        x<sub>t</sub> = √ᾱ<sub>t</sub> · x<sub>0</sub> + √(1 − ᾱ<sub>t</sub>) · ε,  where ε ~ N(0, I).
      </span>
  
      Earlier timesteps have ᾱ<sub>t</sub> close to 1, so the result is similar to the original image; later timesteps have
      ᾱ<sub>t</sub> near 0, so the result becomes nearly pure noise. I applied the
      forward process to the provided Campanile image at 3 timesteps
      (t = 250, 500, 750), where the result become more and more like pur noise.
    </p>
      <div class="grid">
        <figure>
          <img src="media/partA/1_1/campanile.jpg" alt="Campanile clean">
          <figcaption>Clean Campanile (t=0).</figcaption>
        </figure>
        <figure>
          <img src="media/partA/1_1/noise_t_250.png" alt="Campanile t=250">
          <figcaption>Noisy Campanile at t=250.</figcaption>
        </figure>
        <figure>
          <img src="media/partA/1_1/noise_t_500.png" alt="Campanile t=500">
          <figcaption>Noisy Campanile at t=500.</figcaption>
        </figure>
        <figure>
          <img src="media/partA/1_1/noise_t_750.png" alt="Campanile t=750">
          <figcaption>Noisy Campanile at t=750.</figcaption>
        </figure>
      </div>

      <h3>A1.2 Classical Denoising (Gaussian Blur)</h3>
      <p>
        I used a classical Gaussian blur filter as a baseline denoising method for comparison. Gaussian blur works well for
        low levels of noise but it cannot distinguish noise from fine image details. As the noise level increases, gaussian blur
        degrades rapidly as shown below.
      </p>
      <div>
        <figure>
          <img src="media/partA/1_2/gaussian_denoise_t_250.png" alt="Gaussian denoise t=250">
          <figcaption>Gaussian blur at t=250. </figcaption>
        </figure>
        <figure>
          <img src="media/partA/1_2/gaussian_denoise_t_500.png" alt="Gaussian denoise t=500">
          <figcaption>Gaussian blur at t=500.</figcaption>
        </figure>
        <figure>
          <img src="media/partA/1_2/gaussian_denoise_t_750.png" alt="Gaussian denoise t=750">
          <figcaption>Gaussian blur at t=750.</figcaption>
        </figure>
      </div>

      <h3>A1.3 One-Step Denoising (Pretrained Diffusion Denoiser)</h3>
      <p>
        Next, I use the pretrained DeepFloyd Stage-I UNet to perform a single denoising step. First I used the forward function to add noise to the original image, then I passed the noisy image and the corresponding timestep t to the UNet to predict the noise ε̂. Finally, I computed the denoised image x̂<sub>0</sub> using the predicted noise:
        <span class="muted">
          x̂<sub>0</sub> = (x<sub>t</sub> − √(1 − ᾱ<sub>t</sub>) · ε̂) / √ᾱ<sub>t</sub>
        </span>.
        This effectively “subtracts” the predicted noise from x<sub>t</sub> to recover an estimate of the clean image.
      </p>

      <div>
        <figure>
          <img src="media/partA/1_3/reconstructed_t_250.png" alt="One-step t=250">
          <figcaption>One-step denoise at t=250 </figcaption>
        </figure>
        <figure>
          <img src="media/partA/1_3/reconstructed_t_500.png" alt="One-step t=500">
          <figcaption>One-step denoise at t=500 </figcaption>
        </figure>
        <figure>
          <img src="media/partA/1_3/reconstructed_t_750.png" alt="One-step t=750">
          <figcaption>One-step denoise at t=750 </figcaption>
        </figure>
      </div>
      
      <section id="A1-4" class="card">
  <h3>1.4 Iterative Denoising</h3>
  
  <p>
    As demonstrated in the previous section, attempting to recover a clean image from high noise levels in a single step is an ill-posed problem, resulting in blurry, averaged outputs. Diffusion models overcome this by removing noise <b>iteratively</b>.
  </p>

  <p>
    However, running the full diffusion process (e.g., 1000 steps) is computationally expensive. To speed this up, I implemented <b>Strided Sampling</b>. I created a schedule of timesteps starting at <code>t=990</code> and stepping down by 30 until reaching 0. This allows us to skip steps while still adhering to the diffusion dynamics.
  </p>

  <p>
    At each step, we move from a noisier timestep <i>t</i> to a less noisy timestep <i>t'</i>. The update rule involves estimating the clean image, then re-noising it slightly to match the target noise variance of <i>t'</i>. This can be thought of as a weighted interpolation:
  </p>

  <div class="math-block" style="background: rgba(255,255,255,0.05); padding: 15px; border-radius: 6px; text-align: center; margin-bottom: 25px;">
    <i>x<sub>t'</sub></i> = (Interpolation of <i>x<sub>0</sub></i>) + (Interpolation of <i>x<sub>t</sub></i>) + (Random Variance)
  </div>

  <h4>The Denoising Process</h4>
  <p>
    I started with the Campanile image noised to index 10 (approx t=690) and ran the iterative denoiser. Below, I visualize the process by displaying the image at every 5th step of the loop. Notice how the structure gradually emerges from the chaos.
  </p>

  <div class="grid">
    <figure>
      <img src="media/partA/1_4/iter_step_10_t_690.png" alt="t = 690">
      <figcaption>t = 690</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_4/iter_step_15_t_540.png" alt="t = 540">
      <figcaption>t = 540</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_4/iter_step_20_t_390.png" alt="t = 390">
      <figcaption>t = 390</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_4/iter_step_25_t_240.png" alt="t = 240">
      <figcaption>t = 240</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_4/iter_step_30_t_90.png" alt="t = 90">
      <figcaption>t = 90</figcaption>
    </figure>
  </div>

  <h4>Final Comparison</h4>
  <p>
    Here is the final result of the iterative method compared against the baseline methods. The iterative approach recovers fine details (like the brick texture and scaffolding) that the One-Step method blurs out and Gaussian Blur destroys completely.
  </p>

  <div class="grid">
    <figure>
      <img src="media/partA/1_4/comparison.png" alt="Iterative Result">
      <figcaption>Final Iterative Result Comparison</figcaption>
    </figure>
  </div>

  <section id="A1-5" class="card">
  <h3>1.5 Diffusion Model Sampling</h3>
  
  <p>
    The <code>iterative_denoise</code> function is not limited to restoring noisy images; it can also act as a generative tool. 
    By setting <code>i_start = 0</code> and passing pure Gaussian noise as the input, the model hallucinates a completely new image from scratch.
  </p>

  <p>
    I used this method to generate 5 random samples based on the prompt <i>"a high quality photo"</i>. 
    The results below demonstrate the model's ability to form coherent structures (objects, landscapes, etc.) from pure randomness, 
    although the quality is limited without the use of Classifier-Free Guidance (which is explored in the next section).
  </p>

  <div class="grid">
    <figure>
      <img src="media/partA/1_5/sample_1.png" alt="Generated Sample 1">
      <figcaption>Sample 1</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_5/sample_2.png" alt="Generated Sample 2">
      <figcaption>Sample 2</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_5/sample_3.png" alt="Generated Sample 3">
      <figcaption>Sample 3</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_5/sample_4.png" alt="Generated Sample 4">
      <figcaption>Sample 4</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_5/sample_5.png" alt="Generated Sample 5">
      <figcaption>Sample 5</figcaption>
    </figure>
  </div>
</section>

<section id="A1-6" class="card">
  <h3>1.6 Classifier-Free Guidance (CFG)</h3>

  <p>
    As observed in the previous section, generating images purely from the prior often results in incoherent or "nonsensical" outputs. 
    To significantly improve image quality, I implemented <b>Classifier-Free Guidance (CFG)</b>.
  </p>

  <p>
    CFG works by computing two noise estimates at every timestep:
    <ul>
      <li><b>Conditional noise estimate ($\epsilon_c$):</b> Conditioned on the text prompt (e.g., "a high quality photo").</li>
      <li><b>Unconditional noise estimate ($\epsilon_u$):</b> Conditioned on a null prompt (an empty string <code>""</code>).</li>
    </ul>
    The final noise estimate is computed by extrapolating the difference between these two vectors:
  </p>

  <div class="math-block" style="background: rgba(255,255,255,0.05); padding: 15px; border-radius: 6px; text-align: center; margin-bottom: 25px; font-family: 'Times New Roman', serif; font-size: 1.2rem;">
    $\epsilon = \epsilon_u + \gamma (\epsilon_c - \epsilon_u)$
  </div>

  <p>
    Here, <b>$\gamma$</b> is the guidance scale. When $\gamma > 1$, the model pushes the generation <i>away</i> from the generic/average image 
    and <i>towards</i> the specific text prompt.
  </p>

  <p>
    I updated the iterative sampling loop to include CFG (using a scale of $\gamma = 7$). Below are 5 samples generated 
    using the prompt <i>"a high quality photo"</i>. Compared to the non-guided samples in section 1.5, these are significantly 
    sharper, more coherent, and realistic.
  </p>

  <div class="grid">
    <figure>
      <img src="media/partA/1_6/cfg_sample_1.png" alt="CFG Sample 1">
      <figcaption>CFG Sample 1</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_6/cfg_sample_2.png" alt="CFG Sample 2">
      <figcaption>CFG Sample 2</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_6/cfg_sample_3.png" alt="CFG Sample 3">
      <figcaption>CFG Sample 3</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_6/cfg_sample_4.png" alt="CFG Sample 4">
      <figcaption>CFG Sample 4</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_6/cfg_sample_5.png" alt="CFG Sample 5">
      <figcaption>CFG Sample 5</figcaption>
    </figure>
  </div>
</section>

      <section id="A1-7" class="card">
  <h3>1.7 Image-to-Image Translation</h3>

  <p>
    In this section, I implemented the <b>SDEdit</b> algorithm. The core idea is simple but powerful: we take a real image, add a specific amount of noise to it (jumping to timestep <i>t</i>), and then run the iterative denoising process to clear it up.
  </p>
  <p>
    The noise level determines the "creativity" of the edit.
    <ul>
      <li><b>Low noise (e.g., i_start=1):</b> The model makes minor edits, staying very close to the original image.</li>
      <li><b>High noise (e.g., i_start=20):</b> The original structure is heavily corrupted, forcing the model to "hallucinate" new details to project it back onto the natural image manifold.</li>
    </ul>
  </p>

  <h4 style="margin-top: 30px; border-bottom: 1px solid #333; padding-bottom: 5px;">1.7.0 SDEdit: Noise Levels</h4>
  <p>
    Below, I ran SDEdit on the Campanile and two other test images of my own with varying starting noise indices [1, 3, 5, 7, 10, 20]. 
    The image becomes more abstract and distinct from the original as the noise level increases.
    I used the prompt <i>"a high quality photo"</i> and null as the unconditional prompt.
  </p>

  <h5>Test Image 1: Campanile</h5>
  <div class="grid">
    <figure><img src="media/partA/1_7/campanile.jpg"><figcaption>Original</figcaption></figure>
    <figure><img src="media/partA/1_7/campanile_edit_i_start_1.png"><figcaption>i=1</figcaption></figure>
    <figure><img src="media/partA/1_7/campanile_edit_i_start_3.png"><figcaption>i=3</figcaption></figure>
    <figure><img src="media/partA/1_7/campanile_edit_i_start_5.png"><figcaption>i=5</figcaption></figure>
    <figure><img src="media/partA/1_7/campanile_edit_i_start_7.png"><figcaption>i=7</figcaption></figure>
    <figure><img src="media/partA/1_7/campanile_edit_i_start_10.png"><figcaption>i=10</figcaption></figure>
    <figure><img src="media/partA/1_7/campanile_edit_i_start_20.png"><figcaption>i=20</figcaption></figure>
  </div>

  <h5>Test Image 2: Labubu</h5>
  <div class="grid">
    <figure><img src="media/partA/1_7/labubu2.jpg"><figcaption>Original</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg1_i_start_1.png"><figcaption>i=1</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg1_i_start_3.png"><figcaption>i=3</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg1_i_start_5.png"><figcaption>i=5</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg1_i_start_7.png"><figcaption>i=7</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg1_i_start_10.png"><figcaption>i=10</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg1_i_start_20.png"><figcaption>i=20</figcaption></figure>
  </div>

  <h5>Test Image 3: Toy</h5>
  <div class="grid">
    <figure><img src="media/partA/1_7/toy.jpg"><figcaption>Original</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg2_i_start_1.png"><figcaption>i=1</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg2_i_start_3.png"><figcaption>i=3</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg2_i_start_5.png"><figcaption>i=5</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg2_i_start_7.png"><figcaption>i=7</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg2_i_start_10.png"><figcaption>i=10</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg2_i_start_20.png"><figcaption>i=20</figcaption></figure>
  </div>

  <h4 style="margin-top: 30px; border-bottom: 1px solid #333; padding-bottom: 5px;">1.7.1 Editing Hand-Drawn & Web Images</h4>
  <p>
    This procedure works exceptionally well for projecting non-realistic images (like sketches) onto the natural image manifold. 
    I took a web image (Tiger) and 2 hand-drawn sketches (Car and Cat) and ran them through the same pipeline.
  </p>

  <h5>Web Image: Tiger</h5>
  <div class="grid">
    <figure><img src="media/partA/1_7_1/tiger.jpg"><figcaption>Original</figcaption></figure>
    <figure><img src="media/partA/1_7_1/web_dragon_i_start_1.png"><figcaption>i=1</figcaption></figure>
    <figure><img src="media/partA/1_7_1/web_dragon_i_start_3.png"><figcaption>i=3</figcaption></figure>
    <figure><img src="media/partA/1_7_1/web_dragon_i_start_5.png"><figcaption>i=5</figcaption></figure>
    <figure><img src="media/partA/1_7_1/web_dragon_i_start_7.png"><figcaption>i=7</figcaption></figure>
    <figure><img src="media/partA/1_7_1/web_dragon_i_start_10.png"><figcaption>i=10</figcaption></figure>
    <figure><img src="media/partA/1_7_1/web_dragon_i_start_20.png"><figcaption>i=20</figcaption></figure>
  </div>

  <h5>Hand-Drawn: Car Sketch</h5>
  <div class="grid">
    <figure><img src="media/partA/1_7_1/hand_drawn_car_i_start_20.png"><figcaption>Original</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_car_i_start_1.png"><figcaption>i=1</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_car_i_start_3.png"><figcaption>i=3</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_car_i_start_5.png"><figcaption>i=5</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_car_i_start_7.png"><figcaption>i=7</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_car_i_start_10.png"><figcaption>i=10</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_car_i_start_20.png"><figcaption>i=20</figcaption></figure>
  </div>
  <h5>Hand-Drawn: Cat Sketch</h5>
  <div class="grid">
    <figure><img src="media/partA/1_7_1/myImage.png"><figcaption>Original</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_cat_i_start_1.png"><figcaption>i=1</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_cat_i_start_3.png"><figcaption>i=3</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_cat_i_start_5.png"><figcaption>i=5</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_cat_i_start_7.png"><figcaption>i=7</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_cat_i_start_10.png"><figcaption>i=10</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_cat_i_start_20.png"><figcaption>i=20</figcaption></figure>
  </div>

  <h4 style="margin-top: 30px; border-bottom: 1px solid #333; padding-bottom: 5px;">1.7.2 Inpainting</h4>
  <p>
    I implemented the <b>RePaint</b> algorithm to perform inpainting. By using a binary mask, we can force the model to keep the original pixels in the unmasked areas (<i>mask=0</i>) while generating new content in the masked areas (<i>mask=1</i>).
  </p>
  <p>
    This is achieved by modifying the sampling loop: at every step, after the model predicts $x_{t-1}$, we replace the unmasked pixels with the noisy original image at that same timestep.
  </p>
  <p>
    I experimented with inpainting on the Campanile image as well as two custom images of my own. The results below show the original image, the mask used for inpainting, and the final inpainted result. For my own images, I'm showing the mask using a red box for clarity, and inpainting the image by generate a new image where the mask is at the center of the patch. This is because the model will only generate contents at the center of an image, so cropping the mask to the center allows for better inpainting results.
  </p>

  <div class="gallery", style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center; margin-top: 20px;">
    <figure><img src="media/partA/1_7_2/campanile.jpg"><figcaption>campanile</figcaption></figure>

    <figure><img src="media/partA/1_7_2/laptop.jpg"><figcaption>laptop</figcaption></figure>

    <figure><img src="media/partA/1_7_2/toy.jpg"><figcaption>toy</figcaption></figure>
  </div>

  <h4 style="margin-top: 30px; border-bottom: 1px solid #333; padding-bottom: 5px;">1.7.4 Text-Conditional Image-to-Image Translation</h4>
  <p>
    Finally, I extended SDEdit to use specific text prompts rather than just "a high quality photo". This allows me to guide the generation process. For example, I first take the Campanile and guide it toward a "pencil". Then I used two of my own images and guided them toward different prompts as shown here.
  </p>

  <h5>Campanile -> "Pencil"</h5>
  <div class="grid">
    <figure><img src="media/partA/1_7_3/campanile/campanile.jpg"><figcaption>Original</figcaption></figure>
    <figure><img src="media/partA/1_7_3/campanile/edit_i_start_1.png"><figcaption>i=1</figcaption></figure>
    <figure><img src="media/partA/1_7_3/campanile/edit_i_start_3.png"><figcaption>i=3</figcaption></figure>
    <figure><img src="media/partA/1_7_3/campanile/edit_i_start_5.png"><figcaption>i=5</figcaption></figure>
    <figure><img src="media/partA/1_7_3/campanile/edit_i_start_7.png"><figcaption>i=7</figcaption></figure>
    <figure><img src="media/partA/1_7_3/campanile/edit_i_start_10.png"><figcaption>i=10</figcaption></figure>
    <figure><img src="media/partA/1_7_3/campanile/edit_i_start_20.png"><figcaption>i=20</figcaption></figure>
  </div>

  <h5>Labubu -> Dog</h5>
  <div class="grid">
    <figure><img src="media/partA/1_7_3/mydraw1/labubu2.jpg"><figcaption>Original</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw1/edit_i_start_1.png"><figcaption>i=1</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw1/edit_i_start_3.png"><figcaption>i=3</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw1/edit_i_start_5.png"><figcaption>i=5</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw1/edit_i_start_7.png"><figcaption>i=7</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw1/edit_i_start_10.png"><figcaption>i=10</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw1/edit_i_start_20.png"><figcaption>i=20</figcaption></figure>
  </div>
  
  <h5>Toy -> Man wearing a hat</h5>
  <div class="grid">
    <figure><img src="media/partA/1_7_3/mydraw2/toy.jpg"><figcaption>Original</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw2/edit_i_start_1.png"><figcaption>i=1</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw2/edit_i_start_3.png"><figcaption>i=3</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw2/edit_i_start_5.png"><figcaption>i=5</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw2/edit_i_start_7.png"><figcaption>i=7</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw2/edit_i_start_10.png"><figcaption>i=10</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw2/edit_i_start_20.png"><figcaption>i=20</figcaption></figure>
  </div>

</section>

<section id="A1-8" class="card">
  <h3>1.8 Visual Anagrams</h3>

  <p>
    In this section, I implemented <b>Visual Anagrams</b> to create optical illusions: images that look like one thing when upright, 
    but transform into something completely different when flipped upside down.
  </p>

  <p>
    The implementation relies on modifying the diffusion process to satisfy two different prompts simultaneously (e.g., "an oil painting of an old man" vs. "an oil painting of people around a campfire"). 
    To achieve this, I implemented the following algorithm at each timestep $t$ of the denoising loop:
  </p>

  <ol>
    <li>
      <b>Estimate Noise 1 ($\epsilon_1$):</b> I first passed the current noisy image $x_t$ through the UNet using the first prompt (upright).
    </li>
    <li>
      <b>Estimate Noise 2 ($\epsilon_2$):</b> I <i>flipped</i> the image $x_t$ upside down and passed it through the UNet using the second prompt.
    </li>
    <li>
      <b>Average the Estimates:</b> To combine these conflicting goals, I flipped $\epsilon_2$ back to the original orientation and averaged it with $\epsilon_1$.
    </li>
  </ol>

  <div class="math-block" style="background: rgba(255,255,255,0.05); padding: 15px; border-radius: 6px; text-align: center; margin-bottom: 25px; font-family: 'Times New Roman', serif; font-size: 1.1rem;">
    $\epsilon_{final} = \frac{1}{2} (\epsilon_1 + \text{flip}(\epsilon_2))$
  </div>

  <p>
    By performing the diffusion step using this averaged noise estimate $\epsilon_{final}$, the image converges into a state that is visually coherent in both orientations.
  </p>

  <h4 style="margin-top:20px;">Results</h4>
  <p>
    Below are two generated illusions. For each, I display the image in its original orientation and then flipped 180 degrees to reveal the hidden subject.
  </p>

  <div class="grid">
    <figure>
      <img src="media/partA/1_8/illusion1.png" alt="Illusion 1 Upright">
      <figcaption><b>Upright:</b> "a photo of a rabbit"</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_8/illusion1_flip.png" alt="Illusion 1 Flipped">
      <figcaption><b>Flipped:</b> "a photo of a duck"</figcaption>
    </figure>

    <figure>
      <img src="media/partA/1_8/illusion2.png" alt="Illusion 2 Upright">
      <figcaption><b>Upright:</b> "an oil painting of an old man"</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_8/illusion2_flip.png" alt="Illusion 2 Flipped">
      <figcaption><b>Flipped:</b> "an oil painting of a snowy mountain village"</figcaption>
    </figure>
  </div>
</section>


<section id="A1-9" class="card">
  <h3>1.9 Hybrid Images</h3>

  <p>
    In this final section of Part A, I implemented <b>Factorized Diffusion</b> to create Hybrid Images. 
    Hybrid images are optical illusions that change based on viewing distance: one image dominates the low spatial frequencies 
    (seen from afar), while another dominates the high spatial frequencies (seen from up close).
  </p>

  <p>
    To generate these using a diffusion model, I modified the iterative noise estimate step. For every timestep $t$, 
    I computed two separate noise estimates: $\epsilon_1$ (for Prompt 1) and $\epsilon_2$ (for Prompt 2). 
    I then combined them using a frequency filter:
  </p>

  <div class="math-block" style="background: rgba(255,255,255,0.05); padding: 15px; border-radius: 6px; text-align: center; margin-bottom: 25px; font-family: 'Times New Roman', serif; font-size: 1.1rem;">
    $\epsilon_{hybrid} = \underbrace{f_{low}(\epsilon_1)}_{\text{Structure}} + \underbrace{( \epsilon_2 - f_{low}(\epsilon_2) )}_{\text{Details}}$
  </div>

  <h4 style="margin-top:20px;">Implementation Logic</h4>
  <p>
    I implemented the low-pass filter $f_{low}$ using a <b>Gaussian Blur</b> with a kernel size of <b>33</b> and a sigma of <b>2</b>.
  </p>
  <ul>
    <li><b>Low Frequencies:</b> Extracted from Prompt 1 by applying the Gaussian blur.</li>
    <li><b>High Frequencies:</b> Extracted from Prompt 2 by taking the original noise estimate and subtracting the blurred version.</li>
  </ul>
  <p>
    By summing these components, the resulting noise estimate $\epsilon_{hybrid}$ guides the diffusion process to create an image containing both subjects simultaneously.
  </p>

  <h4 style="margin-top:20px;">Results</h4>
  <div class="grid">
    
    <figure>
      <img src="media/partA/1_9/1_9_hybird_3.png" alt="Hybrid Image 1">
      <figcaption>
        <b>Hybrid 1</b><br>
        Low Freq: "a lithograph of a skull"<br>
        High Freq: "a lithograph of waterfalls"
      </figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_9/1_9_hybird_4.png" alt="Hybrid Image 1">
      <figcaption>
        <b>Hybrid 1</b><br>
        Low Freq: "a lithograph of a skull"<br>
        High Freq: "a lithograph of waterfalls"
      </figcaption>
    </figure>

    <figure>
      <img src="media/partA/1_9/1_9_hybird_1.png" alt="Hybrid Image 2">
      <figcaption>
        <b>Hybrid 2</b><br>
        Low Freq: "a painting of a rabbit"<br>
        High Freq: "a painting of a volcano"
      </figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_9/1_9_hybird_2.png" alt="Hybrid Image 2">
      <figcaption>
        <b>Hybrid 2</b><br>
        Low Freq: "a painting of a rabbit"<br>
        High Freq: "a painting of a volcano"
      </figcaption>
    </figure>

  </div>
</section>

  <section id="part2" class="card">
    <h2>Part 2: Bells & Whistles</h2>
    <p>
      For this portion, I extended the capabilities of the diffusion model to support complex geometric transformations and performed style transfer on a logo.
    </p>

    <h3 style="margin-top: 30px;">2.1 More Visual Anagrams</h3>
    <p>
      The logic of visual anagrams (averaging noise estimates) generalizes to any orthogonal transformation. 
      I implemented two new transformations: <b>90° Rotation</b> and <b>Jigsaw Patch Shuffling</b>.
    </p>

    <h4>Transformation 1: 90° Rotation</h4>
    <p>
      I modified the denoising loop to enforce that the image looks like Prompt A when upright, and Prompt B when rotated 90 degrees.
      <br>
      <b>Implementation:</b> I used <code>torch.rot90</code> (k=-1) to transform the image during the diffusion step.
    </p>

    <div class="grid">
      <figure>
        <img src="media/partA/bw/2_1_rotate_1.png" alt="Rotation View 1">
        <figcaption><b>View 1 (Upright)</b><br>"an oil painting of a village"</figcaption>
      </figure>
      <figure>
        <img src="media/partA/bw/2_1_rotate_2.png" alt="Rotation View 2">
        <figcaption><b>View 2 (Rotated 90°)</b><br>"a pencil sketch of a skull"</figcaption>
      </figure>
      <figure>
        <img src="media/partA/bw/2_1_rotate_3.png" alt="Rotation View 1">
        <figcaption><b>View 1 (Upright)</b><br>"an photo of a rabbit"</figcaption>
      </figure>
      <figure>
        <img src="media/partA/bw/2_1_rotate_4.png" alt="Rotation View 2">
        <figcaption><b>View 2 (Rotated 90°)</b><br>"a man wearing a hat"</figcaption>
      </figure>
    </div>

    <h4>Transformation 2: Jigsaw Puzzles</h4>
    <p>
      I implemented a transformation that shuffles the image into random 16x16 patches. The model generates an image that looks like 
      Prompt A in its ordered form, but rearranges into Prompt B when the patches are shuffled according to a fixed permutation.
    </p>
    
    <div class="grid">
      <figure>
        <img src="media/partA/bw/2_2_rotate_1.png" alt="Jigsaw View 1">
        <figcaption><b>View 1 (Ordered)</b><br>"an oil painting of a kitchen"</figcaption>
      </figure>
      <figure>
        <img src="media/partA/bw/2_2_rotate_2.png" alt="Jigsaw View 2">
        <figcaption><b>View 2 (Rearranged)</b><br>"a photo of a waterfall"</figcaption>
      </figure>
      <figure>
        <img src="media/partA/bw/2_2_rotate_3.png" alt="Jigsaw View 1">
        <figcaption><b>View 1 (Ordered)</b><br>"an oil painting of a snowy mountain village"</figcaption>
      </figure>
      <figure>
        <img src="media/partA/bw/2_2_rotate_4.png" alt="Jigsaw View 2">
        <figcaption><b>View 2 (Rearranged)</b><br>"an oil painting of a skull"</figcaption>
      </figure>
    </div>

    <h3 style="margin-top: 40px; border-top: 1px solid #333; padding-top: 20px;">2.2 Course Logo Design</h3>
    <p>
      I used SDEdit (Text-Conditioned Image-to-Image Translation) to redesign a logo. 
      By adding noise to the original UCB logo and guiding the denoising process with the prompt 
      <i>"an oil painting of a bear"</i>, I created a stylized version of the logo that retains the original structure 
      but adopts the artistic style of the prompt.
    </p>

    <div class="grid">
      <figure>
        <img src="media/partA/bw/2_logo_original.png" alt="Original Logo">
        <figcaption>Original Logo</figcaption>
      </figure>
      <figure>
        <img src="media/partA/bw/2_logo_low.png" alt="Generated 64x64">
        <figcaption>Generated (64x64)</figcaption>
      </figure>
      <figure>
        <img src="media/partA/bw/2_logo_high.png" alt="Upsampled Result">
        <figcaption>Upsampled Result<br></figcaption>
      </figure>
    </div>

  </section>
  
    <!-- ========================= PART B ========================= -->
<section id="partB">
    <h2>Part B — Flow Matching from Scratch</h2>
  
    <p>
      In Part B, I train a flow matching model from scratch using the MNIST dataset. I first
      implement a <b>single-step denoising UNet</b> and train it to map a noisy digit back to
      a clean digit.
      Then I create the <b>flow matching</b>, where the UNet learns a <i>time-dependent velocity field</i> and <i>class conditioning</i> that
      transports samples from a simple noise distribution to the data distribution via ODE integration.
    </p>
  
    <!-- ========================= B1 ========================= -->
    <section id="B1" class="card">
      <h3>B1. Train a Single-Step Denoising UNet</h3>
  
      <div class="content-block">
      <h3>B1.1 Implementing the UNet</h3>
      <p>
        In this part, I implemented the denoiser as a <strong>UNet</strong>. The architecture consists of downsampling and upsampling blocks with skip connections, allowing the network to retain high-frequency details while processing semantic information at lower resolutions.
      </p>

      <h4>The Architecture</h4>
      <p>
        The Unconditional UNet (Figure 1) takes a 1-channel image (MNIST digit) and outputs a 1-channel image (denoised result). It uses composed operations (Figure 2) to build deeper blocks without losing spatial resolution unnecessarily.
      </p>

      <div class="gallery" style="grid-template-columns: repeat(2, 1fr); gap: 20px;">
        <figure>
          <img src="media/partB/unet.jpg" alt="UNet Structure">
          <figcaption>Figure 1: Unconditional UNet Architecture</figcaption>
        </figure>
        <figure>
          <img src="media/partB/operations.jpg" alt="UNet Operations">
          <figcaption>Figure 2: Standard UNet Operations</figcaption>
        </figure>
      </div>

      <h4>Code Implementation</h4>
      <p>
        I implemented the atomic blocks (<code>Conv</code>, <code>DownConv</code>, <code>UpConv</code>, <code>Flatten</code>, <code>Unflatten</code>) and composed them into the full <code>UnconditionalUNet</code> class. A few examples of the atomic blocks are shown below:
      </p>

      <div style="background: #2d2d2d; padding: 15px; border-radius: 8px; overflow-x: auto; margin-bottom: 20px; border: 1px solid #444;">
<pre><code style="color: #f8f8f2; font-family: Consolas, monospace;">class Conv(nn.Module):
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.GELU()
        )
    def forward(self, x): return self.net(x)

class DownConv(nn.Module):
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.GELU()
        )
    def forward(self, x): return self.net(x)

      <p>Putting it all together in the main class:</p>
      <div style="background: #2d2d2d; padding: 15px; border-radius: 8px; overflow-x: auto; border: 1px solid #444;">
<pre><code style="color: #f8f8f2; font-family: Consolas, monospace;">class UnconditionalUNet(nn.Module):
    def __init__(self, in_channels=1, num_hiddens=128):
        super().__init__()
        # Encoder
        self.conv_block1 = ConvBlock(...)
        self.down_block1 = DownBlock(...)
        self.down_block2 = DownBlock(...)
        
        # Bottleneck
        self.flatten = Flatten()
        self.unflatten = Unflatten(...)
        
        # Decoder
        self.up_block1 = UpBlock(...)
        self.up_block2 = UpBlock(...)
        self.conv_block_out = ConvBlock(...)
        self.final_conv = nn.Conv2d(...)

    def forward(self, x):
        # Encoder
        
        # Bottleneck
        
        # Decoder (with Skip Connections)
      </div>
    </div>
  
      <div class="content-block">
      <h3>1.2 Using the UNet to Train a Denoiser</h3>
      <p>
        This section train the UNet to act as a denoiser $D_\theta$. Given a noisy image $z$, 
        the model recovers the original clean image $x$.
      </p>
      
      <h4>The Objective</h4>
      <p>
        I optimize the network parameters $\theta$ by minimizing the L2 loss between the predicted image and the ground truth:
      </p>
      <div class="math-block" style="text-align: center; margin: 15px 0; font-family: 'Times New Roman', serif;">
        $$ \mathcal{L} = \| D_\theta(z) - x \|^2 $$
      </div>

      <h4>The Noising Process</h4>
      <p>
        To generate training data pairs $(z, x)$, I corrupt clean MNIST digits on the fly by adding Gaussian noise:
      </p>
      <div class="math-block" style="text-align: center; margin: 15px 0; font-family: 'Times New Roman', serif;">
        $$ z = x + \sigma \epsilon, \quad \text{where } \epsilon \sim \mathcal{N}(0, I) $$
      </div>

      <p>
        Below is a visualization of this noising process across varying noise levels ($\sigma$). 
        As $\sigma$ increases, the original digit becomes increasingly indistinguishable from the background noise.
      </p>

      <figure>
        <img src="media/partB/1_2_0.jpg" alt="Visualization of Noising Process" style="width: 100%; max-width: 800px;">
        <figcaption>
          <b>Varying Noise Levels ($\sigma$):</b> Ranging from 0.0 (clean) to 1.0 (pure noise). 
          Note that at $\sigma=1.0$, the digit is almost entirely obscured.
        </figcaption>
      </figure>
    </div>
  
   <div class="content-block">
      <h3>1.2.1 Training the Denoiser</h3>
      <p>
        I trained the UNet to denoise images corrupted with $\sigma=0.5$ Gaussian noise.
      </p>
      <ul>
        <li><b>Dataset:</b> MNIST (Training Set)</li>
        <li><b>Batch Size:</b> 256</li>
        <li><b>Hidden Dimension:</b> 128</li>
        <li><b>Optimizer:</b> Adam (lr=1e-4)</li>
        <li><b>Epochs:</b> 5</li>
      </ul>

      <h4>Training Loss</h4>
      <figure>
        <img src="media/partB/1_2_1_loss.jpg" alt="Training Loss Curve">
        <figcaption>Training Loss over 5 Epochs</figcaption>
      </figure>

      <h4>Results on Test Set</h4>
      <p>
        Below are the denoising results on the unseen test set. Even after just 1 epoch, the model begins to recover digits. 
        By epoch 5, the reconstructions are sharp and clear.
      </p>

      <div class="gallery" style="grid-template-columns: repeat(2, 1fr); gap: 20px;">
        <figure>
          <img src="media/partB/1_2_1_epoch_1.jpg" alt="Epoch 1 Results">
          <figcaption>Epoch 1 Results</figcaption>
        </figure>
        <figure>
          <img src="media/partB/1_2_1_epoch_5.jpg" alt="Epoch 5 Results">
          <figcaption>Epoch 5 Results</figcaption>
        </figure>
      </div>
    </div>

    <div class="content-block">
      <h3>1.2.2 Out-of-Distribution Testing</h3>
      <p>
        The model was trained strictly on noise level $\sigma=0.5$. Here, I tested its generalization by feeding it images with 
        noise levels it was not trained on, ranging from $\sigma=0.0$ (clean) to $\sigma=1.0$ (very noisy).
      </p>

      <figure>
        <img src="media/partB/1_2_2.jpg" alt="Out of Distribution Testing">
        <figcaption>
          Denoising performance on varying $\sigma$. 
          <br>Top Row: Input (Noisy) | Bottom Row: Output (Denoised)
        </figcaption>
      </figure>

      <p class="muted">
        <b>Observation:</b> The model performs well when $\sigma$ is less than 0.5. However, at $\sigma$ near 1.0, it struggles to identify the structure, often hallucinating incorrect digits or failing to remove the noise entirely.
      </p>
    </div>

    <div class="content-block">
      <h3>1.2.3 Denoising Pure Noise</h3>
      <p>
        I attempted to train the denoiser to map 
        <b>Pure Noise</b> ($\sigma \to \infty$) directly to clean MNIST digits.
      </p>

      <h4>Training Loss & Results</h4>
      <figure>
        <img src="media/partB/1_2_3_loss.jpg" alt="Pure Noise Training Loss">
        <figcaption>Training Loss (Pure Noise)</figcaption>
      </figure>

      <div class="gallery" style="grid-template-columns: repeat(2, 1fr); gap: 20px;">
        <figure>
          <img src="media/partB/1_2_3_epoch_1.jpg" alt="Pure Noise Epoch 1">
          <figcaption>Epoch 1 (Pure Noise Input)</figcaption>
        </figure>
        <figure>
          <img src="media/partB/1_2_3_epoch_5.jpg" alt="Pure Noise Epoch 5">
          <figcaption>Epoch 5 (Pure Noise Input)</figcaption>
        </figure>
      </div>

      <h4 style="margin-top: 20px;">Analysis: Why does it look like this?</h4>
      <p>
        The generated images do not look like specific digits. Instead, they resemble a blurry, generic blob that looks like 
        a mix of all digits overlaid on top of each other.
      </p>
      <p>
        <b>Reason:</b> This happens because the problem is ill-posed. We are training the model to minimize the 
        <b>Mean Squared Error (MSE)</b> loss. Mathematically, the optimal solution that minimizes MSE for a given input 
        is the <b>conditional expectation</b> (the mean) of the possible targets. So the image the model produces is effectively the average of all digits in the dataset.
      </p>
    </div>
  
    <section id="B2" class="card">
        <h3>Part 2. Training a Flow Matching Model</h3>
  
        <h4>2.1 Adding Time Conditioning to UNet</h4>
        <p>
          One-step denoising can clean a digit if some signal remains, but it cannot <i>generate</i> from pure noise.
          Flow matching fixes this by learning a continuous-time transport from noise to data.
          I implement a time-conditioned UNet that takes <code class="inline">(x<sub>t</sub>, t)</code> as input and predicts a
          velocity field. I inject time information using an MLP/FCBlock that
          maps the scalar <code class="inline">t ∈ [0, 1]</code> into a feature embedding that conditions the UNet blocks.
        </p>
  
        <h4>2.2 Training the UNet (Flow Matching Objective)</h4>
        <p>
          For each batch of clean digits <code class="inline">x<sub>1</sub></code>, I sample a noise image
          <code class="inline">x<sub>0</sub> ~ N(0, I)</code> and a random time <code class="inline">t ~ Uniform(0, 1)</code>.
          I construct the interpolated point
          <code class="inline">x<sub>t</sub> = (1 − t) x<sub>0</sub> + t x<sub>1</sub></code>.
          The target velocity for this simple linear path is constant:
          <code class="inline">v<sub>target</sub> = x<sub>1</sub> − x<sub>0</sub></code>.
          The network predicts <code class="inline">v<sub>θ</sub>(x<sub>t</sub>, t)</code>, and I train with MSE
          <code class="inline">||v<sub>θ</sub> − v<sub>target</sub>||<sub>2</sub><sup>2</sup></code>.
        </p>
        <div class="grid">
            <figure>
                <img src="media/PartB/2_2_loss_curve.png" alt="B2.2 time-conditioned training loss curve">
                <figcaption>Training loss curve for time-conditioned flow matching.</figcaption>
              </figure>
        </div>
  
        <h4>2.3 Sampling from the Time-Conditioned UNet</h4>
        <p>
          I train the time-conditioned model with the recommended settings: batch size 64, hidden channels
          <code class="inline">D = 64</code>, and Adam with an initial learning rate <code class="inline">1e−2</code>
          plus an exponential LR decay schedule that drops by 10× over 10 epochs.
          To <b>sample</b>, I start from <code class="inline">x<sub>0</sub> ~ N(0, I)</code> and numerically integrate the learned
          velocity field from <code class="inline">t = 0</code> to <code class="inline">t = 1</code> using
          <code class="inline">num_ts = 50</code> steps (Euler updates). Each step applies:
          <code class="inline">x ← x + (Δt) · v<sub>θ</sub>(x, t)</code>.
          Over the trajectory, samples gradually sharpen from noise into digit-like structure.
        </p>
  
        <div class="grid">
          <figure>
            <img src="media/PartB/2_2_results_epoch1.png" alt="B2.3 samples after epoch 1">
            <figcaption>Samples from the time-conditioned model after epoch 1.</figcaption>
          </figure>
          <figure>
            <img src="media/PartB/2_2_results_epoch5.png" alt="B2.3 samples after epoch 5">
            <figcaption>Samples from the time-conditioned model after epoch 10 (digits become sharper and more coherent).</figcaption>
          </figure>
          <figure>
            <img src="media/PartB/2_2_results_epoch10.png" alt="B2.3 samples after epoch 10">
            <figcaption>Samples from the time-conditioned model after epoch 10 (digits become sharper and more coherent).</figcaption>
          </figure>
        </div>
  
        <h4>2.4 Adding Class-Conditioning to UNet</h4>
        <p>
          Next, I add label information so the model can generate a <b>specific digit class</b>.
          In the notebook, I implement <code class="inline">ClassConditionalUNet</code> by embedding the digit label
          <code class="inline">y ∈ {0,…,9}</code> and injecting it (along with the time embedding) into the UNet.
          This changes the learned vector field to <code class="inline">v<sub>θ</sub>(x<sub>t</sub>, t, y)</code>.
        </p>
  
        <h4>2.5 Training the UNet (Class Conditioning)</h4>
        <p>
          Training mirrors the time-only setup, except the UNet also receives the class label.
          I keep the same core hyperparameters (batch size 64, <code class="inline">D = 64</code>, Adam) and train for 10 epochs
          with exponential LR decay from <code class="inline">1e−2</code>.
          During training I periodically sample a grid of digits by fixing labels and sampling different noise seeds.
        </p>
        <div class="grid">
            <figure>
                <img src="media/PartB/2_6_loss_curve.png" alt="2.5 class-conditioned training loss curve">
                <figcaption>Training loss curve for the class-conditioned flow-matching model.</figcaption>
              </figure>
        </div>
  
        <h4>2.6 Sampling from the Class-Conditioned UNet + CFG</h4>
        <p>
          To strengthen class fidelity at sampling time, I use <b>classifier-free guidance</b>.
          The idea is to compute both an unconditional velocity and a conditional velocity, then interpolate:
          <code class="inline">v<sub>guided</sub> = v<sub>uncond</sub> + s · (v<sub>cond</sub> − v<sub>uncond</sub>)</code>.
          In practice, this requires the model to support unconditional behavior, which I implement via
          <b>label dropout</b> during training: with some probability I replace <code class="inline">y</code> with a special “null”
          label so the same network learns both modes.
        </p>
  
        <div class="grid">
            <figure>
              <img src="media/PartB/2_6_results_epoch1.png" alt="2.6 samples epoch 1">
              <figcaption>Class-conditioned samples after epoch 1 (labels start to control coarse shape).</figcaption>
            </figure>
            <figure>
              <img src="media/PartB/2_6_results_epoch5.png" alt="2.6 samples epoch 5">
              <figcaption>Class-conditioned samples after epoch 5 (labels start to control coarse shape).</figcaption>
            </figure>
              <figure>
                  <img src="media/PartB/2_6_results_epoch10.png" alt="2.6 samples epoch 10">
                  <figcaption>Class-conditioned samples after epoch 10 (stronger class fidelity + sharper strokes).</figcaption>
              </figure>
          </div>
      </section>
  
      <!-- ========================= B4 ========================= -->
      <section id="B4" class="card">
        <h3>Bells &amp; Whistles (Part B / CS280A)</h3>
  
        <h4>B4.1 Training a Class-Conditioned Model without LR Scheduler</h4>
        <p>
          As an ablation, I retrain the class-conditioned model using a <b>constant</b> learning rate
          (<code class="inline">1e−3</code>) and remove the exponential scheduler. This tests whether the scheduler is important
          for stability and final sample quality. In my notebook, I compare the loss curve and sample grids at epochs
          1, 5, and 10.
        </p>
  
        <h4>B4.2 “Better” Time-Only UNet (More Capacity + Longer Training)</h4>
        <p>
          I also improved the time-conditioned model by (1) increasing capacity
          <code class="inline">D: 64 → 128</code> and (2) training longer (<code class="inline">10 → 20</code> epochs) with a smaller
          learning rate (<code class="inline">1e−3</code>) for stability. I then generate a larger grid of samples (4×10)
          to qualitatively evaluate diversity and sharpness.
        </p>
  
        <div class="grid">
          <figure>
            <img src="media/PartB/bells_epoch1.png" alt="results after the first epoch">
            <figcaption>Results after the first epoch.</figcaption>
          </figure>
          <figure>
            <img src="media/PartB/bells_epoch5.png" alt="results after the 5th epoch">
            <figcaption>Results after the 5th epoch.</figcaption>
          </figure>
          <figure>
            <img src="media/PartB/bells_epoch10.png" alt="results after the 10th epoch">
            <figcaption>Results after the 10th epoch.</figcaption>
          </figure>
        </div>
      </section>
    </section>
    <!-- ========================= /PART B ========================= -->
  
    <!-- ========================= WHAT I LEARNED ========================= -->
    <section id="learned" class="card">
      <h2>What I Learned</h2>
      <ul>
        <li>
          <b>Single-step denoising is not a generator.</b> When the input contains no information (pure noise),
          MSE training encourages regression toward an average digit, motivating iterative generative dynamics.
        </li>
        <li>
          <b>Flow matching turns generation into learning a vector field.</b> By training a time-conditioned UNet to predict
          the velocity along an interpolation path, sampling becomes integrating an ODE from noise to data.
        </li>
        <li>
          <b>Conditioning matters.</b> Adding class conditioning and CFG can significantly improve class fidelity, but guidance
          must be tuned to balance fidelity vs diversity.
        </li>
      </ul>
    </section>

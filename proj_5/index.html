<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>CS280A/CS180 Project 5 — Fun With Diffusion Models (Part A + Part B)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root{
      --bg:#111; --fg:#eee; --muted:#b9b9b9; --card:#171717; --border:#2a2a2a;
      --accent:#7bdcff; --accent2:#b0ffb7;
    }
    *{box-sizing:border-box}
    body{
      font-family: system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",sans-serif;
      max-width: 980px; margin: 0 auto; padding: 32px 16px 80px;
      line-height: 1.65; background: var(--bg); color: var(--fg);
    }
    a{ color: var(--accent); text-decoration: none; }
    a:hover{ text-decoration: underline; }
    h1{ font-size: 2.0rem; margin: 0 0 8px; }
    h2{ font-size: 1.45rem; margin-top: 40px; }
    h3{ font-size: 1.15rem; margin-top: 26px; }
    p, li{ color: var(--fg); }
    .muted{ color: var(--muted); }
    .pill{
      display:inline-block; padding:4px 10px; border:1px solid var(--border);
      border-radius: 999px; font-size: 0.9rem; color: var(--muted);
      margin-right: 8px; margin-top: 8px;
    }
    .card{
      background: var(--card); border: 1px solid var(--border);
      border-radius: 16px; padding: 16px 16px; margin: 14px 0;
    }
    .toc a{ display:block; padding: 4px 0; color: var(--accent); }
    .grid{
      /* display:grid; gap: 12px;
      grid-template-columns: repeat(2, minmax(0, 1fr)); */
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(90px, 1fr));
      gap: 12px;
    }
    @media (max-width: 820px){ .grid{ grid-template-columns: 1fr; } }
    figure{ margin: 0; }
    img{
      width: 100%; height: auto; display:block;
      border-radius: 14px; border: 1px solid var(--border);
      background: #0c0c0c;
    }
    figcaption{
      font-size: 0.92rem; color: var(--muted);
      margin-top: 6px;
    }
    .three{ grid-template-columns: repeat(3, minmax(0, 1fr)); }
    @media (max-width: 980px){ .three{ grid-template-columns: 1fr; } }
    .twoWide{ grid-template-columns: 1fr 1fr; }
    .kpi{
      display:flex; flex-wrap:wrap; gap:10px; margin-top: 10px;
    }
    code.inline{
      background:#0e0e0e; border:1px solid var(--border);
      padding: 2px 6px; border-radius: 8px; color: #d7d7d7;
    }
    hr{ border: none; border-top: 1px solid var(--border); margin: 28px 0; }
    .note{ border-left: 3px solid var(--accent2); padding-left: 12px; color: var(--muted); }
  </style>
</head>

<body>
  <header class="card">
    <h1>Project 5 — Fun With Diffusion Models</h1>
    <div class="muted">
      <span class="pill">Author: Yuchen Zhang</span>
      <span class="pill">CS280A Fall 2025</span>
      <span class="pill">Topics: Diffusion + Flow Matching</span>
    </div>
    <p class="muted" style="margin-top:10px;">
      This project contains two parts:
      <b>Part A</b> explores a pretrained diffusion model, DeepFloyd IF, and experimented with sampling loops, inpainting, and optical illusions.
      <b>Part B</b> trains a generative model from scratch on the MNIST dataset using UNets and flow matching, also played with time/class conditioning to improve sampling quality.
    </p>
  </header>

  <section class="card toc">
    <h2 style="margin-top:0;">Table of Contents</h2>
    <a href="#partA">Part A — The Power of Diffusion Models</a>
    <a href="#A0">A0. Setup + Prompt Embeddings</a>
    <a href="#A1">A1. Sampling Loops (Forward, Denoising, CFG, i2i, Inpainting)</a>
    <a href="#A2">A2. Optical Illusions (Visual Anagrams, Hybrid Images)</a>
    <a href="#A3">A3. Bells & Whistles (Part A)</a>
    <a href="#partB">Part B — Flow Matching from Scratch</a>
    <a href="#B1">B1. Train a Single-Step Denoising UNet</a>
    <a href="#B2">B2. Train a Flow Matching Model (Time Conditioning)</a>
    <a href="#B3">B3. Add Class Conditioning + CFG + Simplify Training</a>
    <a href="#B4">B4. Bells & Whistles (Part B)</a>
    <a href="#learned">Takeaways</a>
  </section>

  <hr />

  <!-- ========================= PART A ========================= -->
  <section id="partA">
    <h2>Part A — The Power of Diffusion Models</h2>

    <p>
        In Part A, I explore the fundamental mechanics of diffusion models by manually implementing the sampling loops
    rather than relying on a "black box" pipeline. I start by visualizing the <b>forward process</b>, taking a clean
    test image (the Campanile) and gradually corrupting it with Gaussian noise. Then, I implement the reverse process,
    demonstrating how a model can recover the image through <b>iterative denoising</b>. Finally, I apply these
    concepts to creative tasks, including <b>inpainting</b> (restoring missing parts of an image),
    <b>Visual Anagrams</b> (optical illusions), and <b>Hybrid Images</b>.
      </p>

    <!-- A0 -->

<section id="A0" class="card">
  <h3>A0. Setup & Text Embeddings</h3>
  <p>
    For this project, I utilized the <b>DeepFloyd IF</b> diffusion pipeline, a pixel-based diffusion model. 
    DeepFloyd operates in a cascaded fashion: <b>Stage I</b> generates a base <b>64×64</b> image, which can then be passed to 
    <b>Stage II</b> for super-resolution upscaling to <b>256×256</b>.
  </p>
  
  <p>
    The model is conditioned on text, but it does not process raw strings directly. Instead, text prompts must be encoded into 
    high-dimensional vectors (embeddings) using a T5 encoder. Since this encoding process is computationally heavy, 
    I pre-computed the embeddings for a set of specific prompts. This allowed me to iterate quickly on the diffusion 
    mechanics (like noise levels and guidance scales) without re-running the text encoder every time.
  </p>
  
  <p>
    To ensure scientific reproducibility, I fixed the random seed to <b>404</b> for all experiments. This guarantees that 
    any visual differences observed in later sections are due to algorithmic changes (e.g., comparing random sampling vs. CFG) 
    rather than the inherent randomness of the noise generation.
  </p>

  <ul>
    <li>
      <b>Selected Prompts:</b> I chose three distinct prompts to test the model's ability to handle different subjects and styles:
      <ol>
        <li><i>"a UC Berkeley oski"</i> (Artistic/Landscape)</li>
        <li><i>"a man fighting with a tree"</i> (Portrait/Realistic)</li>
        <li><i>"a frozen university campus"</i> (Lighting/Texture)</li>
      </ol>
    </li>
    <li>
      <b>Inference Parameters:</b> I primarily utilized <code class="inline">num_inference_steps = 20</code> for sampling. 
      This provides a strong balance between generation speed and visual fidelity. Then I also experimented with num_inference_steps = 5, 10, 50 to see the effect of inference steps.
    </li>
  </ul>

  <p class="muted">
    <b>Observation:</b> Even at the coarse Stage I resolution (64x64), the model successfully captures the semantic core 
    of the prompts. The "a man fighting with a tree" and "a frozen university campus" both show good visual of the scene, capturing most of the semantic elements. For example, a university campus and snow in the later prompt. However, the "a UC Berkeley oski" is the least precise visual, none of them correctly displayed the true oski figure. This indicates the model cannot create concepts it never seen before.
    Also the number of inference steps play a vital role. The low steps often produced course and unprecise images, whereas the photos become more precise as the steps increase. For example, in the UC Berkeley oski case, the low steps had completely lost, whereas the high steps showed a university building implying that it realized the comcept of UC Berkeley.
  </p>

  <div class="grid">
        <!-- Prompt 1 -->
        <figure>
          <img src="media/partA/0_0/a_UC_Berkeley_oski_20_64.png" alt="Stage I owl (20 steps)">
          <figcaption>a UC Berkeley oski — Stage I (64×64), 20 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_UC_Berkeley_oski_20_256.png" alt="Stage II owl (20 steps)">
          <figcaption>a UC Berkeley oski — Stage II (256×256), 20 steps</figcaption>
        </figure>
      
        <!-- Prompt 2 -->
        <figure>
          <img src="media/partA/0_0/a_man_fighting_with_a_tree_20_64.png" alt="Stage I village (20 steps)">
          <figcaption>a man fighting with a tree — Stage I (64×64), 20 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_man_fighting_with_a_tree_20_256.png" alt="Stage II village (20 steps)">
          <figcaption>a man fighting with a tree — Stage II (256×256), 20 steps</figcaption>
        </figure>
      
        <!-- Prompt 3 -->
        <figure>
          <img src="media/partA/0_0/a_frozen_university_campus_20_64.png" alt="Stage I islands (20 steps)">
          <figcaption>a frozen university campus — Stage I (64×64), 20 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_frozen_university_campus_20_256.png" alt="Stage II islands (20 steps)">
          <figcaption>a frozen university campus — Stage II (256×256), 20 steps</figcaption>
        </figure>
      </div>
      
      <h3 style="margin-top:18px;">5 inference steps</h3>
      <div class="grid">
        <!-- Prompt 1 -->
        <figure>
          <img src="media/partA/0_0/a_UC_Berkeley_oski_5_64.png" alt="Stage I owl (20 steps)">
          <figcaption>a UC Berkeley oski — Stage I (64×64), 5 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_UC_Berkeley_oski_5_256.png" alt="Stage II owl (20 steps)">
          <figcaption>a UC Berkeley oski — Stage II (256×256), 5 steps</figcaption>
        </figure>
      
        <!-- Prompt 2 -->
        <figure>
          <img src="media/partA/0_0/a_man_fighting_with_a_tree_5_64.png" alt="Stage I village (20 steps)">
          <figcaption>a man fighting with a tree — Stage I (64×64), 5 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_man_fighting_with_a_tree_5_256.png" alt="Stage II village (20 steps)">
          <figcaption>a man fighting with a tree — Stage II (256×256), 5 steps</figcaption>
        </figure>
      
        <!-- Prompt 3 -->
        <figure>
          <img src="media/partA/0_0/a_frozen_university_campus_5_64.png" alt="Stage I islands (20 steps)">
          <figcaption>a frozen university campus — Stage I (64×64), 5 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_frozen_university_campus_5_256.png" alt="Stage II islands (20 steps)">
          <figcaption>a frozen university campus — Stage II (256×256), 5 steps</figcaption>
        </figure>
      </div>

      <h3 style="margin-top:18px;">10 inference steps</h3>
      <div class="grid">
        <!-- Prompt 1 -->
        <figure>
          <img src="media/partA/0_0/a_UC_Berkeley_oski_10_64.png" alt="Stage I owl (20 steps)">
          <figcaption>a UC Berkeley oski — Stage I (64×64), 10 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_UC_Berkeley_oski_10_256.png" alt="Stage II owl (20 steps)">
          <figcaption>a UC Berkeley oski — Stage II (256×256), 10 steps</figcaption>
        </figure>
      
        <!-- Prompt 2 -->
        <figure>
          <img src="media/partA/0_0/a_man_fighting_with_a_tree_10_64.png" alt="Stage I village (20 steps)">
          <figcaption>a man fighting with a tree — Stage I (64×64), 10 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_man_fighting_with_a_tree_10_256.png" alt="Stage II village (20 steps)">
          <figcaption>a man fighting with a tree — Stage II (256×256), 10 steps</figcaption>
        </figure>
      
        <!-- Prompt 3 -->
        <figure>
          <img src="media/partA/0_0/a_frozen_university_campus_10_64.png" alt="Stage I islands (20 steps)">
          <figcaption>a frozen university campus — Stage I (64×64), 10 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_frozen_university_campus_10_256.png" alt="Stage II islands (20 steps)">
          <figcaption>a frozen university campus — Stage II (256×256), 10 steps</figcaption>
        </figure>
      </div>

      <h3 style="margin-top:18px;">50 inference steps</h3>
      <div class="grid">
        <!-- Prompt 1 -->
        <figure>
          <img src="media/partA/0_0/a_UC_Berkeley_oski_50_64.png" alt="Stage I owl (20 steps)">
          <figcaption>a UC Berkeley oski — Stage I (64×64), 50 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_UC_Berkeley_oski_50_256.png" alt="Stage II owl (20 steps)">
          <figcaption>a UC Berkeley oski — Stage II (256×256), 50 steps</figcaption>
        </figure>
      
        <!-- Prompt 2 -->
        <figure>
          <img src="media/partA/0_0/a_man_fighting_with_a_tree_50_64.png" alt="Stage I village (20 steps)">
          <figcaption>a man fighting with a tree — Stage I (64×64), 50 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_man_fighting_with_a_tree_50_256.png" alt="Stage II village (20 steps)">
          <figcaption>a man fighting with a tree — Stage II (256×256), 50 steps</figcaption>
        </figure>
      
        <!-- Prompt 3 -->
        <figure>
          <img src="media/partA/0_0/a_frozen_university_campus_50_64.png" alt="Stage I islands (20 steps)">
          <figcaption>a frozen university campus — Stage I (64×64), 50 steps</figcaption>
        </figure>
        <figure>
          <img src="media/partA/0_0/a_frozen_university_campus_50_256.png" alt="Stage II islands (20 steps)">
          <figcaption>a frozen university campus — Stage II (256×256), 50 steps</figcaption>
        </figure>
      </div>
      
  </section>
  
</section>


    <!-- A1 -->
    <section id="A1" class="card">
      <h3>A1. Sampling Loops</h3>

      <h3 style="margin-top:10px;">A1.1 Implementing the Forward Process</h3>
    <p>
      The forward process produces a noisy image <b>x<sub>t</sub></b> from a clean image
      <b>x<sub>0</sub></b> by adding a Gaussian noise according to the noise schedule.
      
      Concretely, the equation at timestep <b>t</b>:
      <span class="muted">
        x<sub>t</sub> = √ᾱ<sub>t</sub> · x<sub>0</sub> + √(1 − ᾱ<sub>t</sub>) · ε,  where ε ~ N(0, I).
      </span>
  
      Earlier timesteps have ᾱ<sub>t</sub> close to 1, so the result is similar to the original image; later timesteps have
      ᾱ<sub>t</sub> near 0, so the result becomes nearly pure noise. I applied the
      forward process to the provided Campanile image at 3 timesteps
      (t = 250, 500, 750), where the result become more and more like pur noise.
    </p>
      <div class="grid">
        <figure>
          <img src="media/partA/1_1/campanile.jpg" alt="Campanile clean">
          <figcaption>Clean Campanile (t=0).</figcaption>
        </figure>
        <figure>
          <img src="media/partA/1_1/noise_t_250.png" alt="Campanile t=250">
          <figcaption>Noisy Campanile at t=250.</figcaption>
        </figure>
        <figure>
          <img src="media/partA/1_1/noise_t_500.png" alt="Campanile t=500">
          <figcaption>Noisy Campanile at t=500.</figcaption>
        </figure>
        <figure>
          <img src="media/partA/1_1/noise_t_750.png" alt="Campanile t=750">
          <figcaption>Noisy Campanile at t=750.</figcaption>
        </figure>
      </div>

      <h3>A1.2 Classical Denoising (Gaussian Blur)</h3>
      <p>
        I used a classical Gaussian blur filter as a baseline denoising method for comparison. Gaussian blur works well for
        low levels of noise but it cannot distinguish noise from fine image details. As the noise level increases, gaussian blur
        degrades rapidly as shown below.
      </p>
      <div>
        <figure>
          <img src="media/partA/1_2/gaussian_denoise_t_250.png" alt="Gaussian denoise t=250">
          <figcaption>Gaussian blur at t=250. </figcaption>
        </figure>
        <figure>
          <img src="media/partA/1_2/gaussian_denoise_t_500.png" alt="Gaussian denoise t=500">
          <figcaption>Gaussian blur at t=500.</figcaption>
        </figure>
        <figure>
          <img src="media/partA/1_2/gaussian_denoise_t_750.png" alt="Gaussian denoise t=750">
          <figcaption>Gaussian blur at t=750.</figcaption>
        </figure>
      </div>

      <h3>A1.3 One-Step Denoising (Pretrained Diffusion Denoiser)</h3>
      <p>
        Next, I use the pretrained DeepFloyd Stage-I UNet to perform a single denoising step. First I used the forward function to add noise to the original image, then I passed the noisy image and the corresponding timestep t to the UNet to predict the noise ε̂. Finally, I computed the denoised image x̂<sub>0</sub> using the predicted noise:
        <span class="muted">
          x̂<sub>0</sub> = (x<sub>t</sub> − √(1 − ᾱ<sub>t</sub>) · ε̂) / √ᾱ<sub>t</sub>
        </span>.
        This effectively “subtracts” the predicted noise from x<sub>t</sub> to recover an estimate of the clean image.
      </p>

      <div>
        <figure>
          <img src="media/partA/1_3/reconstructed_t_250.png" alt="One-step t=250">
          <figcaption>One-step denoise at t=250 </figcaption>
        </figure>
        <figure>
          <img src="media/partA/1_3/reconstructed_t_500.png" alt="One-step t=500">
          <figcaption>One-step denoise at t=500 </figcaption>
        </figure>
        <figure>
          <img src="media/partA/1_3/reconstructed_t_750.png" alt="One-step t=750">
          <figcaption>One-step denoise at t=750 </figcaption>
        </figure>
      </div>
      
      <section id="A1-4" class="card">
  <h3>1.4 Iterative Denoising</h3>
  
  <p>
    As demonstrated in the previous section, attempting to recover a clean image from high noise levels in a single step is an ill-posed problem, resulting in blurry, averaged outputs. Diffusion models overcome this by removing noise <b>iteratively</b>.
  </p>

  <p>
    However, running the full diffusion process (e.g., 1000 steps) is computationally expensive. To speed this up, I implemented <b>Strided Sampling</b>. I created a schedule of timesteps starting at <code>t=990</code> and stepping down by 30 until reaching 0. This allows us to skip steps while still adhering to the diffusion dynamics.
  </p>

  <p>
    At each step, we move from a noisier timestep <i>t</i> to a less noisy timestep <i>t'</i>. The update rule involves estimating the clean image, then re-noising it slightly to match the target noise variance of <i>t'</i>. This can be thought of as a weighted interpolation:
  </p>

  <div class="math-block" style="background: rgba(255,255,255,0.05); padding: 15px; border-radius: 6px; text-align: center; margin-bottom: 25px;">
    <i>x<sub>t'</sub></i> = (Interpolation of <i>x<sub>0</sub></i>) + (Interpolation of <i>x<sub>t</sub></i>) + (Random Variance)
  </div>

  <h4>The Denoising Process</h4>
  <p>
    I started with the Campanile image noised to index 10 (approx t=690) and ran the iterative denoiser. Below, I visualize the process by displaying the image at every 5th step of the loop. Notice how the structure gradually emerges from the chaos.
  </p>

  <div class="grid">
    <figure>
      <img src="media/partA/1_4/iter_step_10_t_690.png" alt="t = 690">
      <figcaption>t = 690</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_4/iter_step_15_t_540.png" alt="t = 540">
      <figcaption>t = 540</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_4/iter_step_20_t_390.png" alt="t = 390">
      <figcaption>t = 390</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_4/iter_step_25_t_240.png" alt="t = 240">
      <figcaption>t = 240</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_4/iter_step_30_t_90.png" alt="t = 90">
      <figcaption>t = 90</figcaption>
    </figure>
  </div>

  <h4>Final Comparison</h4>
  <p>
    Here is the final result of the iterative method compared against the baseline methods. The iterative approach recovers fine details (like the brick texture and scaffolding) that the One-Step method blurs out and Gaussian Blur destroys completely.
  </p>

  <div class="grid">
    <figure>
      <img src="media/partA/1_4/comparison.png" alt="Iterative Result">
      <figcaption>Final Iterative Result Comparison</figcaption>
    </figure>
  </div>

  <section id="A1-5" class="card">
  <h3>1.5 Diffusion Model Sampling</h3>
  
  <p>
    The <code>iterative_denoise</code> function is not limited to restoring noisy images; it can also act as a generative tool. 
    By setting <code>i_start = 0</code> and passing pure Gaussian noise as the input, the model hallucinates a completely new image from scratch.
  </p>

  <p>
    I used this method to generate 5 random samples based on the prompt <i>"a high quality photo"</i>. 
    The results below demonstrate the model's ability to form coherent structures (objects, landscapes, etc.) from pure randomness, 
    although the quality is limited without the use of Classifier-Free Guidance (which is explored in the next section).
  </p>

  <div class="grid">
    <figure>
      <img src="media/partA/1_5/sample_1.png" alt="Generated Sample 1">
      <figcaption>Sample 1</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_5/sample_2.png" alt="Generated Sample 2">
      <figcaption>Sample 2</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_5/sample_3.png" alt="Generated Sample 3">
      <figcaption>Sample 3</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_5/sample_4.png" alt="Generated Sample 4">
      <figcaption>Sample 4</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_5/sample_5.png" alt="Generated Sample 5">
      <figcaption>Sample 5</figcaption>
    </figure>
  </div>
</section>

<section id="A1-6" class="card">
  <h3>1.6 Classifier-Free Guidance (CFG)</h3>

  <p>
    As observed in the previous section, generating images purely from the prior often results in incoherent or "nonsensical" outputs. 
    To significantly improve image quality, I implemented <b>Classifier-Free Guidance (CFG)</b>.
  </p>

  <p>
    CFG works by computing two noise estimates at every timestep:
    <ul>
      <li><b>Conditional noise estimate ($\epsilon_c$):</b> Conditioned on the text prompt (e.g., "a high quality photo").</li>
      <li><b>Unconditional noise estimate ($\epsilon_u$):</b> Conditioned on a null prompt (an empty string <code>""</code>).</li>
    </ul>
    The final noise estimate is computed by extrapolating the difference between these two vectors:
  </p>

  <div class="math-block" style="background: rgba(255,255,255,0.05); padding: 15px; border-radius: 6px; text-align: center; margin-bottom: 25px; font-family: 'Times New Roman', serif; font-size: 1.2rem;">
    $\epsilon = \epsilon_u + \gamma (\epsilon_c - \epsilon_u)$
  </div>

  <p>
    Here, <b>$\gamma$</b> is the guidance scale. When $\gamma > 1$, the model pushes the generation <i>away</i> from the generic/average image 
    and <i>towards</i> the specific text prompt.
  </p>

  <p>
    I updated the iterative sampling loop to include CFG (using a scale of $\gamma = 7$). Below are 5 samples generated 
    using the prompt <i>"a high quality photo"</i>. Compared to the non-guided samples in section 1.5, these are significantly 
    sharper, more coherent, and realistic.
  </p>

  <div class="grid">
    <figure>
      <img src="media/partA/1_6/cfg_sample_1.png" alt="CFG Sample 1">
      <figcaption>CFG Sample 1</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_6/cfg_sample_2.png" alt="CFG Sample 2">
      <figcaption>CFG Sample 2</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_6/cfg_sample_3.png" alt="CFG Sample 3">
      <figcaption>CFG Sample 3</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_6/cfg_sample_4.png" alt="CFG Sample 4">
      <figcaption>CFG Sample 4</figcaption>
    </figure>
    <figure>
      <img src="media/partA/1_6/cfg_sample_5.png" alt="CFG Sample 5">
      <figcaption>CFG Sample 5</figcaption>
    </figure>
  </div>
</section>

      <section id="A1-7" class="card">
  <h3>1.7 Image-to-Image Translation</h3>

  <p>
    In this section, I implemented the <b>SDEdit</b> algorithm. The core idea is simple but powerful: we take a real image, add a specific amount of noise to it (jumping to timestep <i>t</i>), and then run the iterative denoising process to clear it up.
  </p>
  <p>
    The noise level determines the "creativity" of the edit.
    <ul>
      <li><b>Low noise (e.g., i_start=1):</b> The model makes minor edits, staying very close to the original image.</li>
      <li><b>High noise (e.g., i_start=20):</b> The original structure is heavily corrupted, forcing the model to "hallucinate" new details to project it back onto the natural image manifold.</li>
    </ul>
  </p>

  <h4 style="margin-top: 30px; border-bottom: 1px solid #333; padding-bottom: 5px;">1.7.0 SDEdit: Noise Levels</h4>
  <p>
    Below, I ran SDEdit on the Campanile and two other test images of my own with varying starting noise indices [1, 3, 5, 7, 10, 20]. 
    The image becomes more abstract and distinct from the original as the noise level increases.
    I used the prompt <i>"a high quality photo"</i> and null as the unconditional prompt.
  </p>

  <h5>Test Image 1: Campanile</h5>
  <div class="grid">
    <figure><img src="media/partA/1_7/campanile.jpg"><figcaption>Original</figcaption></figure>
    <figure><img src="media/partA/1_7/campanile_edit_i_start_1.png"><figcaption>i=1</figcaption></figure>
    <figure><img src="media/partA/1_7/campanile_edit_i_start_3.png"><figcaption>i=3</figcaption></figure>
    <figure><img src="media/partA/1_7/campanile_edit_i_start_5.png"><figcaption>i=5</figcaption></figure>
    <figure><img src="media/partA/1_7/campanile_edit_i_start_7.png"><figcaption>i=7</figcaption></figure>
    <figure><img src="media/partA/1_7/campanile_edit_i_start_10.png"><figcaption>i=10</figcaption></figure>
    <figure><img src="media/partA/1_7/campanile_edit_i_start_20.png"><figcaption>i=20</figcaption></figure>
  </div>

  <h5>Test Image 2: Labubu</h5>
  <div class="grid">
    <figure><img src="media/partA/1_7/labubu2.jpg"><figcaption>Original</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg1_i_start_1.png"><figcaption>i=1</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg1_i_start_3.png"><figcaption>i=3</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg1_i_start_5.png"><figcaption>i=5</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg1_i_start_7.png"><figcaption>i=7</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg1_i_start_10.png"><figcaption>i=10</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg1_i_start_20.png"><figcaption>i=20</figcaption></figure>
  </div>

  <h5>Test Image 3: [Your Image Name]</h5>
  <div class="grid">
    <figure><img src="media/partA/1_7/toy.jpg"><figcaption>Original</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg2_i_start_1.png"><figcaption>i=1</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg2_i_start_3.png"><figcaption>i=3</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg2_i_start_5.png"><figcaption>i=5</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg2_i_start_7.png"><figcaption>i=7</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg2_i_start_10.png"><figcaption>i=10</figcaption></figure>
    <figure><img src="media/partA/1_7/myimg2_i_start_20.png"><figcaption>i=20</figcaption></figure>
  </div>

  <h4 style="margin-top: 30px; border-bottom: 1px solid #333; padding-bottom: 5px;">1.7.1 Editing Hand-Drawn & Web Images</h4>
  <p>
    This procedure works exceptionally well for projecting non-realistic images (like sketches) onto the natural image manifold. 
    I took a web image (Tiger) and 2 hand-drawn sketches (Car and Cat) and ran them through the same pipeline.
  </p>

  <h5>Web Image: Tiger</h5>
  <div class="grid">
    <figure><img src="media/partA/1_7_1/tiger.jpg"><figcaption>Original</figcaption></figure>
    <figure><img src="media/partA/1_7_1/web_dragon_i_start_1.png"><figcaption>i=1</figcaption></figure>
    <figure><img src="media/partA/1_7_1/web_dragon_i_start_3.png"><figcaption>i=3</figcaption></figure>
    <figure><img src="media/partA/1_7_1/web_dragon_i_start_5.png"><figcaption>i=5</figcaption></figure>
    <figure><img src="media/partA/1_7_1/web_dragon_i_start_7.png"><figcaption>i=7</figcaption></figure>
    <figure><img src="media/partA/1_7_1/web_dragon_i_start_10.png"><figcaption>i=10</figcaption></figure>
    <figure><img src="media/partA/1_7_1/web_dragon_i_start_20.png"><figcaption>i=20</figcaption></figure>
  </div>

  <h5>Hand-Drawn: Car Sketch</h5>
  <div class="grid">
    <figure><img src="media/partA/1_7_1/hand_drawn_car_i_start_20.png"><figcaption>Original</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_car_i_start_1.png"><figcaption>i=1</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_car_i_start_3.png"><figcaption>i=3</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_car_i_start_5.png"><figcaption>i=5</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_car_i_start_7.png"><figcaption>i=7</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_car_i_start_10.png"><figcaption>i=10</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_car_i_start_20.png"><figcaption>i=20</figcaption></figure>
  </div>
  <h5>Hand-Drawn: Cat Sketch</h5>
  <div class="grid">
    <figure><img src="media/partA/1_7_1/myImage.png"><figcaption>Original</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_cat_i_start_1.png"><figcaption>i=1</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_cat_i_start_3.png"><figcaption>i=3</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_cat_i_start_5.png"><figcaption>i=5</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_cat_i_start_7.png"><figcaption>i=7</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_cat_i_start_10.png"><figcaption>i=10</figcaption></figure>
    <figure><img src="media/partA/1_7_1/hand_drawn_cat_i_start_20.png"><figcaption>i=20</figcaption></figure>
  </div>

  <h4 style="margin-top: 30px; border-bottom: 1px solid #333; padding-bottom: 5px;">1.7.2 Inpainting</h4>
  <p>
    I implemented the <b>RePaint</b> algorithm to perform inpainting. By using a binary mask, we can force the model to keep the original pixels in the unmasked areas (<i>mask=0</i>) while generating new content in the masked areas (<i>mask=1</i>).
  </p>
  <p>
    This is achieved by modifying the sampling loop: at every step, after the model predicts $x_{t-1}$, we replace the unmasked pixels with the noisy original image at that same timestep.
  </p>
  <p>
    I experimented with inpainting on the Campanile image as well as two custom images of my own. The results below show the original image, the mask used for inpainting, and the final inpainted result. For my own images, I'm showing the mask using a red box for clarity, and inpainting the image by generate a new image where the mask is at the center of the patch. This is because the model will only generate contents at the center of an image, so cropping the mask to the center allows for better inpainting results.
  </p>

  <div class="grid">
    <figure><img src="media/partA/1_7_2/campanile.jpg"><figcaption>campanile</figcaption></figure>

    <figure><img src="media/partA/1_7_2/laptop.jpg"><figcaption>laptop</figcaption></figure>

    <figure><img src="media/partA/1_7_2/toy.jpg"><figcaption>toy</figcaption></figure>
  </div>

  <h4 style="margin-top: 30px; border-bottom: 1px solid #333; padding-bottom: 5px;">1.7.4 Text-Conditional Image-to-Image Translation</h4>
  <p>
    Finally, I extended SDEdit to use specific text prompts rather than just "a high quality photo". This allows me to guide the generation process. For example, I first take the Campanile and guide it toward a "pencil". Then I used two of my own images and guided them toward different prompts as shown here.
  </p>

  <h5>Campanile -> "Pencil"</h5>
  <div class="grid">
    <figure><img src="media/partA/1_7_3/campanile/campanile.jpg"><figcaption>Original</figcaption></figure>
    <figure><img src="media/partA/1_7_3/campanile/edit_i_start_1.png"><figcaption>i=1</figcaption></figure>
    <figure><img src="media/partA/1_7_3/campanile/edit_i_start_3.png"><figcaption>i=3</figcaption></figure>
    <figure><img src="media/partA/1_7_3/campanile/edit_i_start_5.png"><figcaption>i=5</figcaption></figure>
    <figure><img src="media/partA/1_7_3/campanile/edit_i_start_7.png"><figcaption>i=7</figcaption></figure>
    <figure><img src="media/partA/1_7_3/campanile/edit_i_start_10.png"><figcaption>i=10</figcaption></figure>
    <figure><img src="media/partA/1_7_3/campanile/edit_i_start_20.png"><figcaption>i=20</figcaption></figure>
  </div>

  <h5>Labubu -> Dog</h5>
  <div class="grid">
    <figure><img src="media/partA/1_7_3/mydraw1/labubu2.jpg"><figcaption>Original</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw1/edit_i_start_1.png"><figcaption>i=1</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw1/edit_i_start_3.png"><figcaption>i=3</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw1/edit_i_start_5.png"><figcaption>i=5</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw1/edit_i_start_7.png"><figcaption>i=7</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw1/edit_i_start_10.png"><figcaption>i=10</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw1/edit_i_start_20.png"><figcaption>i=20</figcaption></figure>
  </div>
  
  <h5>Toy -> Man wearing a hat</h5>
  <div class="grid">
    <figure><img src="media/partA/1_7_3/mydraw2/toy.jpg"><figcaption>Original</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw2/edit_i_start_1.png"><figcaption>i=1</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw2/edit_i_start_3.png"><figcaption>i=3</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw2/edit_i_start_5.png"><figcaption>i=5</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw2/edit_i_start_7.png"><figcaption>i=7</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw2/edit_i_start_10.png"><figcaption>i=10</figcaption></figure>
    <figure><img src="media/partA/1_7_3/mydraw2/edit_i_start_20.png"><figcaption>i=20</figcaption></figure>
  </div>

</section>

    <h3>A1.8 Visual Anagrams</h3>
<p>
  In this section, I generate a <b>visual anagram</b>: a single image that matches one prompt when viewed normally,
  but becomes a different, coherent image when rotated 180° (flipped upside down). The key trick is that during
  <b>every</b> reverse-diffusion step, I enforce consistency with <b>two</b> prompts under <b>two</b> views of the
  same latent image. Concretely, the “upright” view is guided by Prompt A, while the “flipped” view is guided by
  Prompt B. By combining the two guidance signals at each timestep, the final sample is forced to contain features
  that can be explained by Prompt A in the upright orientation and by Prompt B in the flipped orientation.
</p>

<div class="card note">
  <b>High-level idea:</b> compute two noise predictions (upright + flipped), align them into the same coordinate
  system, then average them to produce one update that satisfies both prompts.
</div>

<p>
  <b>Denoising with classifier-free guidance (CFG).</b> For a given timestep <code class="inline">t</code> and latent
  image <code class="inline">x_t</code>, the diffusion UNet predicts noise. CFG improves prompt adherence by mixing
  an unconditional and conditional prediction:
</p>

<p class="muted">
  ε̂ = ε̂<sub>uncond</sub> + s · (ε̂<sub>cond</sub> − ε̂<sub>uncond</sub>),
  where <b>s</b> is the guidance scale.
</p>

<p>
  <b>Visual-anagram update rule.</b> At each timestep, I compute two CFG noise estimates:
</p>
<ul>
  <li>
    <b>Upright branch (Prompt A):</b>
    ε̂<sub>A</sub> = CFG(UNet(x<sub>t</sub>, t, Prompt A)).
  </li>
  <li>
    <b>Flipped branch (Prompt B):</b>
    first rotate the current latent by 180°,
    x̃<sub>t</sub> = flip(x<sub>t</sub>),
    then compute noise under Prompt B,
    ε̃̂<sub>B</sub> = CFG(UNet(x̃<sub>t</sub>, t, Prompt B)),
    and rotate the noise prediction back,
    ε̂<sub>B</sub> = flip(ε̃̂<sub>B</sub>).
  </li>
</ul>

<p>
  Since ε̂<sub>A</sub> and ε̂<sub>B</sub> are now expressed in the <b>same</b> (upright) coordinates, I combine them
  by simple averaging:
</p>

<p class="muted">
  ε̂<sub>final</sub> = (ε̂<sub>A</sub> + ε̂<sub>B</sub>) / 2
</p>

<p>
  Finally, I use ε̂<sub>final</sub> in the standard DDPM reverse update to obtain x<sub>t−1</sub>. Intuitively, the
  model is “pulled” toward satisfying Prompt A in the normal view while simultaneously being “pulled” toward
  satisfying Prompt B when the image is flipped. Because this constraint is applied at <b>every</b> timestep, the
  illusion is not a post-processing trick; it is baked into the sampling trajectory.
</p>

<p class="muted">
  <b>Qualitative observation:</b> When the prompts are too semantically different, the averaged guidance can create
  compromise artifacts (e.g., muddier textures or ambiguous shapes). Prompts with compatible global composition
  (similar scene layout) typically produce stronger, cleaner anagrams.
</p>

<div class="grid">
  <figure>
    <img src="media/PartA/part1/8/dragon.png" alt="Anagram upright view (Prompt A)">
    <figcaption>Upright view (Dragon).</figcaption>
  </figure>
  <figure>
    <img src="media/PartA/part1/8/island.png" alt="Anagram flipped view (Prompt B)">
    <figcaption>Flipped view (Island).</figcaption>
  </figure>
</div>

<div class="grid">
    <figure>
      <img src="media/PartA/part1/8/cherry_blossom.png" alt="Anagram upright view (Prompt A)">
      <figcaption>Upright view (Cherry Blossom).</figcaption>
    </figure>
    <figure>
      <img src="media/PartA/part1/8/village.png" alt="Anagram flipped view (Prompt B)">
      <figcaption>Flipped view (Village).</figcaption>
    </figure>
  </div>


<h3>A1.9 Hybrid Images (Diffusion Edition)</h3>
<p>
  In this section, I create <b>hybrid images</b>, where the output appears like one concept when viewed from far
  away (or blurred), but reveals a different concept when viewed up close. This mirrors the classic hybrid-image
  construction (low-pass one image + high-pass another), except here I perform the “frequency split” in
  <b>diffusion guidance space</b> rather than directly combining two RGB images.
</p>

<div class="card note">
  <b>Core idea:</b> at each timestep, compute two guided noise estimates (Prompt A and Prompt B), then take
  <b>low-frequency</b> components from one and <b>high-frequency</b> components from the other, and recombine them
  into a single ε̂ used for the reverse update.
</div>

<p>
  <b>Step 1 — compute guided noise for both prompts.</b> For the same latent x<sub>t</sub> and timestep t:
</p>
<ul>
  <li>ε̂<sub>A</sub> = CFG(UNet(x<sub>t</sub>, t, Prompt A))</li>
  <li>ε̂<sub>B</sub> = CFG(UNet(x<sub>t</sub>, t, Prompt B))</li>
</ul>

<p>
  <b>Step 2 — separate frequencies using a Gaussian low-pass filter.</b> Let LP(·) be a Gaussian blur operator
  (implemented with the project-recommended parameters, e.g., kernel size 33 and sigma 2). Then:
</p>
<ul>
  <li><b>Low-frequency component:</b> ε̂<sub>low</sub> = LP(ε̂<sub>A</sub>)</li>
  <li><b>High-frequency component:</b> ε̂<sub>high</sub> = ε̂<sub>B</sub> − LP(ε̂<sub>B</sub>)</li>
</ul>

<p>
  <b>Step 3 — recombine into a hybrid guidance signal.</b> The final noise estimate used for the DDPM reverse step is:
</p>

<p class="muted">
  ε̂<sub>hybrid</sub> = ε̂<sub>low</sub> + ε̂<sub>high</sub>
</p>

<p>
  This construction gives Prompt A control over the <b>global structure</b> (overall shapes, layout, large-scale
  shading), while Prompt B injects <b>fine details</b> (textures, edges, small semantic features). Because the
  combination happens inside the sampling loop, the diffusion process learns to satisfy both constraints over time:
  the sample stabilizes its coarse composition according to Prompt A while progressively sharpening with Prompt B’s
  high-frequency details.
</p>

<p class="muted">
  <b>How I evaluate the illusion:</b> To verify the hybrid effect, I visualize the result at multiple scales:
  (1) zoomed out / downsampled (emphasizes low frequencies) and (2) zoomed in (reveals high-frequency details).
  A strong hybrid should clearly shift “interpretation” depending on viewing distance.
</p>

<div class="grid">
  <figure>
    <img src="media/PartA/part1/9/wolf_knight.png" alt="Hybrid, wolf and knight">
    <figcaption>Hybrid, wolf and knight</figcaption>
  </figure>
</div>
<p>Low-frequency prompt: a realistic portrait of a knight wearing polished silver armor</p>
<p>High-frequency prompt: a realistic image of a wolf standing on a cliff under a full moon</p>

<div class="grid">
    <figure>
      <img src="media/PartA/part1/9/hybrid.png" alt="Hybrid, man and village">
      <figcaption>Hybrid, wolf and knight</figcaption>
    </figure>
  </div>
  <p>Low-frequency prompt: an oil painting of a snowy mountain village</p>
  <p>High-frequency prompt: a man wearing a hat</p>

  <h2>Bells & Whistles</h2>
  <p>
    For Part A bells & whistles, I explored two additional transformation-based visual anagrams beyond the standard 180°
    flip, and I also experimented with text-conditioned image-to-image translation (SDEdit) on a custom course logo.
  </p>
  
  <h3>More Visual Anagrams: 90° Rotation</h3>
  <p>
    In the required visual anagram (Part 1.8), the “second view” is created by flipping the latent image 180°. Here, I
    extend the same idea to a different transformation: a <b>90° rotation</b>. The denoising loop stays identical in
    spirit: at each timestep, I compute the CFG noise prediction for Prompt A on the original latent, then compute the
    CFG noise prediction for Prompt B on a <b>rotated</b> version of the latent, rotate that predicted noise back to the
    original coordinate system, and average the two noise estimates to produce a single update.
  </p>
  <p class="muted">
    Intuition: Prompt A controls the upright interpretation, while Prompt B controls the interpretation after rotating
    the image by 90°. Enforcing both constraints at every timestep produces a single image that can be read in two ways
    depending on the rotation.
  </p>
  
  <div class="grid">
    <figure>
      <img src="media/PartA/part1/bells/rotate.png" alt="Rotation anagram: view 1 (Prompt A)">
      <figcaption><b>View 1 (normal):</b> aligns with Prompt A.</figcaption>
    </figure>
    <figure>
      <img src="media/PartA/part1/bells/rotated.png" alt="Rotation anagram: view 2 (Prompt B, rotated 90 degrees)">
      <figcaption><b>View 2 (rotated 90°):</b> aligns with Prompt B.</figcaption>
    </figure>
  </div>
  <p>Prompt A: an artistic painting of a dragon flying over ancient mountains</p>
  <p>Prompt B: a watercolor painting of cherry blossom trees in spring</p>
  
  <hr/>
  
  <h3>More Visual Anagrams: Jigsaw (Patch Permutation)</h3>
  <p>
    I also implemented a more aggressive transformation: a <b>jigsaw rearrangement</b>. Instead of rotating or flipping,
    the transformation divides the image into a grid of patches and permutes them using a fixed permutation. During
    sampling, Prompt A is applied to the normal latent. Prompt B is applied to a <b>jigsaw-transformed</b> latent; its
    predicted noise is then mapped back through the inverse permutation before averaging with Prompt A’s noise.
  </p>
  <p class="muted">
    This produces an illusion where the image looks like Prompt A in the original layout, but when rearranged into the
    same patch ordering (the jigsaw view), it reveals Prompt B.
  </p>
  
  <div class="grid">
    <figure>
      <img src="media/PartA/part1/bells/jigsaw.png" alt="Jigsaw anagram: view 1 (normal)">
      <figcaption><b>View 1 (normal layout):</b> aligns with Prompt A.</figcaption>
    </figure>
    <figure>
      <img src="media/PartA/part1/bells/jigsaw_rearranged.png" alt="Jigsaw anagram: view 2 (rearranged)">
      <figcaption><b>View 2 (jigsaw rearranged):</b> reveals Prompt B.</figcaption>
    </figure>
  </div>
  <p>Prompt A: a peaceful landscape of rolling green hills at sunrise</p>
  <p>Prompt B: a vibrant image of a bustling street market in the rain</p>
  
  <hr/>
  
  <h3>Design a Course Logo + SDEdit Image-to-Image Translation</h3>
  <p>
    For the “design a course logo” bells & whistles option, I created a simple custom logo image and then applied
    <b>SDEdit-style image-to-image translation</b>. Concretely, I start from the logo image, add noise corresponding to
    a chosen start timestep (controlled by a <code class="inline">start_index</code>), and then run the reverse diffusion
    process conditioned on a target text prompt. A smaller start index preserves more of the original logo structure,
    while a larger start index gives the model more freedom to change the image.
  </p>
  <p class="muted">
    In my experiment, I used a moderate start index (e.g., 10) to keep the logo’s overall structure while translating it
    into the style described by the text prompt. After generating the 64×64 result from Stage 1, I used Stage 2 to
    upsample to 256×256 for cleaner visualization.
  </p>
  
  <div class="grid">
    <figure>
      <img src="media/PartA/part1/bells/logo.png" alt="Original custom course logo">
      <figcaption><b>Original logo</b> (input image).</figcaption>
    </figure>
    <figure>
      <img src="media/PartA/part1/bells/low_res_logo.png" alt="Edited logo low resolution (Stage 1)">
      <figcaption><b>Edited (Stage 1, 64×64)</b> using SDEdit.</figcaption>
    </figure>
    <figure>
      <img src="media/PartA/part1/bells/high_res_logo.png" alt="Edited logo high resolution (Stage 2)">
      <figcaption><b>Edited (Stage 2, 256×256)</b> upsampled result.</figcaption>
    </figure>
  </div>
  <p>Prompt: "a watercolor painting of cherry blossom trees in spring"</p>
  
    <!-- ========================= PART B ========================= -->
<section id="partB">
    <h2>Part B — Flow Matching from Scratch</h2>
  
    <p>
      In Part B, I train a generative model on <b>MNIST</b> from scratch. I start with a warm-up task:
      implement a lightweight <b>single-step denoising UNet</b> and train it to map a noisy digit back to
      a clean digit using an L2 reconstruction loss. :contentReference[oaicite:1]{index=1}
      Then I move to <b>flow matching</b>, where the UNet learns a <i>time-dependent velocity field</i> that
      transports samples from a simple noise distribution to the data distribution via ODE integration.
    </p>
  
    <!-- ========================= B1 ========================= -->
    <section id="B1" class="card">
      <h3>B1. Train a Single-Step Denoising UNet</h3>
  
      <h4>B1.1 Implementing the UNet</h4>
      <p>
        I implemented the UNet architecture specified in the handout using a small set of reusable tensor
        operations: <code class="inline">Conv</code> (resolution-preserving), <code class="inline">DownConv</code>
        (stride-2 downsample), <code class="inline">UpConv</code> (transpose-conv upsample),
        <code class="inline">Flatten</code> (avgpool 7×7 → 1×1 bottleneck), <code class="inline">Unflatten</code>
        (1×1 → 7×7), and <code class="inline">Concat</code> for skip connections. :contentReference[oaicite:2]{index=2}
      </p>
  
      <ul>
        <li>
          <b>Encoder (down path):</b> progressively reduces spatial resolution while increasing channels.
          This lets the network capture larger receptive fields and global digit structure.
        </li>
        <li>
          <b>Bottleneck:</b> the <code class="inline">Flatten</code> step compresses the 7×7 feature map into a
          1×1 representation, forcing the model to summarize global information.
        </li>
        <li>
          <b>Decoder (up path):</b> upsamples back to 28×28, using <b>skip connections</b> to re-inject
          fine-grained spatial details (strokes/edges) from earlier layers.
        </li>
      </ul>
  
      <p class="muted">
        <b>Why UNet?</b> For denoising, we need both (1) global context (what digit is it / overall shape)
        and (2) local detail (clean strokes). UNet’s multi-scale pathway + skip connections is a strong
        inductive bias for this.
      </p>
  
      <h4>B1.2 Using the UNet to Train a Denoiser</h4>
      <p>
        To train the one-step denoiser, I generate noisy/clean pairs by corrupting clean MNIST images
        with Gaussian noise:
        <code class="inline">z = x + σ · ε</code>, where <code class="inline">ε ~ N(0, I)</code>.
        The network <code class="inline">Dθ(z)</code> is trained to directly predict <code class="inline">x</code>
        using an L2 loss:
        <code class="inline">E[ || Dθ(z) − x ||² ]</code>. :contentReference[oaicite:3]{index=3}
      </p>

      <div class="grid">
        <figure>
          <img src="media/PartB/1_2_0_noisy.png" alt="B1 denoising examples">
          <figcaption> Visualization of the noisy images</figcaption>
        </figure>
      </div>
  
      <h4>B1.2.1 Training</h4>
      <p>
        During training I fix a noise level (e.g. <code class="inline">σ = 0.5</code>) and repeatedly sample
        batches of MNIST digits, corrupt them into <code class="inline">z</code>, and optimize the UNet to
        minimize reconstruction error. This setup forces the model to learn a “typical” denoising transform
        for that corruption strength.
      </p>
  
      <div class="grid">
        <figure>
          <img src="media/PartB/1_2_1_training_curve.png" alt="B1 training loss curve">
          <figcaption>Training loss curve for the single-step denoiser.</figcaption>
        </figure>
        <figure>
          <img src="media/PartB/1_2_1_training_e1.png" alt= "Training examples at epoch 1">
          <figcaption>Results on the test set with noise level 0.5 after the first epoch</figcaption>
        </figure>
        <figure>
            <img src="media/PartB/1_2_1_training_e5.png" alt= "Training examples at epoch 5">
            <figcaption>Results on the test set with noise level 0.5 after the 5th epoch</figcaption>
          </figure>
      </div>
  
      <p class="muted">
        <b>What to look for:</b> Early in training the output is blurry/averaged because the model only
        learns coarse structure first. As training progresses, strokes sharpen and background noise is
        suppressed more consistently.
      </p>
  
      <h4>B1.2.2 Out-of-Distribution (OOD) Testing</h4>
      <p>
        After training at a fixed σ, I evaluate robustness by testing the denoiser on noise levels it
        was <b>not</b> trained on (smaller σ = easier, larger σ = harder). This reveals whether the model
        learned a generally useful denoising prior or whether it overfit to a specific corruption strength.
      </p>
  
      <div class="grid">
        <figure>
          <img src="media/PartB/1_2_2.png" alt="B1 OOD sigma sweep">
          <figcaption>OOD sweep across σ values (increasing corruption strength).</figcaption>
        </figure>
      </div>
  
      <p class="muted">
        <b>Expected behavior:</b> For σ lower than training, outputs should look clean (often slightly
        oversmoothed). For σ higher than training, the model often fails by hallucinating ambiguous
        strokes or collapsing to “average digit-like” blobs because too much information is destroyed.
      </p>
  
      <h4>B1.2.3 Denoising Pure Noise</h4>
      <p>
        I also test the extreme case: input is <b>pure Gaussian noise</b> (no underlying digit), and I apply
        the denoiser anyway. This is a stress test: a single-step denoiser is <b>not</b> trained to be a
        full generative model, so it typically does not converge to realistic samples from pure noise.
        Instead, it tends to produce faint digit-like textures or unstable artifacts.
      </p>
  
      <div class="grid">
        <figure>
          <img src="media/PartB/1_2_3_loss_curve.png" alt="Training loss curve">
          <figcaption>Training loss curve for the single-step denoiser.</figcaption>
        </figure>
        <figure>
            <img src="media/PartB/1_2_3_results_epoch1.png" alt="Results on the test set with noise level 0.5 after the first epoch">
            <figcaption>Results on the test set with noise level 0.5 after the first epoch</figcaption>
          </figure>
          <figure>
            <img src="media/PartB/1_2_3_results_epoch5.png" alt="Results on the test set with noise level 0.5 after the 5th epoch">
            <figcaption>Results on the test set with noise level 0.5 after the 5th epoch</figcaption>
          </figure>
      </div>
    </section>
  
    <section id="B2" class="card">
        <h3>Part 2. Training a Flow Matching Model</h3>
  
        <h4>2.1 Adding Time Conditioning to UNet</h4>
        <p>
          One-step denoising can clean a digit if some signal remains, but it cannot <i>generate</i> from pure noise.
          Flow matching fixes this by learning a continuous-time transport from noise to data.
          I implement a time-conditioned UNet that takes <code class="inline">(x<sub>t</sub>, t)</code> as input and predicts a
          velocity field. I inject time information using an MLP/FCBlock that
          maps the scalar <code class="inline">t ∈ [0, 1]</code> into a feature embedding that conditions the UNet blocks.
        </p>
  
        <h4>2.2 Training the UNet (Flow Matching Objective)</h4>
        <p>
          For each batch of clean digits <code class="inline">x<sub>1</sub></code>, I sample a noise image
          <code class="inline">x<sub>0</sub> ~ N(0, I)</code> and a random time <code class="inline">t ~ Uniform(0, 1)</code>.
          I construct the interpolated point
          <code class="inline">x<sub>t</sub> = (1 − t) x<sub>0</sub> + t x<sub>1</sub></code>.
          The target velocity for this simple linear path is constant:
          <code class="inline">v<sub>target</sub> = x<sub>1</sub> − x<sub>0</sub></code>.
          The network predicts <code class="inline">v<sub>θ</sub>(x<sub>t</sub>, t)</code>, and I train with MSE
          <code class="inline">||v<sub>θ</sub> − v<sub>target</sub>||<sub>2</sub><sup>2</sup></code>.
        </p>
        <div class="grid">
            <figure>
                <img src="media/PartB/2_2_loss_curve.png" alt="B2.2 time-conditioned training loss curve">
                <figcaption>Training loss curve for time-conditioned flow matching.</figcaption>
              </figure>
        </div>
  
        <h4>2.3 Sampling from the Time-Conditioned UNet</h4>
        <p>
          I train the time-conditioned model with the recommended settings: batch size 64, hidden channels
          <code class="inline">D = 64</code>, and Adam with an initial learning rate <code class="inline">1e−2</code>
          plus an exponential LR decay schedule that drops by 10× over 10 epochs.
          To <b>sample</b>, I start from <code class="inline">x<sub>0</sub> ~ N(0, I)</code> and numerically integrate the learned
          velocity field from <code class="inline">t = 0</code> to <code class="inline">t = 1</code> using
          <code class="inline">num_ts = 50</code> steps (Euler updates). Each step applies:
          <code class="inline">x ← x + (Δt) · v<sub>θ</sub>(x, t)</code>.
          Over the trajectory, samples gradually sharpen from noise into digit-like structure.
        </p>
  
        <div class="grid">
          <figure>
            <img src="media/PartB/2_2_results_epoch1.png" alt="B2.3 samples after epoch 1">
            <figcaption>Samples from the time-conditioned model after epoch 1.</figcaption>
          </figure>
          <figure>
            <img src="media/PartB/2_2_results_epoch5.png" alt="B2.3 samples after epoch 5">
            <figcaption>Samples from the time-conditioned model after epoch 10 (digits become sharper and more coherent).</figcaption>
          </figure>
          <figure>
            <img src="media/PartB/2_2_results_epoch10.png" alt="B2.3 samples after epoch 10">
            <figcaption>Samples from the time-conditioned model after epoch 10 (digits become sharper and more coherent).</figcaption>
          </figure>
        </div>
  
        <h4>2.4 Adding Class-Conditioning to UNet</h4>
        <p>
          Next, I add label information so the model can generate a <b>specific digit class</b>.
          In the notebook, I implement <code class="inline">ClassConditionalUNet</code> by embedding the digit label
          <code class="inline">y ∈ {0,…,9}</code> and injecting it (along with the time embedding) into the UNet.
          This changes the learned vector field to <code class="inline">v<sub>θ</sub>(x<sub>t</sub>, t, y)</code>.
        </p>
  
        <h4>2.5 Training the UNet (Class Conditioning)</h4>
        <p>
          Training mirrors the time-only setup, except the UNet also receives the class label.
          I keep the same core hyperparameters (batch size 64, <code class="inline">D = 64</code>, Adam) and train for 10 epochs
          with exponential LR decay from <code class="inline">1e−2</code>.
          During training I periodically sample a grid of digits by fixing labels and sampling different noise seeds.
        </p>
        <div class="grid">
            <figure>
                <img src="media/PartB/2_6_loss_curve.png" alt="2.5 class-conditioned training loss curve">
                <figcaption>Training loss curve for the class-conditioned flow-matching model.</figcaption>
              </figure>
        </div>
  
        <h4>2.6 Sampling from the Class-Conditioned UNet + CFG</h4>
        <p>
          To strengthen class fidelity at sampling time, I use <b>classifier-free guidance</b>.
          The idea is to compute both an unconditional velocity and a conditional velocity, then interpolate:
          <code class="inline">v<sub>guided</sub> = v<sub>uncond</sub> + s · (v<sub>cond</sub> − v<sub>uncond</sub>)</code>.
          In practice, this requires the model to support unconditional behavior, which I implement via
          <b>label dropout</b> during training: with some probability I replace <code class="inline">y</code> with a special “null”
          label so the same network learns both modes.
        </p>
  
        <div class="grid">
            <figure>
              <img src="media/PartB/2_6_results_epoch1.png" alt="2.6 samples epoch 1">
              <figcaption>Class-conditioned samples after epoch 1 (labels start to control coarse shape).</figcaption>
            </figure>
            <figure>
              <img src="media/PartB/2_6_results_epoch5.png" alt="2.6 samples epoch 5">
              <figcaption>Class-conditioned samples after epoch 5 (labels start to control coarse shape).</figcaption>
            </figure>
              <figure>
                  <img src="media/PartB/2_6_results_epoch10.png" alt="2.6 samples epoch 10">
                  <figcaption>Class-conditioned samples after epoch 10 (stronger class fidelity + sharper strokes).</figcaption>
              </figure>
          </div>
      </section>
  
      <!-- ========================= B4 ========================= -->
      <section id="B4" class="card">
        <h3>Bells &amp; Whistles (Part B / CS280A)</h3>
  
        <h4>B4.1 Training a Class-Conditioned Model without LR Scheduler</h4>
        <p>
          As an ablation, I retrain the class-conditioned model using a <b>constant</b> learning rate
          (<code class="inline">1e−3</code>) and remove the exponential scheduler. This tests whether the scheduler is important
          for stability and final sample quality. In my notebook, I compare the loss curve and sample grids at epochs
          1, 5, and 10.
        </p>
  
        <h4>B4.2 “Better” Time-Only UNet (More Capacity + Longer Training)</h4>
        <p>
          I also improved the time-conditioned model by (1) increasing capacity
          <code class="inline">D: 64 → 128</code> and (2) training longer (<code class="inline">10 → 20</code> epochs) with a smaller
          learning rate (<code class="inline">1e−3</code>) for stability. I then generate a larger grid of samples (4×10)
          to qualitatively evaluate diversity and sharpness.
        </p>
  
        <div class="grid">
          <figure>
            <img src="media/PartB/bells_epoch1.png" alt="results after the first epoch">
            <figcaption>Results after the first epoch.</figcaption>
          </figure>
          <figure>
            <img src="media/PartB/bells_epoch5.png" alt="results after the 5th epoch">
            <figcaption>Results after the 5th epoch.</figcaption>
          </figure>
          <figure>
            <img src="media/PartB/bells_epoch10.png" alt="results after the 10th epoch">
            <figcaption>Results after the 10th epoch.</figcaption>
          </figure>
        </div>
      </section>
    </section>
    <!-- ========================= /PART B ========================= -->
  
    <!-- ========================= WHAT I LEARNED ========================= -->
    <section id="learned" class="card">
      <h2>What I Learned</h2>
      <ul>
        <li>
          <b>Single-step denoising is not a generator.</b> When the input contains no information (pure noise),
          MSE training encourages regression toward an average digit, motivating iterative generative dynamics.
        </li>
        <li>
          <b>Flow matching turns generation into learning a vector field.</b> By training a time-conditioned UNet to predict
          the velocity along an interpolation path, sampling becomes integrating an ODE from noise to data.
        </li>
        <li>
          <b>Conditioning matters.</b> Adding class conditioning and CFG can significantly improve class fidelity, but guidance
          must be tuned to balance fidelity vs diversity.
        </li>
      </ul>
    </section>
